\chapter{Discussion}\label{Discussion}

\section{Conclusions}

The main motivation for this work was the observation that characteristic sparse representations emerge from sparse coding models trained over a variety of natural scene datasets by a variety of learning algorithms. The theorems proven here provide some insight into this phenomenon by establishing very general conditions under which identification of the model parameters is not only possible but also robust to measurement and modeling error.

%Specifically, given a dictionary and sparse codes purported to solve to Prob.~\ref{InverseProblem}, one checks that that they satisfy the corresponding constraints on $\mathbf{A}$ and the $\mathbf{x_i}$ given in the statement of Thm.~\ref{DeterministicUniquenessTheorem}. 

The guarantees concerning the identification of a dictionary and corresponding sparse codes of minimal average support size (Thm.~\ref{SLCopt}), which is the optimization problem of most interest to practitioners (Prob.~\ref{OptimizationProblem}), are to my knowledge the first of their kind in both the noise-free and noisy domains. It has been shown here that, given sufficient data, this problem reduces to an instance of Prob.~\ref{InverseProblem} to which the main result (Thm.~\ref{DeterministicUniquenessTheorem}) then applies: every dictionary and corresponding set of sparse codes consistent with the data are equivalent up to inherent relabeling/scaling ambiguities and a discrepancy (error) that scales linearly with the noise. 
In fact, %provided $n \geq \min(2k,m)$, 
in almost all cases these problems are well-posed given a sufficient amount of data (Thm.~\ref{robustPolythm} and Cor.~\ref{ProbabilisticCor}). 
Furthermore, the derived scaling constants are explicit and computable; as such, there is an effective procedure that suffices to affirm if a proposed solution to these problems is indeed unique up to noise and inherent ambiguities, although it is not efficient in general. 

%to the recovery of ``mouse neuronal activity representing location on a track \cite{agarwal2014spatially}

%Beyond an extension of existing noiseless guarantees \cite{Hillar15} to the noisy regime and their novel application to Prob.~\ref{OptimizationProblem}, this work contains a theory of combinatorial designs for support sets key to identification of dictionaries. This idea is incorporated into a fundamental lemma in matrix theory (Lem.~\ref{MainLemma}) that draws upon the definition of a matrix lower bound (\ref{Ldef}) induced by a hypergraph. As a consequence of this combinatorial approach, recovery of some or all dictionary elements can be guaranteed even if: i) dictionary size is overestimated, ii) data cover only a polynomial number of distinct sparse supports, and iii) dictionaries do not satisfy the spark condition. 

A reviewer of this work lamented that while the extension from exact recovery to the noisy stability of dictionary learning is significant, the fact that the analysis relies on metrics of the data that are not feasible to compute limits its impact to the scientific community beyond computer science and applied mathematics. What sets the main results of this work apart from the vast majority of results in the field is that they are deterministic, and do not depend on any kind of assumption about the particular random distribution from which the sparse supports, coefficients, or dictionary entries are drawn (though Cor.~\ref{ProbabilisticCor} makes a sweeping statement applicable to all continuous distributions). Consequently, the inferences of those applying dictionary learning methods to inverse problems in their research are justified only in principle; but this is unavoidably the case for NP-hard problems. 

Indeed, theoretical validation makes little practical difference if the methodology is already in widespread use, while practical criteria establishing whether the data models obtained by practitioners are optimal or not would have very high impact. To this end, the work has been laid out for those wanting to derive statistical criteria for inference with respect to more domain-specific structured dictionaries and codes (i.e. estimate $C_1$), and reduced by half for those hoping to prove the consistency of any dictionary learning algorithm (i.e. prove convergence to within $\varepsilon(\delta_1,\delta_2)$ given in \eqref{epsdel}). 
%who will derive from our deterministic guarantees the statistical criteria for inference in more domain-specific probabilistic models

Nonetheless, a main reason for the sustained interest in dictionary learning as an unsupervised method for data analysis seems to be the assumed well-posedness of parameter identification in the model, confirmation of which forms the core of these findings. Several groups have applied compressed sensing to signal processing tasks; for instance, in MRI analysis \cite{lustig2008compressed}, image compression \cite{Duarte08}, and even the design of an ultrafast camera \cite{Gao14}. It is only a matter of time before these systems incorporate dictionary learning to encode and decode signals (e.g., in a device that learns structure from motion \cite{kong2016prior}), just as scientists have used sparse coding to %make sense of their data \cite{jung2001imaging, agarwal2014spatially, lee2016sparse, wu2016stability}. 
uncover latent structure in data (e.g., forgery detection \cite{hughes2010, olshausen2010applied}; brain recordings \cite{jung2001imaging, agarwal2014spatially, lee2016sparse}; and gene expression \cite{wu2016stability}). As uniqueness guarantees with minimal assumptions apply to all areas of data science and engineering that utilize learned sparse structure, assurances offered by these theorems give hope that different devices and algorithms may learn equivalent representations given enough data from statistically identical systems.\footnote{To contrast with the current hot topic of ``Deep Learning'', there are few such uniqueness guarantees for these models of data; moreover, even small noise can dramatically alter their output \cite{goodfellow2014explaining}.} 


Within the field of theoretical neuroscience in particular, dictionary learning for sparse coding and related methods have recovered characteristic components of natural images \cite{Olshausen96, hyvarinen1999fast, bell1997independent, van1998independent} and sounds \cite{bellsejnowski1996, smithlewicki2006, Carlson12} that reproduce response properties of cortical neurons. The results of this work suggest that this correspondence could be due to the ``universality" of sparse representations in natural data, an early mathematical idea in neural theory \cite{pitts1947}. Furthermore, they justify the soundness of one of the few hypothesized theories of bottleneck communication in the brain \cite{Isely10}: that sparse neural population activity is recoverable from its noisy linear compression through a randomly constructed (but unknown) wiring bottleneck by any biologically plausible unsupervised sparse coding method that solves Prob.~\ref{DeterministicUniquenessTheorem} or \ref{SLCopt} (e.g., \cite{rehnsommer2007, rozell2007neurally, pehlevan2015normative}).\footnote{We refer the reader to \cite{ganguli2012compressed} for more on interactions between dictionary learning and neuroscience.}

%The solution to Prob.~\ref{OptimizationProblem}, that of most interest to practitioners of dictionary learning methods, is (to my knowledge) the first of its kind in both the noise-free and noisy domains. 