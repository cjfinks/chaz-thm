\chapter{Discussion}\label{Discussion}

\section{Conclusions}

The main motivation for this work was the observation that characteristic sparse representations emerge from sparse coding models trained over a variety of natural scene datasets by a variety of learning algorithms. The theorems proven here provide some insight into this phenomenon by establishing very general conditions under which identification of the model parameters is not only possible but also robust to measurement and modeling error.

The guarantees concerning the identification of a dictionary and corresponding sparse codes of minimal average support size (Thm.~\ref{SLCopt}), which is the optimization problem of most interest to practitioners (Prob.~\ref{OptimizationProblem}), are to my knowledge the first of their kind in both the noise-free and noisy domains. It has been shown here that, given sufficient data, this problem reduces to an instance of Prob.~\ref{InverseProblem} to which the main result (Thm.~\ref{DeterministicUniquenessTheorem}) then applies: every dictionary and corresponding set of sparse codes consistent with the data are equivalent up to inherent relabeling/scaling ambiguities and a discrepancy (error) that scales linearly with the noise. 
In fact, %provided $n \geq \min(2k,m)$, 
in almost all cases these problems are well-posed given a sufficient amount of data (Thm.~\ref{robustPolythm} and Cor.~\ref{ProbabilisticCor}). 
Furthermore, the derived scaling constants are explicit and computable; as such, there is an effective procedure that suffices to affirm if a proposed solution to these problems is indeed unique up to noise and inherent ambiguities, although it is not efficient in general. %Specifically, given a dictionary and sparse codes purported to solve to Prob.~\ref{InverseProblem}, one checks that that they satisfy the corresponding constraints on $\mathbf{A}$ and the $\mathbf{x_i}$ given in the statement of Thm.~\ref{DeterministicUniquenessTheorem}. 



%to the recovery of ``mouse neuronal activity representing location on a track \cite{agarwal2014spatially}

The absence of any assumptions at all about dictionaries that solve Prob.~\ref{InverseProblem} was a major technical hurdle in proving Thm.~\ref{DeterministicUniquenessTheorem}. 
This very general guarantee was sought because of the practical difficulty of ensuring that an algorithm maintain a dictionary satisfying the spark condition \eqref{SparkCondition} at each iteration, which (to my knowledge) has been an explicit or implicit assumption of all previous works except \cite{Hillar15}; indeed, even certifying a dictionary has this property is NP-hard \cite{tillmann2014computational}.

Several results in the literature had to be combined to extend the guarantees derived in \cite{Hillar15} into the noisy regime. The main challenge was to generalize Lem.~\ref{MainLemma} to the case where the $k$-dimensional subspaces spanned by corresponding submatrices $\mathbf{A}_S$ and $\mathbf{B}_{\pi(S)}$ are assumed to be``close" but not identical. Unlike in \cite{Hillar15} where an inductive argument could be applied to the noiseless case, here it had to be explicitly demonstrated that this proximity relation is propagated through iterated intersections right down to the spans of the dictionary elements themselves. Lem.~\ref{DistanceToIntersectionLemma} was designed to encapsulate this fact, proven by appeal to a convergence guarantee for an alternating projections algorithm first proposed by von Neumann. This result, combined with a little known fact \eqref{eqdim} about the distance metric between subspaces, make up the more obscure components of the deduction.

The proof of Lem.~\ref{MainLemma} diverges most significantly from the approach taken in \cite{Hillar15} by way of Lem.~\ref{NonEmptyLemma}, which utilizes a combinatorial design for support sets (the ``singleton intersection property") to reduce the deterministic sample complexity by an exponential factor. This constitutes a significant advance toward legitimizing dictionary learning in practice, since data must otherwise exhibit the exponentially many possible $k$-wise combinations of dictionary elements required by (to my knowledge) all previously published results; although an exponential number of samples per support is still required (unless $k$ is held fixed). The issue is that the map $\pi:\mathcal{H} \to {[m] \choose k}$ is surjective only when $\mathcal{H}$ is taken to be ${[m] \choose k}$, in which case one may proceed by induction as in \cite{Hillar15}, freely choosing supports in the codomain of $\pi$ to intersect over $(k-1)$ indices to then map back to some corresponding set of $(k-1)$ indices at the intersection of supports in the domain. Here, for $\mathcal{H} \subset {[m] \choose k}$, a bijection between indices had to instead be established by pigeonholing the image of $\pi$ under constraints imposed by the SIP, which was formulated specifically for this purpose. It just so happened that this more general argument for a non-surjective $\pi$ constrained by the SIP applied just as well to the situation where the number of dictionary elements $m$ is over-estimated (i.e. $\overline m > m$), in which case a one-to-one correspondence can be guaranteed between a subset of columns of $\mathbf{A}$ and $\mathbf{B}$ of a size simply expressed in terms of the width of each matrix and the regularity of $\mathcal{H}$. 

One of the mathematically significant achievements in \cite{Hillar15} was to break free from the constraint that the recovery matrix $\mathbf{B}$ satisfy the spark condition in addition to the generating dictionary $\mathbf{A}$. %, in contrast to all previously known uniqueness results which (to my knowledge) have all either explicitly or implicitly made this assumption. 
Here, it has been demonstrated that, in fact, even $\mathbf{A}$ need not satisfy the spark condition! Rather, $\mathbf{A}$ need only be injective on the union of subspaces with supports forming a regular $k$-uniform hypergraph satisfying the SIP (a distinguishing example is given in Sec.~\ref{Results}). This relaxation of constraints inspired the definition of the restricted matrix lower bound $L_{\mathcal{H}}$ in \eqref{Ldef}, which generalizes the well-known (see footnote \ref{ripfootnote}) restricted matrix lower bound $L_k$ to be in terms of a hypergraph $\mathcal{H}$, and an interesting object for further study in its own right. %(For instance, how hard is it to check that $L_\mathcal{H}(\mathbf{A}) > 0$?) 
%The absence of any restrictions on dictionaries solving Prob.~\ref{InverseProblem} is a major technical obstruction, but highly desirable because of the practical difficulty in ensuring that an algorithm maintain a dictionary satisfying the spark condition \eqref{SparkCondition} at each iteration; indeed, even certifying a dictionary has this property is NP-hard \cite{tillmann2014computational}. 

To reiterate, the methods applied here to prove Thm.~\ref{DeterministicUniquenessTheorem} yield the following results beyond a straightforward extension of those in \cite{Hillar15} to the noisy case:

\begin{enumerate}
\item A reduction in deterministic sample complexity: To identify the $n \times m$ generating dictionary $\mathbf{A}$, it is required in \cite{Hillar15} that $k{m \choose k}$ data points be sampled from each of the ${m \choose k}$ subspaces spanned by subsets of $k$ columns of $\mathbf{A}$. It is shown here that in fact it suffices to sample from at most $m$ such subspaces (see Cor.~\ref{DeterministicUniquenessCorollary}). %; thus, for fixed $k$, the deterministic sample complexity is polynomial in $m$.  %HS15 was already poly for fixed k
\item An extension of guarantees to the case where the number of dictionary elements is unknown: The results of \cite{Hillar15} only apply to the case where the matrix $\mathbf{B}$ has the same number of columns as $\mathbf{A}$. It is shown here that if $\mathbf{B}$ has at least as many columns as $\mathbf{A}$ then it contains (up to noise) a subset of the columns of $\mathbf{A}$.
\item Relaxed requirements (no spark condition) on the generating matrix $\mathbf{A}$: Rather, $\mathbf{A}$ need only be injective on the union of subspaces with supports that form a regular uniform hypergraph satisfying the SIP. 
\end{enumerate}

As it seemed to benefit a reviewer of this work, a brief discussion on how the deterministic sample complexity $N = |\mathcal{H}\left[|(k-1){m \choose k} + 1\right]$ derived here compares to those listed in the the top two rows listed in Table I of \cite{Hillar15} may be in order. To be clear, the theory developed here is strictly more general, since $\mathcal{H}$ can always be taken to be ${[m] \choose k}$. The point of difference in this comparison is the assumed set of supports for sparse codes, which is always ${[m] \choose k}$ in \cite{Hillar15}, whereas here it can be assumed to be any regular $k$-uniform hypergraph that satisfies the SIP. By row, %, assuming for our results that sufficient data have been sampled from all supports in a hypergraph H satisfying the SIP:
\begin{enumerate}
\item[I.] The result here improves upon the listed $k{m \choose k}^2$ by an exponential factor, since for every $k < m$ there exists a regular $k$-uniform hypergraph $\mathcal{H}$ with $|\mathcal{H}| = m$ satisfying the SIP. 
\item[II.] The authors in \cite{Hillar15} have applied measure-theoretic arguments to achieve $(k+1){m \choose k}$ with almost-certainty (i.e. with probability one), a factor of $m$ reduction over that for which certainty can alternatively be guaranteed here.    
%\item[III.] A direct comparison cannot be made because it is an open question as to the probability that a random set of supports satisfies the SIP. This would be an interesting problem for the community to solve; regardless, the result of \cite{Hillar15} is still implied by the more general theory given here.
%\item[IV.] The result here is $|\mathcal{H}\left[|(k-1){m \choose k} + 1\right]$ with probability one compared to $(k+1){m \choose k}$ with probability one. In this case the sample complexity stated in \cite{Hillar15} is marginally better, but it relies on a noise-free technicality unique to their assumption $\mathcal{H} = {m \choose k}$. %differs in flavor: their $\mathbf{x_i}$ are assumed to be distributed as $(k+1)$ samples per support for every support of size $k \in [m]$, whereas here it is assumed that $(k-1){m \choose k}$ codes $\mathbf{x_i}$ are distributed over each support in some hypergraph $\mathcal{H}$ satisfying the SIP. 
%\item[V.] Again, a direct comparison cannot be made for the reason stated for row III.
\end{enumerate}

%Beyond an extension of existing noiseless guarantees \cite{Hillar15} to the noisy regime and their novel application to Prob.~\ref{OptimizationProblem}, this work contains a theory of combinatorial designs for support sets key to identification of dictionaries. This idea is incorporated into a fundamental lemma in matrix theory (Lem.~\ref{MainLemma}) that draws upon the definition of a matrix lower bound (\ref{Ldef}) induced by a hypergraph. As a consequence of this combinatorial approach, recovery of some or all dictionary elements can be guaranteed even if: i) dictionary size is overestimated, ii) data cover only a polynomial number of distinct sparse supports, and iii) dictionaries do not satisfy the spark condition. 

Another reviewer of this work lamented that while the extension from exact recovery to the noisy stability of dictionary learning is significant, the fact that the analysis relies on metrics of the data that are not feasible to compute limits its impact to the scientific community beyond computer science and applied mathematics. What sets the main results of this work apart from the vast majority of results in the field is that they are deterministic, and do not depend on any kind of assumption about the particular random distribution from which the sparse supports, coefficients, or dictionary entries are drawn (though Cor.~\ref{ProbabilisticCor} makes a sweeping statement applicable to all continuous distributions). Consequently, the inferences of those applying dictionary learning methods to inverse problems in their research are justified only in principle; but this is unavoidably the case for NP-hard problems. 

Indeed, theoretical validation makes little practical difference if the methodology is already in widespread use, while practical criteria establishing whether the data models obtained by practitioners are optimal or not would have very high impact. To this end, the work has been laid out for those wanting to derive statistical criteria for inference with respect to more domain-specific structured dictionaries and codes (i.e. estimate $C_1$), and reduced by half for those hoping to prove the consistency of any dictionary learning algorithm (i.e. prove convergence to within $\varepsilon(\delta_1,\delta_2)$ given in \eqref{epsdel}). 
%who will derive from our deterministic guarantees the statistical criteria for inference in more domain-specific probabilistic models

Nonetheless, a main reason for the sustained interest in dictionary learning as an unsupervised method for data analysis seems to be the assumed well-posedness of parameter identification in the model, confirmation of which forms the core of these findings. Several groups have applied compressed sensing to signal processing tasks; for instance, in MRI analysis \cite{lustig2008compressed}, image compression \cite{Duarte08}, and even the design of an ultrafast camera \cite{Gao14}. It is only a matter of time before these systems incorporate dictionary learning to encode and decode signals (e.g., in a device that learns structure from motion \cite{kong2016prior}), just as scientists have used sparse coding to %make sense of their data \cite{jung2001imaging, agarwal2014spatially, lee2016sparse, wu2016stability}. 
uncover latent structure in data (e.g., forgery detection \cite{hughes2010, olshausen2010applied}; brain recordings \cite{jung2001imaging, agarwal2014spatially, lee2016sparse}; and gene expression \cite{wu2016stability}). As uniqueness guarantees with minimal assumptions apply to all areas of data science and engineering that utilize learned sparse structure, assurances offered by these theorems give hope that different devices and algorithms may learn equivalent representations given enough data from statistically identical systems.\footnote{To contrast with the current hot topic of ``Deep Learning'', there are few such uniqueness guarantees for these models of data; moreover, even small noise can dramatically alter their output \cite{goodfellow2014explaining}.} 


Within the field of theoretical neuroscience in particular, dictionary learning for sparse coding and related methods have recovered characteristic components of natural images \cite{Olshausen96, hyvarinen1999fast, bell1997independent, van1998independent} and sounds \cite{bellsejnowski1996, smithlewicki2006, Carlson12} that reproduce response properties of cortical neurons. The results of this work suggest that this correspondence could be due to the ``universality" of sparse representations in natural data, an early mathematical idea in neural theory \cite{pitts1947}. Furthermore, they justify the soundness of one of the few hypothesized theories of bottleneck communication in the brain \cite{Isely10}: that sparse neural population activity is recoverable from its noisy linear compression through a randomly constructed (but unknown) wiring bottleneck by any biologically plausible unsupervised sparse coding method that solves Prob.~\ref{DeterministicUniquenessTheorem} or \ref{SLCopt} (e.g., \cite{rehnsommer2007, rozell2007neurally, pehlevan2015normative}).\footnote{We refer the reader to \cite{ganguli2012compressed} for more on interactions between dictionary learning and neuroscience.}

%The solution to Prob.~\ref{OptimizationProblem}, that of most interest to practitioners of dictionary learning methods, is (to my knowledge) the first of its kind in both the noise-free and noisy domains. 