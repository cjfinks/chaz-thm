\documentclass[9pt,twocolumn]{pnas-new}
% Use the lineno option to display guide line numbers if required.
% Note that the use of elements such as single-column equations
% may affect the guide line number alignment. 

%%% CHAZ TODO %%%

%%% CHRIS TODO %%
% steamroll

% IMPORTED PACKAGES
\usepackage{amsmath, amssymb, amsthm} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

%\usepackage[pdftex]{graphicx}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}
%\usepackage{cite}

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} = Template for a one-column mathematics article
% {pnasinvited} = Template for a PNAS invited submission

\title{On the uniqueness and stability of dictionaries for sparse representation of noisy signals}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,1]{Charles J. Garfinkle}
\author[a,1]{Christopher J. Hillar} 

\affil[a]{Redwood Center for Theoretical Neuroscience, Berkeley, CA, USA}

% Please give the surname of the lead author for the running footer
\leadauthor{Garfinkle} 

\significancestatement{Many naturally occurring signals (visual scenery, speech, EEG, etc.) lacking domain-specific formal models can nonetheless be usefully characterized as combinations of few elementary waveforms drawn from a large `dictionary'. We give general conditions guaranteeing when such dictionaries are uniquely and stably determined by data. The result justifies the application of this model to blind source separation, and may help to explain the observed universality of emergent representations in sparse coding models of some natural phenomena.
%These results provide theoretical grounding for  the application of dictionary learning to recovering sparse signals from unknown noisy compressive measurements.
}

\authordeclaration{The authors declare no conflict of interest.}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: chaz@berkeley.edu, chillar@msri.org}

\keywords{Sparse coding, dictionary learning, identifiability, compressive sensing} 

\begin{abstract}
Dictionary learning for sparse linear coding has exposed underlying structure in many natural signals.
However, there are few universal theorems guaranteeing uniqueness of model estimation independent of implementation.
Here, we prove that for all diverse enough datasets generated from the sparse coding model, latent dictionaries and codes are uniquely determined up to measurement error.  Applications are given to data analysis, engineering, and neuroscience. 
\end{abstract}

% \dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

% Optional adjustment to line up main text (after abstract) of first page with line numbers, when using both lineno and twocolumn options.
% You should only change this length when you've finalised the article contents.
\verticaladjustment{-2pt}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.
\dropcap{B}lind source separation is a classical problem in signal processing \cite{sato1975method}.
A common modern assumption is that each of $N$ observed $n$-dimensional signals is a (noisy) linear combination of at most $k$ elementary waveforms drawn from some unknown ``dictionary'' of size $m$, typically with $k < m \ll N$ (see \cite{Zhang15} for a comprehensive review of this and related models).
%A popular underlying assumption is that there exists a `dictionary' of $m$ elementary waveforms, at most $k$ of which need be linearly combined to represent each signal in a given dataset of size $N$, typically where $k < m \ll N$.  
Approximating solutions to this sparsity-constrained inverse problem have provided insight into the structure of many signal classes lacking domain-specific formal models (e.g., in vision \cite{wang2015sparse}).  In particular, it has been shown that response properties of simple-cell neurons in mammalian visual cortex emerge from optimizing a dictionary to represent small patches of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent}, a major advance in computational neuroscience. A curious aspect of this finding is that the latent waveforms (e.g. `Gabor' wavelets) estimated from data appear to be canonical \cite{donoho2001can};
i.e., they are found in learned dictionaries independent of algorithmic implementation or natural image training set.

% (e.g., Fourier bases, wavelets, etc.)

Motivated by these discoveries and earlier work in the theory of neural communication \cite{Coulter10, Isely10}, we address when dictionaries and the sparse representations they induce are uniquely determined by data.  Answers to this question also have other real-world implications.  For example, a sparse coding analysis of local painting style can be used for forgery detection \cite{hughes2010, Olshausen10}, but only if the identifying sparse components of the artwork can be consistently estimated. % are independent of implementation idiosyncrasies. 
Fortunately, algorithms with proven recovery of generating dictionaries under certain 
conditions have recently been proposed (see \cite[Sec.~I-E]{Sun16} for a summary of the state-of-the-art). Few theorems, however, can be cited to explain this uniqueness phenomenon more generally.

Here, we prove very generally that uniqueness and stability in sparse linear coding is an expected property of the model. %; that is, it is a well-posed problem (in the sense of Hadamard \cite{Hadamard1902}). 
%In other words, whenever enough sparsely represented data generated from a dictionary (satisfying a spark condition) are observed, the original codes and dictionary are uniquely determined up to an error that is commensurate with the measurement noise. 
More specifically, dictionaries that preserve sparse codes (i.e., satisfy a `spark condition') are identifiable from as few as \mbox{$N = m(k-1){m \choose k} + m$} noisy sparse linear combinations of their columns up to an error linear in the noise (Thm.~\ref{DeterministicUniquenessTheorem}). In fact, provided $n \geq \min(2k,m)$, in almost all cases the dictionary learning problem is well-posed (as per Hadamard \cite{Hadamard1902}) given enough data (Cor.~\ref{ProbabilisticCor}). Moreover, these guarantees hold without assuming the recovered matrix satisfies a spark condition, even when the number $m$ of dictionary elements is unknown. The explicit, algorithm-independent criteria we provide should be a useful tool in the theory of sparse representations.

%Other applications include: 1) a sufficient diversity of sparse codes sent through a random linear compressive channel are identifiable and 2) an explanation for universal representation of natural signals in sparse coding models from neuroscience.

More formally, let $\mathbf{A} \in \mathbb R^{n \times m}$ be a matrix with columns $\mathbf{A}_j$ ($j = 1,\ldots,m$) and let dataset $Z$ consist of measurements:
\begin{align}\label{LinearModel}
\mathbf{z}_i = \mathbf{A}\mathbf{x}_i + \mathbf{n}_i,\ \ \  \text{$i=1,\ldots,N$},
\end{align}
for $k$-\emph{sparse} $\mathbf{x}_i \in \mathbb{R}^m$ having at most $k$ nonzero entries and \emph{noise} $\mathbf{n}_i \in \mathbb{R}^n$, with bounded norm $\| \mathbf{n}_i \|_2 \leq  \eta$ representing our combined worst-case uncertainty in  measuring $\mathbf{A}\mathbf{x}_i$.
The precise mathematical problem addressed here is the following.

\begin{problem}[Sparse linear coding]\label{InverseProblem}
Find a matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$ such that $\|\mathbf{z}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \eta$ for all $i$.
\end{problem}

Note that any particular solution $(\mathbf{B}, \mathbf{\bar x}_1 \ldots, \mathbf{\bar x}_N)$ to this problem gives rise to an orbit of equivalent solutions $(\mathbf{BPD}, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_1, \ldots, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_N)$, where $\mathbf{P}$ is any permutation matrix and $\mathbf{D}$ any invertible diagonal. Previous theoretical work addressing the noiseless case $\eta =0$ (e.g., \cite{li2004analysis, Georgiev05, Aharon06, Hillar15}) has shown that a solution to Prob.~\ref{InverseProblem} (when it exists) is indeed unique up to this inherent ambiguity provided the $\mathbf{x}_i$ are sufficiently diverse and the generating matrix $\mathbf{A}$ satisfies the \textit{spark condition} from compressive sensing (CS):
\begin{align}\label{SparkCondition}
\mathbf{A}\mathbf{x}_1 = \mathbf{A}\mathbf{x}_2 \implies \mathbf{x}_1 = \mathbf{x}_2, \ \ \ \text{for all $k$-sparse } \mathbf{x}_1, \mathbf{x}_2,
\end{align}
%
which would be, in any case, necessary for uniqueness. Our concern here is the stability of these solutions with respect to noise in the data.

\begin{definition}\label{maindef}
Fix $Y = \{ \mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$. We say $Y$ has a \textbf{$k$-sparse representation in $m$ dimensions} if there exists a matrix $\mathbf{A}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ such that $\mathbf{y}_i = \mathbf{A}\mathbf{x}_i$ for all $i$. %Equivalently, $\mathbf{Y} = \mathbf{A}\mathbf{X}$ for $\mathbf{Y}$ and $\mathbf{X}$ having the $\mathbf{y}_i$ and $\mathbf{x}_i$ as their columns, respectively. 
This representation is \textbf{stable} if for every $\delta_1, \delta_2 \geq 0$, there exists $\varepsilon(\delta_1, \delta_2) \geq 0$ (with $\varepsilon > 0$ when  $\delta_1, \delta_2 > 0$) such that if $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$ satisfy:
\begin{align*}
\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon,\ \ \   \text{for all $i$},
\end{align*}
%
then there is some permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ such that for all $i, j$:
\begin{align}\label{def1}
\|\mathbf{A}_j - (\mathbf{BPD})_j\|_2 \leq \delta_1 \ \ \text{and} \ \ \|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i\|_1 \leq \delta_2.
\end{align}
\end{definition}

To see how Def. \ref{maindef} directly relates to Prob. \ref{InverseProblem}, suppose that $Y$
% \mbox{$Y = \{\mathbf{A} \mathbf{x}_1, \ldots, \mathbf{A}\mathbf{x}_N\}$} 
has a stable $k$-sparse representation in $m$ dimensions and fix $\delta_1, \delta_2$ to be the desired recovery accuracy in \eqref{def1}. Consider now any dataset $Z$ generated as in \eqref{LinearModel} with $\eta \leq \frac{1}{2} \varepsilon(\delta_1, \delta_2)$. Then from the triangle inequality, it follows that any matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$ solving Prob.~\ref{InverseProblem} are necessarily close to the original dictionary $\mathbf{A}$ and sparse codes $\mathbf{x}_i$. 

%Our main technical finding is that a sufficient quantity of measurements generated from any dictionary satisfying the spark condition has a stable sparse representation.  More specifically, such dictionaries are identifiable from as few as \mbox{$N = m(k-1){m \choose k} + m$} sparse linear combinations of their columns up to an error that is linear in the measurement noise (Thm.~\ref{DeterministicUniquenessTheorem}). In fact, provided $n \geq k$, in almost all cases the dictionary learning problem is well-posed (in the sense of Hadamard \cite{Hadamard1902}) given enough data (Cor.~\ref{ProbabilisticCor}). Importantly, these guarantees hold without assuming the recovered matrix satisfies a spark condition. Our results  also apply when only an upper bound on the number of dictionary elements is known. The explicit, algorithm-independent criteria we provide should be a useful tool in the theory of dictionary learning.  

In the next section, we give precise statements of our main results, which include an explicit form for $\varepsilon(\delta_1, \delta_2)$. We then prove our main theorem (Thm.~\ref{DeterministicUniquenessTheorem}) in Sec.~\ref{DUT} after stating some additional definitions and lemmas required for the proof, including a useful result in combinatorial matrix analysis (Lem.~\ref{MainLemma}). We also give a simple argument extending our guarantees to the following more common optimization formulation of the dictionary learning problem (Cor.~\ref{SLCopt}).

\begin{problem}\label{OptimizationProblem}
Find a matrix $\mathbf{B} \in \mathbb{R}^{\bar m}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{\bar m}$ that solve:
\begin{align}\label{minsum}
\min \sum_{i = 1}^N \|\mathbf{\bar x}_i\|_0 \ \ 
\text{subject to} \ \ \|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon, \ \text{for all $i$}.
\end{align}
\end{problem}

All other proofs are relegated to the Supplemental. 
Finally, we present several applications in Discussion Sec.~\ref{Discussion}. 

\section{Results}\label{Results}

Before stating our main results, we first identify combinatorial criteria on the support sets of generating codes that demand stable sparse representations.  Letting $\{1, \ldots, m\}$ be denoted $[m]$, its power set $2^{[m]}$, and ${[m] \choose k}$ the set of subsets of $[m]$ of size $k$,
% Let $\text{\rm Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_\ell\}$ be the $\mathbb{R}$-linear span of vectors $\mathbf{v}_1, \ldots, \mathbf{v}_\ell$. 
we say a hypergraph $E \subseteq 2^{[m]}$ on vertices $[m]$ is \textit{$k$-uniform} when in fact $E \subseteq {[m] \choose k}$. We also say $E$ is \emph{regular} when every element of $[m]$ is contained in exactly $\ell$ elements of $E$ for some $\ell > 0$ (for given $\ell$, we say $E$ is \textit{$\ell$-regular}).

\begin{definition}\label{sip}
Given $E \subseteq 2^{[m]}$, the \textbf{star} $F(i)$ at $i$ is the set of $S \in E$ with $i \in S$. We say $E$ has the \textbf{singleton intersection property} (\textbf{SIP}) when $\cap F(i) = \{i\}$ for all $i \in [m]$. 
\end{definition}

It is easy to verify that for every $k < m$, there is a regular $k$-uniform hypergraph that satisfies the SIP; for instance, consecutive intervals of length $k$ in some cyclic order on $[m]$.
%\begin{proposition}
%For every $k < m$, there is a regular $k$-uniform hypergraph that satisfies the singleton intersection property.
%\end{proposition}

%The second ingredient necessary for the statement of Thm.~\ref{DeterministicUniquenessTheorem} 
Next, we describe a quantitative version of the spark condition.  % We first explain how the spark condition \eqref{SparkCondition} relates to t
The \emph{lower bound} \cite{Grcar10} of a matrix $\mathbf{A} \in \mathbb R^{n \times m}$ is the largest number $\alpha$ such that \mbox{$\|\mathbf{A}\mathbf{x}\|_2 \geq \alpha\|\mathbf{x}\|_2$} for all $\mathbf{x} \in \mathbb{R}^m$. By compactness of the unit sphere, every injective linear map has a nonzero lower bound; hence, if $\mathbf{A}$ satisfies \eqref{SparkCondition}, then each submatrix formed from $2k$ of its columns or less has a nonzero lower bound. This motivates the following definition: % We therefore define the following domain-restricted lower bound:
\begin{align*}
L_k(\mathbf{A}) := \frac{1}{\sqrt{k}}\max \{ \alpha : \|\mathbf{A}\mathbf{x}\|_2 \geq \alpha\|\mathbf{x}\|_2 \text{ for all $k$-sparse } \mathbf{x}\}.
\end{align*} 

%Clearly, $\sqrt{k} L_k(\mathbf{A}) \geq \sqrt{k'}L_{k'}(\mathbf{A})$ whenever $k < k'$, and 
Clearly, for any $\mathbf{A}$ satisfying \eqref{SparkCondition}, we have $L_{k'}(\mathbf{A}) > 0$ for  $k' \leq 2k$. We note that the quantity $1 - \sqrt{k} L_k(\mathbf{A})$ is also known in the CS literature as the (asymmetric) lower restricted isometry constant \cite{Blanchard2011}.%, Foucart2009}.

The following theorem is the precise statement of our main result. 
A vector $\mathbf{x}$ is said to be \emph{supported} on $S \subseteq [m]$ %,
when $\mathbf{x} \in \text{\rm Span} \{\mathbf{e}_j\}_{j\in S}$, where $\mathbf{e}_j$ are the standard basis in $\mathbb R^m$. Denote by $\mathbf{M}_S$ the submatrix formed by the columns of $\mathbf{M}$ indexed by $S$, where we set $\text{\rm Span}\{\mathbf{M}_\emptyset\} := \{\textbf{0}\}$, and similarly by $\mathbf{x}^J$ the subvector formed from the entries of $\mathbf{x}$ indexed by $J$. We say that a set of $k$-sparse vectors is in \emph{general linear position} when any $k$ of them are linearly independent.

\begin{theorem}\label{DeterministicUniquenessTheorem}
%Fix integers $n, \ell$, and $k < m$. 
Let $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ satisfy \eqref{SparkCondition} for $k < m$, and suppose for some $\bar m$ that more than \mbox{$(k-1){\bar m \choose k}$} $k$-sparse \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} are supported in general linear position on each set in an $\ell$-regular $E \subseteq {[m] \choose k}$ with the SIP.  There is a constant\footnote{We delay defining the constant $C_1$ until Section \ref{DUT} (\eqref{Cdef1}).} $C_1 > 0$ for which the following holds for $\varepsilon < L_2(\mathbf{A}) / C_1$.

Every matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{\bar m}$ with  \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$} for all $i$ must have $\bar m \geq m$ and, provided $p = \bar m - \ell(\bar m - m)$ is positive, necessarily satisfy:
\begin{align}\label{Cstable}
\|(\mathbf{A}_J- \mathbf{B}_{\bar J} \mathbf{PD} )_j\|_2 \leq C_1 \varepsilon, \ \  \text{for $j \in J$},
\end{align}
for some $J \in {[m] \choose p}$, $\bar J \in {[\bar m] \choose p}$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.

Furthermore, if $\varepsilon < L_{2k}(\mathbf{A}) / C_1$, then $\mathbf{B}_{\bar J}$ also satisfies \eqref{SparkCondition} with $L_{2k}(\mathbf{B}_{\bar J}\mathbf{PD}) \geq L_{2k}(\mathbf{A}_J) - C_1 \varepsilon$, and:
\begin{align}\label{b-PDa}
\|\mathbf{x}_i^J - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i^{\bar J}\|_1 &\leq  \left( \frac{ 1+C_1 \|\mathbf{x}_i^{J}\|_1 }{ L_{2k}(\mathbf{A}_J) -  C_1\varepsilon } \right) \varepsilon,\ \ \   \text{for $i \in [N]$}.
\end{align}
\end{theorem}

%\begin{theorem}\label{DeterministicUniquenessTheorem}
%Fix integers $n, \ell$, and $k < m$. Suppose $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ satisfies the spark condition \eqref{SparkCondition} and that $k$-sparse \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} contain at least \mbox{$(k-1){m \choose k}+1$} vectors supported in general linear position on each set in some $\ell$-regular $E \subseteq {[m] \choose k}$ with the SIP.  There exists a constant $C_1 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_1$.

%Every matrix $\mathbf{B} \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{m}$ with  \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$} for all $i$ necessarily satisfies:
%\begin{align}\label{Cstable}
%\|\mathbf{A}_j - (\mathbf{B}\mathbf{PD})_j\|_2 \leq C_1 \varepsilon,\ \ \   \text{for all $j \in [m]$},
%\end{align}
%for some permutation matrix $\mathbf{P}$ and invertible diagonal $\mathbf{D}$.

%Furthermore, if $\varepsilon < L_{2k}(\mathbf{A}) / C_1$, then $\mathbf{B}$ also satisfies \eqref{SparkCondition} with $L_{2k}(\mathbf{B}\mathbf{PD}) \geq L_{2k}(\mathbf{A}) - C_1 \varepsilon$, and:
%\begin{align}\label{b-PDa}
%\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i\|_1 &\leq  \left( \frac{ 1+C_1 \|\mathbf{x}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right) \varepsilon,\ \ \   \text{for all $i \in [N]$}.
%\end{align}
%\end{theorem}

%\begin{theorem}\label{DeterministicUniquenessTheorem}
%Fix integers $n, \ell$, and $k < m \leq \bar m$. Suppose $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ satisfies the spark condition \eqref{SparkCondition} and that $k$-sparse \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} contain at least \mbox{$(k-1){\bar m \choose k}+1$} vectors supported in general linear position on each set in some $\ell$-regular $E \subseteq {[m] \choose k}$ satisfying the SIP.  There exists a constant $C_1 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_1$.
%
%Every matrix $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{\bar m}$ with  \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$} for all $i$ necessarily satisfies:
%\begin{align}\label{Cstable}
%\|\mathbf{A}_{\phi(j)} - (\mathbf{B}_{J}\mathbf{PD})_j\|_2 \leq C_1 \varepsilon,\ \ \   \text{for $j = 1, \ldots, |J|$},
%\end{align}
%%
%for some $J \subseteq [\bar m]$ of size \mbox{$\lfloor \bar m - \ell(\bar m - m) \rfloor$}, injective map $\phi$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.
%%\footnote{We note that the condition $\varepsilon < L_2(\mathbf{A})$ above is necessary. When \mbox{$\mathbf{A}$ = $I$} and $\mathbf{x}_i = \mathbf{e}_i$, it turns out that $C_1 = 1$ and there is a $\mathbf{B}$ and $1$-sparse $\mathbf{\bar x}_i$ with $\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$ violating \eqref{Cstable}.}
%
%Furthermore, if $\varepsilon < L_{2k}(\mathbf{A}) / C_1$, then $\mathbf{B}_J$ also satisfies \eqref{SparkCondition} with $L_{2k}(\mathbf{B}_J\mathbf{PD}) \geq L_{2k}(\mathbf{A}) - C_1 \varepsilon$, and:
%\begin{align}\label{b-PDa}
%\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i\|_1 &\leq  \left( \frac{ 1+C_1 \|\mathbf{x}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right) \varepsilon,\ \ \   \text{for all $i \in [N]$}.
%\end{align}
%\end{theorem}

In other words, the smaller the difference $\bar m - m$, the more columns and coefficients of the original dictionary $\mathbf{A}$ and codes $\mathbf{x}_i$ contained (up to noise) in the appropriately scaled dictionary $\mathbf{B}$ and codes $\mathbf{\bar x}_i$. 
%In particular, when $\bar m = m$ all columns of $\mathbf{A}$ and coefficients of each $\mathbf{x}_i$ recoverable, as per Def.~\ref{maindef}. 
The implication in the case $\bar{m} = m$ is precisely that $Y = \{\mathbf{Ax}_1, \ldots, \mathbf{Ax}_N\}$ has a stable $k$-sparse representation in $m$ dimensions, with \eqref{def1} guaranteed provided $\varepsilon$ in Def.~\ref{maindef} does not exceed: 
\begin{align}\label{epsdel}
\varepsilon(\delta_1, \delta_2) := \min \left\{ \frac{\delta_1}{ C_1 }, \frac{ \delta_2 L_{2k}(\mathbf{A})}{ 1 + C_1 \left( \max_{i \in [N]} \|\mathbf{x}_i\|_1  + \delta_2 \right) } \right\}.
\end{align}
  % , since it requires additional definitions not critical to the statement of our results.


% Note also that Thm~\ref{DeterminsiticUniquenessTheorem} gives conditions for when a single solution to Problem~\ref{InverseProblem} in fact represents \emph{all} possible solutions, related by transformations of the form $PD$.

In fact, it is straightforward to produce sparse codes in general linear position with a ``Vandermonde'' matrix construction (e.g., see \cite{Hillar15}). We therefore have:

\begin{corollary}\label{DeterministicUniquenessCorollary}
Given $n, m$, $k < m$, and a regular hypergraph $E \subseteq {[m] \choose k}$ with the SIP, there are $N =  |E| \left[ (k-1){m \choose k} + 1  \right]$ vectors \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} such that every matrix $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} generates a dataset $Y = \{\mathbf{A}\mathbf{x}_1, \ldots, \mathbf{A}\mathbf{x}_N\}$ with a stable $k$-sparse representation in $m$ dimensions.
\end{corollary}

%\begin{proposition}
%If $E$ is an $\ell$-regular hypergraph of rank $k$ satisfying the singleton intersection property then $|E| \geq \ell m/k$. 
%\end{proposition}
%Note that every $\ell$-regular hypergraph $E \subseteq {[m] \choose k}$ satisfies: \[|E|k = \sum_{S \in E}|S| = \sum_{i \in [m]} \deg(i) = \ell m.\] 
As mentioned above, it is easy to construct a regular hypergraph $E \subseteq {[m] \choose k}$ with the SIP having cardinality $|E| = m$, implying the lower bound for sample size $N$ from the introduction. In many cases, however, the SIP can be achieved with far fewer supports; for example, when $k = \sqrt{m}$ one can take $E$ to be the $2k$ rows and columns formed by arranging $[m]$ in a square grid.  
%In light of Cor.~\ref{DeterministicUniquenessCorollary}, it would be interesting to determine other such $E$. %[ ** TODO ** can we show for every $k, m$ that there is an $E$ satisfying the SIP with $|E| = \ell m/k$ for every $\ell$ such that $\ell m/k$ is an integer? Then lower bound on $N$ comes from the smallest such $\ell$.]

It turns out that Thm.~\ref{DeterministicUniquenessTheorem} also has implications for the optimization problem posed in Prob.~\ref{SLCopt}:

\begin{corollary}\label{SLCopt}
If all of the assumptions of Thm.\ref{DeterministicUniquenessTheorem} hold with more than $(k-1)\left[ {\bar m \choose k} + |E|k{m \choose k-1}\right]$ vectors $\mathbf{x}_i$ supported on each $S \in E$, then all solutions to Prob.~\ref{OptimizationProblem} necessarily satisfy the implications \eqref{Cstable} and \eqref{b-PDa} of Thm.~\ref{DeterministicUniquenessTheorem}.
\end{corollary}
%\begin{corollary}\label{SLCopt}
%If the assumptions of Thm.\ref{DeterministicUniquenessTheorem} hold with more than $(k-1)\left[ {\bar m \choose k} + |E|k{\bar m \choose k-1}\right]$ vectors $\mathbf{x}_i$ supported on each $S \in E$, then all matrices $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ and $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{\bar m}$ solving:
%\begin{align}\label{minsum}
%\min \sum_{i = 1}^N \|\mathbf{\bar x}_i\|_0 \ \ 
%\text{subject to} \ \ \|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon, \ \text{for all $i$},
%\end{align}
%
%necessarily satisfy implications 1) and 2) from Thm.~\ref{DeterministicUniquenessTheorem}.

%must have $\bar m \geq m$ and, provided $p = \bar m - \ell(\bar m - m) > 0$, necessarily satisfy \eqref{Cstable}
%%\begin{align}\label{Cstable}
%%\|(\mathbf{A}_J- \mathbf{B}_{\bar J} \mathbf{PD} )_j\|_2 \leq C_1 \varepsilon, \ \  \text{for  $j \in J$},
%%\end{align}
%for some $J \in {[m] \choose p}$, $\bar J \in {[\bar m] \choose p}$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.
%\end{corollary}

%\begin{corollary}
%Fix $n, \ell$, and $k < m$. Let $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ satisfy the spark condition \eqref{SparkCondition} and $k$-sparse \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} have at least \mbox{$\left[ (k-1){ m \choose k} + 1 \right] + (k-1)^2{m \choose k-1}$} vectors supported in general linear position on each set in an $\ell$-regular $E \subseteq {[m] \choose k}$ with the SIP. Given any $\delta_1, \delta_2 \geq 0$ and $\varepsilon \leq \varepsilon(\delta_1, \delta_2)$,
%every solution $\mathbf{B}, \mathbf{\bar x}_i, \ldots, \mathbf{\bar x}_N$ to the $\ell_0$ optimization problem:
%\begin{align}\label{minsum}
%\arg \min \sum_{i = 1}^N \|\mathbf{\bar x}_i\|_0 \ \ \text{s.t.} \ \ \mathbf{B}\mathbf{\bar x}_i = \mathbf{A}\mathbf{x}_i, \ \ \text{$i \in [N]$}, %\|\mathbf{A}\end{align}
%necessarily satisfies the recovery inequalities in \eqref{def1}. % \eqref{Cstable} and \eqref{b-PDa}.
%\end{corollary}
%
%\begin{proof}
%We derive a lower bound on the number of $k$-sparse $\mathbf{\bar x}_i$ and then apply Thm.~\ref{DeterministicUniquenessCorollary}. 
%First, note that $\|\mathbf{\bar x}_i\|_0 \neq 0$ for all $i$, since otherwise we would have $\mathbf{A}\mathbf{x}_i = \mathbf{B}\mathbf{\bar x}_i = \mathbf{0} \implies \mathbf{x}_i = \mathbf{0}$ by the spark condition, contradicting the general linear position of the $\mathbf{x}_i$. Let $n_p$ be the number of $\mathbf{\bar x}_i$ with $\|\mathbf{\bar x}_i\|_0 = p$.  %, so that $\sum_{p = 1}^{m} n_p = N$. 
%Since the $\mathbf{x}_i$ are all $k$-sparse, by \eqref{minsum} we have:
%\begin{align}
%k \sum_{p = 1}^{m} n_p = kN \geq \sum_{i=1}^N \|\mathbf{x}_i\|_0 \geq \sum_{i=1}^N \|\mathbf{\bar x}_i\|_0 = \sum_{p=1}^{m} p n_p.
%\end{align}
%%\[\implies \sum_{p = 1}^k (k-p)n_p \geq \sum_{p = k+1}^{m} (p-k) n_p.\]
%Hence,
%\begin{align}
%\sum_{p = k+1}^m n_p \leq \sum_{p = k+1}^m (p-k) n_p \leq \sum_{p = 1}^k (k-p)n_p \leq (k-1) \sum_{p = 1}^{k-1} n_p.
%\end{align}
%%\[ \implies \sum_{p = k+1}^m n_p \leq (k-1)^2 {\bar m \choose k-1} \]
%
%By the spark condition and general linear position of the $\mathbf{x}_i$, every $k$ vectors $\mathbf{Ax}_i$ span a $k$-dimensional space; hence, at most $k-1$ vectors $\mathbf{\bar x}_i$ can share a support of size $k-1$ or less, and the number of $(k-1)$-sparse vectors $\mathbf{\bar x}_i$ is then bounded from above by $(k-1) { m \choose k-1}$. It follows that at least $N - (k-1)^2 {m \choose k-1}$ of the vectors $\mathbf{\bar  x}_i$ are $k$-sparse, and that there must then be some $N' \subseteq [N]$ for which for each $S \in E$ there are $(k-1){m \choose k}+1$ of the vectors $\mathbf{x}_i$ with $i \in [N']$ supported on $S$ and for which $\mathbf{\bar x}_i$ is $k$-sparse for all $i \in [N']$.  The result now follows directly from Thm.~\ref{DeterministicUniquenessCorollary}. % \eqref{Cstable} and \eqref{b-PDa} follow.
%\end{proof}

%Another straightforward application of Thm.~\ref{DeterministicUniquenessTheorem} is a probabilistic extension, which takes advantage of the following well-known application of random matrix theory to compressed sensing.  A random $n \times m$ matrix obeys \eqref{SparkCondition} with probability one (or ``high probability'' for discrete variables) provided:
%\begin{align}\label{CScondition}
%n \geq \gamma k\log\left(\frac{m}{k}\right),
%\end{align}
%in which $\gamma >0$ is a constant that depends on the particular distribution from which the entries of $\mathbf{A}$ are sampled i.i.d. (many ensembles suffice, e.g. see \cite[Sec.~4]{Baraniuk08}). 

Another straightforward extension of Thm.~\ref{DeterministicUniquenessTheorem} applies to probabilistic scenarios and follows from the following analytic characterization of the spark condition.  Let $\mathbf{A}$  be the $n \times m$ matrix of $nm$ indeterminates $A_{ij}$. When real numbers are substituted for $A_{ij}$, the resulting matrix satisfies \eqref{SparkCondition} if and only if the following polynomial is nonzero:
\begin{align*}
f(\mathbf{A}) := \prod_{S \in {[m] \choose k}} \sum_{S' \in {[n] \choose k}} (\det \mathbf{A}_{S',S})^2,
\end{align*}
%
where for any $S' \in {[n] \choose k}$ and $S \in {[m] \choose k}$, the symbol $\mathbf{A}_{S',S}$ denotes the submatrix of entries $A_{ij}$ with $(i,j) \in S' \times S$.   We note that the large number of terms in this product is likely necessary due to the NP-hardness of deciding whether a given matrix $A$ satisfies the spark condition \cite{tillmann2014computational}.

Since $f$ is an analytic function, having one substitution of real numbers with $f(\mathbf{A}) \neq 0$ implies that its zeroes form a set of measure zero. Fortunately, such a matrix $\mathbf{A}$ is easily constructed by adding rows of zeroes to any $\min(2k,m) \times m$ ``Vandermonde'' matrix $[\gamma_i^j]_{i,j=1}^{k,m}$ with distinct $\gamma_i$ (so that each term in the product above is nonzero).
%$A_{ij} = \gamma_i (since the $\gamma_i$ are distinct, any $k \times k$ ``Vandermonde'' sub-determinant is nonzero)
%inserting $n-k$ rows of zeros into a $k \times m$ ``Vandermonde" matrix construction. %Let $\gamma_1, \ldots, \gamma_m$ be any distinct numbers. Then the columns of the $k \times m$ matrix $V = (\gamma^{\ell}_i)^{k,m}_{\ell,i=1}$ are in general linear position (since the $\gamma_i$ are distinct, any $k \times k$ ``Vandermonde" sub-determinant is nonzero). Concatenating $n-k$ rows of zeros to the bottom of this matrix yields a $n \times m$ matrix $\mathbf{A}$ satisfying the spark condition, i.e. with $f(\mathbf{A}) = 0$.
Hence, almost every real $n \times m$ matrix $\mathbf{A}$ with $n \geq \max(2k,m)$ satisfies \eqref{SparkCondition}.

A similar phenomenon applies to datasets of vectors with a stable sparse representation. As in \cite[Sec.~IV]{Hillar15}, consider the ``symbolic'' dataset $Y = \{\mathbf{A}\mathbf{x}_1,\ldots,\mathbf{A} \mathbf{x}_N\}$ generated by indeterminate $\mathbf{A}$ and indeterminate $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N$.  

\begin{theorem}\label{robustPolythm}
There is a polynomial $g$ in the entries of $\mathbf{A}$ and $\mathbf{x}_i$ with the following property:  if $g$ evaluates to a nonzero number and more than \mbox{$(k-1){m \choose k}$} of the resulting $\mathbf{x}_i$ are supported on each $S \in E$ for some regular $E \subseteq {[m] \choose k}$ with the SIP, then $Y$ has a stable $k$-sparse representation in $m$ dimensions (Def.~\ref{maindef}). In particular, either no substitutions impart to $Y$ this property or all but a Borel set of measure zero do.
\end{theorem}

\begin{corollary}\label{ProbabilisticCor}
Fix $k < m$ and $n \geq \min(2k, m)$ % satisfying \eqref{CScondition} for $\gamma = \gamma_0$, 
and let the entries of $\mathbf{A} \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ be drawn independently from probability measures absolutely continuous with respect to the standard Borel measure $\mu$. If more than $(k-1){m \choose k}$ of the vectors $\mathbf{x}_i$ are supported on each $S \in E$ for a regular $E \subseteq {[m] \choose k}$ with the SIP, then $Y$ has a stable $k$-sparse representation in $m$ dimensions with probability one.
\end{corollary}

Thus, choosing dictionary and sparse codes ``randomly'' from \emph{any} continuous distribution generates data with stable sparse representations almost certainly.
% , as long as the relevant parameters satisfy \eqref{CScondition}.

%Next, we address the case when only an upper bound $m'$ on the latent dimension $m$ is known (assuming that $\mathbf{B}$ satisfies \eqref{SparkCondition}).

%\begin{theorem}\label{DeterministicUniquenessTheorem2}
%Fix integers $n$ and $k < m \leq m'$ and matrices $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ and $\mathbf{B} \in \mathbb{R}^{n \times m'}$ both satisfying \eqref{SparkCondition}. Suppose \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} include at least \mbox{$(k-1){m' \choose k}+1$} $k$-sparse vectors in general linear position supported on each set in some $k$-uniform $E \subseteq 2^{[m]}$ satisfying the singleton intersection property. Then there exists a constant $C_3 > 0$ for which the following holds.

%If for some $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{m'}$ and $\varepsilon < L_2(\mathbf{A}) / C_3$ we have \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$} for all $i$, then:
%\begin{align}\label{Cstablem'}
%\|\mathbf{A}_j-(\mathbf{B}_J\mathbf{PD})_j\|_2 \leq C_3\varepsilon \ \ \text{for all $j \in [m]$}
%\end{align}
%
%for some $J \in {[m'] \choose m}$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.
%\end{theorem}

%In other words, the columns of $B$ contain (up to noise, after appropriate scaling) the columns of the original dictionary $\mathbf{A}$. Similarly, one can show by the same arguments at the beginning of Sec.~\ref{DUT} that the $\mathbf{\bar x}_i$ contain the original codes $\mathbf{x}_i$. The constant $C_3$ here is expression (\ref{Cdef2}) from the proof of Thm.~\ref{DeterministicUniquenessTheorem2}. Note that, in contrast to Thm.~\ref{DeterministicUniquenessTheorem}, this constant is dependent on $\mathbf{B}$; hence, \eqref{Cstablem'} holds for \emph{all} matrices $\mathbf{B} \in \mathbb{R}^{n \times m'}$ satisfying the spark condition only in the case $\varepsilon = 0$. 

\section{Proofs of Theorem~\ref{DeterministicUniquenessTheorem} and Corollary~\ref{SLCopt}}\label{DUT}

% ======== b - PDa =========
First note that $\|\mathbf{x}\|_1 \leq \sqrt{k} \|\mathbf{x}\|_2$ for $k$-sparse $\mathbf{x} \in \mathbb{R}^m$, which by definition of $L_k(A)$ implies the following often used inequality:
\begin{align}\label{delrho}
L_k(\mathbf{A}) \leq \frac{\|\mathbf{A}\mathbf{x}\|_2}{\sqrt{k} \|\mathbf{x}\|_2} %\leq \frac{\|\mathbf{x}\|_1}{\sqrt{k} \|\mathbf{x}\|_2} \max_{i \in [m]}\|\mathbf{A}_i\|_2 
\leq  \max_{j \in [m]}\|\mathbf{A}_j\|_2.
\end{align}

Our first step is to show how dictionary recovery \eqref{Cstable} already implies sparse code recovery \eqref{b-PDa} when \mbox{$\varepsilon < L_{2k}(\mathbf{A}) / C_1$}. For $2k$-sparse $\mathbf{x} \in \mathbb{R}^m$, the triangle inequality gives \mbox{$\|(\mathbf{A}-\mathbf{BPD})\mathbf{x}\|_2  \leq C_1\varepsilon \|\mathbf{x}\|_1 \leq C_1 \varepsilon \sqrt{2k}  \|\mathbf{x}\|_2$}. Thus, 
\begin{align*}
\|\mathbf{BPD}\mathbf{x}\|_2 
&\geq | \|\mathbf{A}\mathbf{x}\|_2 - \|(\mathbf{A}-\mathbf{BPD})\mathbf{x}\|_2 | \\
&\geq \sqrt{2k} (L_{2k}(\mathbf{A}) -  C_1\varepsilon) \|\mathbf{x}\|_2.
\end{align*}
%
%where we drop the absolute value due to the upper bound on $\varepsilon$. 
Hence, $L_{2k}(\mathbf{BPD}) \geq L_{2k}(\mathbf{A}) - C_1\varepsilon  > 0$ and \eqref{b-PDa} then follows from:
\begin{align*}
\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i \|_1
%&\leq \sqrt{2k} |\mathbf{x}_i - D^{-1}P^{\top}\mathbf{\bar x}_i\|_2 \\
&\leq \frac{\|\mathbf{BPD}(\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i)\|_2}{L_{2k}(\mathbf{BPD})} \\
&\leq \frac{\|\mathbf{B}\mathbf{\bar x}_i - \mathbf{A}\mathbf{x}_i\|_2 + \|(\mathbf{A} - \mathbf{BPD})\mathbf{x}_i\|_2}{L_{2k}(\mathbf{BPD})} \\
&\leq \frac{\varepsilon (1+C_1 \|\mathbf{x}_i\|_1)}{L_{2k}(\mathbf{BPD})}.
%&\leq \frac{\varepsilon}{\sqrt{2k}} \left( \frac{ 1+C_1 \|\mathbf{x}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right).
%\leq \frac{\varepsilon }{\varepsilon_0 - \varepsilon} \left( C_1^{-1}+|\mathbf{x}_i\|_1 \right).
\end{align*}

The heart of the matter is therefore \eqref{Cstable}, which we now finally establish, but first in the important special case $k = 1$.

%In fact, in the case $k=1$ we can relax our assumptions even further and assume that the matrix $\mathbf{B}$ has only at least as many columns as $\mathbf{A}$; as we will see, the conclusions of Thm.~\ref{DeterministicUniquenessTheorem} then hold for some $n \times m$ submatrix of $\mathbf{B}$.  [*** ??? ***]

\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessCorollary} for $k=1$]
The only 1-uniform hypergraph with the SIP is $[m]$; hence, we have $\mathbf{x}_i = c_i \mathbf{e}_i$ for $c_i \in \mathbb{R} \setminus \{\mathbf{0}\}$, $i \in [m]$. In this case, we may take $C_1 = 1/ \min_{\ell \in [m]} |c_{\ell}|$. 

Fix $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} and suppose that for some $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ and $1$-sparse $\mathbf{\bar x}_i \in \mathbb{R}^{\bar m}$ we have  $\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon < L_2(\mathbf{A}) / C_1$ for all $i$. Then, there are $\bar{c}_1, \ldots, \bar{c}_m \in \mathbb{R}$ and a map $\pi: [m] \to [\bar m]$ with:
\begin{align}\label{1D}
\|c_j\mathbf{A}_j - \bar{c}_j\mathbf{B}_{\pi(j)}\|_2 \leq \varepsilon,\ \ \text{for $j \in [m]$}.
\end{align} 
Note that if $\bar{c}_j = 0$, then $\|c_j\mathbf{A}_j \|_2 \leq \varepsilon$ implies from \eqref{delrho} that $|c_j| < \min_{\ell \in [m]} | c_\ell |$, a contradiction.  Thus, $\bar{c}_j \neq 0$, $j \in [m]$.
%Note that $\bar{c}_j \neq 0$, since otherwise (by definition of $L_2(\mathbf{A})$) we reach the contradiction $|c_j| < \min_{\ell \in [m]} | c_\ell |$. %by \eqref{delrho} and definition of $C_1$ we would have $\|c_j\mathbf{A}_j\|_2 < \min_{\ell \in [m]}\|c_{\ell}\mathbf{A}_{\ell}\|_2$.  [*** TODO:  This doesn't seem correct ***]
%\begin{align*}
%|c_j| \sqrt{2} L_2(\mathbf{A}) \leq \|\mathbf{A}(c_j\mathbf{e}_j)\|_2 \leq \varepsilon < L_2(\mathbf{A}) \min_{\ell \in [m]} | c_\ell |.
%\end{align*}

We  now show that $\pi$ is injective (and thus is a permutation if $\bar m = m$). Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell$. Then, $\|c_{j}\mathbf{A}_{j} - \bar{c}_{j}\mathbf{B}_{\ell}\|_2 \leq \varepsilon$ and $\|c_{i}\mathbf{A}_{i} - \bar{c}_{i} \mathbf{B}_{\ell}\|_2  \leq \varepsilon$. Scaling and summing these inequalities by $|\bar{c}_{i}|$ and $|\bar{c}_{j}|$, respectively, and applying the triangle inequality, we obtain:
\begin{align*}%\label{contra}
(|\bar{c}_{i}| + |\bar{c}_{j}|) \varepsilon
&\geq\|\mathbf{A}(\bar{c}_{i}c_{j} \mathbf{e}_{j} - \bar{c}_{j}c_{i}\mathbf{e}_{i})\|_2 \nonumber \\ 
&\geq  \left( |\bar{c}_{i}| + |\bar{c}_{j}| \right) L_2(\mathbf{A}) \min_{\ell \in [m]} |c_\ell |,
\end{align*}
%
which contradicts the bound $\varepsilon < L_2(\mathbf{A})/C_1$. Hence, the map $\pi$ is injective and $\bar m \geq m$. Setting $J = [m]$, $\bar J = \pi([m])$, and letting $P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}\right)$ and $D = \text{diag}(\frac{\bar{c}_1}{c_1},\ldots,\frac{\bar{c}_m}{c_m})$, we see that \eqref{1D} becomes, for all $j \in [m]$:
\begin{align*}%\label{k=1result}
\|\mathbf{A}_j - (\mathbf{B}_J\mathbf{PD})_j\|_2 
= \|\mathbf{A}_j - \frac{\bar{c}_j}{c_j}\mathbf{B}_{\pi(j)}\|_2 
\leq \frac{\varepsilon}{|c_j|} 
\leq C_1\varepsilon.
\end{align*}
\end{proof}

%\begin{remark}
%The above arguments can be easily modified to show that the conclusions of Thm.~\ref{DeterministicUniquenessTheorem} in this case ($k=1$) hold for some $n \times m$ submatrix of $\mathbf{B}$ in the event where only an upper bound on the number of columns in $\mathbf{A}$ is known; i.e., the recovered matrix $\mathbf{B}$ is set to have as many or more columns than $\mathbf{A}$. 
%\end{remark}

%It is easy to see that when $m < m'$, the above result holds for the submatrix of $B$ composed of columns indexed by the image of $\pi$.

We require a few additional tools to extend the proof to the general case $k < m$. These include a general notion of distance (Def.~\ref{dDef}) and angle (Def.~\ref{FriedrichsDefinition}) between subspaces and a stability result in matrix analysis (Lem.~\ref{MainLemma}).

\begin{definition}\label{dDef}
For $\mathbf{u} \in \mathbb R^m$, let $\text{\rm dist}(\mathbf{u}, V) := \inf \{\| \mathbf{u}-\mathbf{v} \|_2: \mathbf{v} \in V\}$, and for vector spaces $U,V \subseteq \mathbb{R}^m$, define:
%\footnote{Although identical, note the reference \cite{Morris10} defines the supremum over the unit ball.}: %[Kato p.197]
\begin{align}\label{d}
d(U,V) := \sup_{\mathbf{u} \in U, \ \|\mathbf{u}\|_2 \leq 1} \text{\rm dist}(\mathbf{u},V).
\end{align}
% Note that $d(U,\{\textbf{0}\}) = 1$ for $U \neq \{\mathbf{0}\}$, whereas if $U = \{\textbf{0}\}$ then $d$ has no meaning; in this case, set $d(\{\textbf{0}\},V) = 0$ for any $V$.
\end{definition}
%Note that the supremum is always achievable by a point in $U$ when $U, V \subseteq \mathbb{R}^m$.

We note the following facts about $d$. For subspaces $U \subseteq U', V \subseteq \mathbb{R}^m$, we have $d(U,V) \leq d(U',V)$ and \cite[Cor.~2.6]{Kato2013}:
\begin{align}\label{dimLem}
d(U,V) < 1 \implies \dim(U) \leq \dim(V).
\end{align}
Also, from \cite[Lem.~3.2]{Morris10}, we have:
\begin{align}\label{eqdim}
\dim(U) = \dim(V) \implies d(U,V) = d(V,U).
\end{align}

%\begin{lemma}\label{dimLem} %Kato p.200, 
%\cite[Cor.~2.6]{Kato2013} For subspaces $U,V \subseteq \mathbb{R}^m$, $d(U,V) < 1$ implies $\dim(U) \leq \dim(V)$. 
%\end{lemma}

%\begin{lemma}\label{eqdim}
%\cite[Lem.~3.2]{Morris10} If $\dim(U) = \dim(V)$ then $d(U,V) = d(V,U)$. 
%\end{lemma}

Our result in combinatorial matrix theory is the following.

%, which we derive by the following arguments (see the Supplemental for the full proof). First, we show that the aforementioned proximity between $k$-dimensional subspaces implies a proximity between smaller subspaces spanned by columns of $\mathbf{A}$ indexed by the intersections of sets in $E$ and those spanned by as many or fewer columns of $\mathbf{B}$. Another pigeonholing argument here combined with our assumptions on $E$ (e.g., the singleton intersection property) reveals that, in fact, each column of $\mathbf{A}$ spans a subspace proximal to that spanned by some column of $\mathbf{B}$. \eqref{Cstable} is a simple consequence of this last fact, which actually constitutes the proof of the theorem for the case $k=1$. % We present this special case now before restating the above arguments in greater detail.

\begin{lemma}\label{MainLemma}
%Suppose $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfies the spark condition \eqref{SparkCondition} for some $k < m$ and that $E \subseteq {[m] \choose k}$ is $\ell$-regular and satisfies the SIP. There is a constant $C_2 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_2$. 
Suppose $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfies \eqref{SparkCondition} for some $k < m$ and that $E \subseteq {[m] \choose k}$ is $\ell$-regular with the SIP. There exists $C_2 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_2$. 


If for some  $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ and map $\pi: E \mapsto {[\bar m] \choose k}$ we have:
\begin{align}\label{GapUpperBound}
d(\text{\rm Span}\{\mathbf{A}_{S}\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \leq \varepsilon, \ \  \text{for $S \in E$},
\end{align}
%
then $\bar m \geq m$, and if $p = \bar m - \ell(\bar m - m)$ is positive:
\begin{align}\label{MainLemmaBPD}
\|(\mathbf{A}_J - \mathbf{B}_{\bar J}\mathbf{PD})_j\|_2 \leq C_2 \varepsilon, \ \  \text{for } j \in J,
\end{align}
for some $J \in {[m] \choose p}$, $\bar J \in {[\bar m] \choose p}$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.
\end{lemma}

%\begin{lemma}\label{MainLemma}
%Fix integers $n$ and $k < m$, and suppose $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfies the spark condition \eqref{SparkCondition}. There exists a constant $C_2 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_2$. If for some  $\mathbf{B} \in \mathbb{R}^{n \times m}$ and regular $E \subseteq {[m] \choose k}$ with the SIP there exists a map $\pi: E \mapsto {[m] \choose k}$ satisfying:
%\begin{align}\label{GapUpperBound}
%d(\text{\rm Span}\{\mathbf{A}_{S}\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \leq \varepsilon,\ \ \   \text{for all $S \in E$},
%\end{align}
%
%then there exists a permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ with:
%\begin{align}\label{MainLemmaBPD}
%\|\mathbf{A}_j - (\mathbf{BPD})_j\|_2 \leq C_2 \varepsilon, \ \ \  \text{for all } j \in [m].
%\end{align}
%\end{lemma}

Given vectors $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$, we let $\mathbf{X}$ be the $m \times N$ matrix with columns $\mathbf{x}_i$ and $I(S) := \{i \in [N]: \text{supp}(\mathbf{x}_i) \subseteq S\}$. The constant $C_1$ in Thm.~\ref{DeterministicUniquenessTheorem} is then defined by\footnote{Note that $\|\mathbf{AX}_{I(S)}\mathbf{c}\|_2 \geq \sqrt{k} L_k(\mathbf{A})\|\mathbf{X}_{I(S)}\mathbf{c}\|_2 \geq k L_k(\mathbf{A}) L_k(\mathbf{X}_{I(S)})\|\mathbf{c}\|_2$ for $S \in E$ and $k$-sparse $\mathbf{c}$. Therefore, $\sqrt{k} L_k(\mathbf{AX}_{I(S)}) \geq k L_k(\mathbf{A}) L_k(\mathbf{X}_{I(S)}) > 0$ since $L_k(\mathbf{A}), L_k(\mathbf{X}_{I(S)}) > 0$ by \eqref{SparkCondition} and general linear position of the $\mathbf{x}_i$.  Thus, $C_1 > 0$.  }:
\begin{align}\label{Cdef1}
C_1(\mathbf{A}, \{\mathbf{x}_i\}_{i=1}^N, E) := \frac{ C_2(\mathbf{A}, E) } { \min_{S \in E} L_k(\mathbf{AX}_{I(S)}) }.
\end{align}

The constant $C_2 = C_2(\mathbf{A}, E)$ is given below in terms of one used in \cite{Deutsch12} to analyze the convergence of the alternating projections algorithm.
% for projecting a point onto the intersection of a set of subspaces. 
We use it to bound the distance between a point and the intersection of a set of subspaces given an upper bound on its distance from each subspace individually.

\begin{definition}\label{SpecialSupportSet}\label{FriedrichsDefinition}
For subspaces $V_1, \ldots, V_\ell \subseteq \mathbb{R}^m$, set $r := 1$ when $\ell = 1$ and define for $\ell \geq 2$:
\begin{align*}
r(\{V_i\}_{i=1}^\ell) := 1 - \left(1 -  \max \prod_{i=1}^{\ell-1} \sin^2  \theta \left(V_i, \cap_{j>i} V_j \right)  \right)^{1/2},
\end{align*} 
%
where the maximum is taken over all orderings\footnote{We modify the quantity in \cite{Deutsch12} in this way since the subspace ordering is irrelevant to our purpose.} of the $V_i$ and the angle $\theta \in (0,\frac{\pi}{2}]$ is defined implicitly as \cite[Def.~9.4]{Deutsch12}:
\begin{align*}
\cos{\theta(U,W)} := \max\left\{ |\langle \mathbf{u}, \mathbf{w} \rangle|: \substack{ \mathbf{u} \in U \cap (U \cap W)^\perp, \ \|\mathbf{u}\|_2 \leq 1 \\ \mathbf{w} \in W \cap (U \cap W)^\perp, \  \|\mathbf{w}\|_2 \leq 1 } \right\}.
\end{align*}
\end{definition}
Note that $\theta \in (0,\frac{\pi}{2}]$ implies $0 < r \leq 1$.\footnote{We acknowledge the counter-intuitive property that $\theta =  \pi/2$ when $U = V$ or $U \perp V$.}  %\textbf{Note:} we can also define $\theta$ by \cite[Lem.~9.5]{Deutsch12}:
%\begin{align*}
%\cos{\theta(U,W)} := \max\left\{ |\langle \mathbf{u}, \mathbf{w} \rangle|: \mathbf{u} \in U \cap (U \cap W)^\perp, \|\mathbf{u}\|_2 \leq 1, \mathbf{w} \in W \|\mathbf{w}\|_2 \leq 1  \right\}.
%\end{align*}
The constant $C_2$ in Lem.~\ref{MainLemma} can then be expressed as:  % \footnote{Note that $C_2 > 0$ is well-defined since $r > 0$ by definition.}
\begin{align}\label{Cdef2}
C_2(\mathbf{A}, E) := \frac{ 2^{|E|} \max_{j \in [m]} \|\mathbf{A}_j\|_2}{ \min_{F \subseteq E} r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F}) }.
\end{align}
Note that $k=1$ gives the same $C_1$ as that used to prove $k=1$.

\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessCorollary} for $k < m$] 
We shall show that for every $S \in E$ there is some $\bar S \in {[\bar m] \choose k}$, for which the distance $d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\bar S}\})$ is controlled by $\varepsilon$. Applying Lem.~\ref{MainLemma} with the map $\pi$ defined by $S \mapsto \bar S$ then completes the proof.  

Since there are $(k-1){\bar m \choose k}+1$ vectors $\mathbf{x}_i$ supported on $S$, the pigeonhole principle implies that there is some $\bar S \in {[\bar m] \choose k}$ and some set of $k$ indices $K$ such that the supports of all $\mathbf{x}_i$ and $\mathbf{\bar x}_i$ with $i \in K$ are contained in $S$ and $\bar S$, respectively.

%Let $\mathbf{X}$ and $\mathbf{\bar{X}}$ be the $m \times N$ matrices with columns $\mathbf{x}_i$ and $\mathbf{\bar x}_i$, respectively. 
It follows from the general linear position of the $\mathbf{x}_i$ and the linear independence of every $k$ columns of $\mathbf{A}$ that $L_k(\mathbf{AX}_{K}) > 0$; that is, the columns of the $n \times k$ matrix $\mathbf{AX}_K$ form a basis for $\text{\rm Span}\{\mathbf{A}_{S}\}$. Fixing $\mathbf{0} \neq \mathbf{y} \in \text{\rm Span}\{\mathbf{A}_{S}\}$, there exists $\mathbf{0} \neq \mathbf{c} = (c_1, \ldots, c_k) \in \mathbb{R}^k$ such that $\mathbf{y} = \mathbf{AX}_K\mathbf{c}$. Setting \mbox{$\mathbf{\bar{y}} = \mathbf{B\bar{X}}_K\mathbf{c} \in \text{\rm Span}\{\mathbf{B}_{\bar S}\}$}, we have:
\begin{align*}
\|\mathbf{y} - \mathbf{\bar{y}}\|_2 
&= \|\sum_{i=1}^k c_i(\mathbf{AX}_K - \mathbf{B\bar{X}}_K)_i\|_2
\leq \varepsilon \sum_{i=1}^k |c_i| \\
&\leq \varepsilon \sqrt{k}  \|\mathbf{c}\|_2 
%\leq \frac{\varepsilon \sqrt{k}}{L(\mathbf{A}X_{J(S)})} \|\mathbf{A}X_{J(S)}\mathbf{c}\|_2 
\leq \frac{\varepsilon}{L_k(\mathbf{AX}_K)} \|\mathbf{y}\|_2,
\end{align*}
where the last inequality follows directly from the definition of $L_k$. Finally, from Def.~\ref{dDef}, it follows that:
\begin{align}\label{rhs222}
d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\bar S}\}) 
\leq \frac{\varepsilon}{  L_k(\mathbf{AX}_{K}) }.
\end{align}
In particular, with $C_1$ as in \eqref{Cdef1}, the theorem follows.
\end{proof}
% for all $S \in E$:
%\begin{align}\label{rhs222}
%d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\bar S}\}) 
%\leq \frac{\varepsilon}{ \min_{S \in E} L_k(\mathbf{AX}_{I(S)}) }.
%\end{align}
%\end{proof}

%We then show that the map over subsets of column indices defined by this association is such that the number of common elements shared by sets in its domain bounds the number of elements common to their images under this map. 
 
\begin{proof}[Proof of Cor.~\ref{SLCopt}]
We bound the number of $k$-sparse $\mathbf{\bar x}_i$ and then apply Thm.~\ref{DeterministicUniquenessCorollary}. 
Let $n_p$ be the number of $\mathbf{\bar x}_i$ with $\|\mathbf{\bar x}_i\|_0 = p$.  %, so that $\sum_{p = 1}^{m} n_p = N$. 
Since the $\mathbf{x}_i$ are all $k$-sparse, by \eqref{minsum} we have:
%\begin{align}
\mbox{$k \sum_{p = 0}^{\bar m} n_p \geq \sum_{i=0}^N \|\mathbf{x}_i\|_0 \geq \sum_{i=0}^N \|\mathbf{\bar x}_i\|_0 = \sum_{p=0}^{\bar m} p n_p.$}
%\end{align}
Hence,
\begin{align}\label{eqn}
\sum_{p = k+1}^{\bar m} n_p \leq \sum_{p = k+1}^{\bar m} (p-k) n_p \leq \sum_{p = 0}^k (k-p)n_p \leq k \sum_{p = 0}^{k-1} n_p.
\end{align}

We now show that no more than $(k-1)|E|$ of the $\mathbf{\bar x}_i$ share a support of size less than $k$. 
%Let $\mathbf{X}$ and $\mathbf{\bar{X}}$ be the $m \times N$ and $\bar m \times N$ matrices with columns $\mathbf{x}_i$ and $\mathbf{\bar x}_i$, respectively, and let $I(S) = \{i \in [N]: \text{supp}(\mathbf{x}_i) \subseteq S\}$. 
From previous arguments, for every $S \in E$, the columns of each $n \times k$ submatrix of $\mathbf{AX}_{I(S)}$ form a basis for $\text{\rm Span}\{\mathbf{A}_S\}$.
% the general linear position of the $\mathbf{x}_i$ and the linear independence of every $k$ columns of $\mathbf{A}$ that $L_k(\mathbf{AX}_{I(S)}) > 0$ for all $S \in E$; that is, for every $S \in E$, the columns of every $n \times k$ submatrix of $\mathbf{AX}_{I(S)}$ form a basis for $\text{\rm Span}\{\mathbf{A}_S\}$. 
Suppose that more than $(k-1)|E|$ vectors $\mathbf{\bar x}_i$ share a support $\bar S$ of size $|\bar{S}| < k$. By the pigeonhole principle, there is some $S \in E$ supporting $k$ or more of the corresponding $\mathbf{x}_i$; let them be indexed by $K \subseteq I(S)$.  By the argument as in the proof of Thm.\ref{DeterministicUniquenessTheorem}, we also have \eqref{rhs222}. Since our bound on $\varepsilon$ implies that the right-hand side of \eqref{rhs222} is less than one, by \eqref{dimLem} we have the contradiction: \[k = \dim(\text{Span}(\mathbf{A}_S)) \leq \dim(\text{Span}\{\mathbf{B}_{\bar S}\}) \leq |\bar S|.\]

Hence, the total number of $(k-1)$-sparse vectors $\mathbf{\bar x}_i$ is no greater than $|E|(k-1){ m \choose k-1}$. Taking \eqref{eqn} into account, no more than $|E|k(k-1){ m \choose k-1}$ vectors $\mathbf{\bar x}_i$ are \emph{not} $k$-sparse. Since for every $S \in E$ there are over $(k-1)\left[ {\bar m \choose k} + |E|k{ m \choose k-1} \right]$ vectors $\mathbf{x}_i$ supported there, it follows that more than $(k-1){\bar m \choose k}$ of them must have corresponding $\mathbf{\bar x}_i$ that are also $k$-sparse. The result now follows directly from Thm.~\ref{DeterministicUniquenessCorollary}.
\end{proof}


 
\section{Discussion}\label{Discussion}

In this note, we generalize the approach of \cite{Hillar15} to prove the stability of unique solutions to Probs.~\ref{InverseProblem} and \ref{OptimizationProblem} while also significantly reducing the known sample complexity.
% FRITZ: Inverse problems instead of Data Analysis. Don't assume that everyone is assuming they are recovering ground truth. Cite results again and state implications! Focus on them (probabilistic ones too).
Our results provide theoretical support to the application of the sparse linear coding model to blind source separation problems, wherein the goal is to infer the generating dictionary and sparse codes from noisy measurements, \eqref{LinearModel}. We also collect a set of useful mathematical tools and basic facts for future research.
The main motivation for this work, however, was to understand how seemingly universal representations emerge from sparse coding models fit to natural data by a variety of methodologies. We elaborate more on these applications below after discussing theoretical aspects.  % We close with several application areas.

%\textbf{Inverse Problems}.  
% from $N=k{m \choose k}^2$ to $N = m(k-1){m \choose k}+m$. 
% CUT OK? Surprisingly, almost all $n \times m$ dictionaries satisfying the standard assumption \eqref{CScondition} from compressed sensing (CS) are identifiable from enough generic noisy $k$-sparse linear combinations of their elements, up to an error linear in the noise. Moreover, if solutions are constrained to satisfy \eqref{SparkCondition}, then only an upper bound on the number of dictionary elements need be taken as given. 

What we have shown here is that the sparse linear coding model generally produces a \textit{well-posed} inverse problem to be approximately solved by a numerical algorithm. This early concept of Hadamard \cite{Hadamard1902} can be paraphrased as the idea that inferences from observations should be robust to the inevitable uncertainty in measurement; that is, continuous in the data. In other words, perturbing a dataset by a small amount should only slightly influence the inferred parameters. In this regard, for the sparse linear coding model we demonstrate a linear relationship, \eqref{epsdel}, between measurement noise and the deviation of any solution from the true parameters, with explicit constants expressed in terms of these parameters. Moreover, we show that even if the meta-parameter defining the number of dictionary elements is overestimated, a subset of parameters may still be identifiable.
We remark that our uniqueness result accounts for deterministic ``worst-case'' noise, whereas the ``effective'' noise might be smaller when sampled from a distribution; in such cases, the constants %Thms.~\ref{DeterministicUniquenessTheorem},~\ref{DeterministicUniquenessTheorem2} 
will improve as well.
It would therefore be of practical utility to determine the best possible dependence of $\varepsilon$ on $\delta_1, \delta_2$ in Def.\ref{maindef} as well as the minimal requirements on the number and diversity of generating codes, and we hope that other researchers continue to improve and extend our results.

%We note also that these results extend trivially to cases where point-wise injective nonlinearities are applied to the data. 

One notable component of our contribution is a combinatorial criteria (regular hypergraphs satisfying the singleton intersection property, Def.~\ref{sip}) for the support sets of sparse codes key to the identification of the dictionary. Fully understanding those combinatorial designs allowing for stable sparse representations is an interesting research area for the future. For instance, whether there is a support set size $N$ that is polynomial in $m,k$ gauranteeing the conclusions of Thm.~\ref{DeterministicUniquenessTheorem} has implications for the computational complexity of Probs.~\ref{InverseProblem} and \ref{OptimizationProblem} and related questions \cite{Tillmann15}. 

A technical difficulty in proving Thm.~\ref{DeterministicUniquenessTheorem} was the absence of a spark condition assumption on solutions to Prob.~\ref{InverseProblem}.
Although mathematically interesting that no such requirement is necessary, there are other reasons to seek out such a theoretical guarantee. For instance, it is difficult to ensure that an algorithm maintain a dictionary satisfying \eqref{SparkCondition} at each iteration; indeed, even certifying a dictionary has this property is likely intractable given its NP-hardness \cite{tillmann2014computational}.

In fact, uniqueness guarantees with minimal assumptions apply to all areas of data science and engineering that utilize learned sparse representations. For example, several groups have utilized compressive sensing for signal processing tasks: MRI analysis \cite{lustig2008compressed}, image compression \cite{Duarte08}, and, more recently, the design of an ultrafast camera \cite{Gao14}. Given such effective uses of compressive sensing, it is only a matter of time before these systems incorporate sparse representational learning to encode and process data (e.g., in a device that learns structure from motion \cite{kong2016prior}). In these cases, assurances such as those offered by our theorems certify that different devices (with different initialization, samples, etc.) are equivalent as soon as enough data originate from a statistically identical system.
%Data science:
%  
%in a common application of compressed sensing in engineering 

%\textbf{Theoretical Neuroscience}.
Additionally, in the field of theoretical neuroscience, sparse dictionary learning and related methods have recovered characteristic components of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent} and sounds \cite{bellsejnowski1996, smithlewicki2006, Carlson12}, reproducing response properties of cortical neurons. Our results suggest that this correspondence could be due to  sparse representations being ``universal'' in natural data, an early mathematical idea in neural theory \cite{pitts1947}. Furthermore, our guarantees justify the hypothesis of \cite{Coulter10, Isely10} that sparse codes passed through a communication bottleneck in the brain are recovered from random projections via (unsupervised) biologically plausible sparse coding (e.g., \cite{rehnsommer2007, rozell2007neurally, hu2014hebbian}).

% \small{\small{\textbf{ACKNOWLEDGMENTS.}}} We thank Friedrich Sommer and Darren Rhea for early thoughts, and Ian Morris for \eqref{eqdim}.

% This breaks:  Proof of Main Lemma 1 somehow appears earlier than it should be....weird
\acknow{We thank Friedrich Sommer and Darren Rhea for early thoughts, and Ian Morris for posting \eqref{eqdim} online.}
 \showacknow % Display the acknowledgments section

\bibliography{chazthm_pnas}

\clearpage

\section{Proof of Main Lemma}

In this supplemental, we prove Lem.\ref{MainLemma} after stating auxiliary lemmas.

\begin{lemma}\label{SpanIntersectionLemma}
Let $\mathbf{M} \in \mathbb{R}^{n \times m}$. If every $2k$ columns of $\mathbf{M}$ are linearly independent, then for any $F \subseteq {[m] \choose k}$:
\begin{align*}
\text{\rm Span}\{\mathbf{M}_{\cap F}\}  = \bigcap_{S \in F} \text{\rm Span}\{\mathbf{M}_S\}.
\end{align*}
\end{lemma}
\begin{proof}
Using induction on $|F|$, it is enough to prove the lemma when $|F| = 2$; but this case follows directly from the assumption.
\end{proof}

\begin{lemma}\label{DistanceToIntersectionLemma}
Fix $k \geq 2$. Let $V_1, \ldots, V_k$ be subspaces of $\mathbb{R}^m$ and set $V = \cap_{i = 1}^k V_i$. For  $\mathbf{x} \in \mathbb{R}^m$, we have (where $r$ is given in Def.~\ref{SpecialSupportSet}):
\begin{align}\label{DTILeq}
\text{\rm dist}(\mathbf{x}, V) \leq \frac{1}{r(\{V_i\}_{i = 1}^k)} \sum_{i=1}^k \text{\rm dist}(\mathbf{x}, V_i).
\end{align}
\end{lemma}
\begin{proof} 
Recall that orthogonal projection onto a subspace $V \subseteq \mathbb{R}^m$ is the mapping $\Pi_V: \mathbb{R}^m \to V$ that associates with each $\mathbf{x}$ its unique nearest point in $V$; i.e., $\|\mathbf{x} - \Pi_V\mathbf{x}\|_2 = \text{\rm dist}(\mathbf{x}, V)$.
Next, observe:
\begin{align}\label{f}
\|\mathbf{x} - \Pi_V\mathbf{x}\|_2 &\leq \|\mathbf{x} - \Pi_{V_k} \mathbf{x}\|_2 + \|\Pi_{V_k}  \mathbf{x} - \Pi_{V_k}\Pi_{V_{k-1}}\mathbf{x}\|_2 \nonumber \\
&\ \ \ + \cdots + \|\Pi_{V_k} \Pi_{V_{k-1}}\cdots \Pi_{V_1} \mathbf{x} - \Pi_V \mathbf{x}\|_2 \nonumber \\
&\leq \|\Pi_{V_k}\cdots\Pi_{V_{1}} \mathbf{x} - \Pi_V \mathbf{x}\|_2 + \sum_{\ell=1}^k \|\mathbf{x} - \Pi_{V_{\ell}} \mathbf{x}\|_2,
\end{align}
using the triangle inequality and that the spectral norm satisfies $\|\Pi_{V_{\ell}}\|_2 \leq 1$ for all $\ell$ (since $\Pi_{V_{\ell}}$ are orthogonal projections).

The desired result, \eqref{DTILeq}, now follows by bounding the second term on the right-hand side using the following fact \cite[Thm.~9.33]{Deutsch12}:
\begin{align}
\|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} \mathbf{x} - \Pi_V\mathbf{x}\|_2 \leq z \|\mathbf{x}\|_2,
\end{align}
for \mbox{$z^2= 1 - \prod_{\ell =1}^{k-1}(1-z_{\ell}^2)$} and \mbox{$z_{\ell} = \cos\theta\left(V_{\ell}, \cap_{s=\ell+1}^k V_s\right)$}. Together with $\Pi_{V_\ell} \Pi_V = \Pi_V$ for all $\ell \in [k]$ and $\Pi_V^2 = \Pi_V$ yields:
\begin{align*}
\|\Pi_{V_k} \cdots \Pi_{V_1}\mathbf{x}  - \Pi_V \mathbf{x} \|_2 
&= \|\left( \Pi_{V_k} \cdots\Pi_{V_1} - \Pi_V \right) (\mathbf{x} - \Pi_V\mathbf{x})\|_2 \\
&\leq z\|\mathbf{x} - \Pi_V\mathbf{x}\|_2.
\end{align*}

Finally, substituting this into \eqref{f} and rearranging produces \eqref{DTILeq} after replacing $1 - z$ with $r(\{V_i\}_{i=1}^k)$.
\end{proof}

%\begin{lemma}\label{NonEmptyLemma} 
%Fix $\ell, m$, and $\bar m$. Suppose $E \subseteq 2^{[m]}$ is $\ell$-regular with the SIP.  If $\pi: E \to 2^{[\bar m]}$ has $\sum_{S \in E} |S| = \sum_{S \in E} |\pi(S)|$ and:
%\begin{align}\label{cond}
%|\bigcap_{S \in F} \pi(S)| \leq |\bigcap_{S \in F} S |,\ \ \   \text{for } F \subseteq E,
%\end{align}
%then $\bar m \geq m$ and the association $i \mapsto \cap \pi(F(i))$ defines an injective map from $J$ to $[\bar m]$ for some $J \subseteq [m]$ of size $p = \bar m - \ell(\bar m - m)$. In particular, if $\bar m = m$ then the map $\pi$ is induced by a permutation.
%\end{lemma}
% i'm placing this on hold since one thing i found was that i needed pi to be injective, but for that i needed the fact that |pi(S)| = |S|

\begin{lemma}\label{NonEmptyLemma} 
Fix $\ell, m$, and $\bar m$. Suppose $E \subseteq 2^{[m]}$ is $\ell$-regular with the SIP.  If $\pi: E \to 2^{[\bar m]}$ has $|S| = |\pi(S)|$ and:
\begin{align}\label{cond}
|\bigcap_{S \in F} \pi(S)| \leq |\bigcap_{S \in F} S |,\ \ \   \text{for } F \subseteq E,
\end{align}
then $\bar m \geq m$ and the association $i \mapsto \cap \pi(F(i))$ defines an injective map from $J$ to $[\bar m]$ for some $J \subseteq [m]$ of size $p = \bar m - \ell(\bar m - m)$. In particular, if $\bar m = m$ then the map $\pi$ is induced by a permutation.
\end{lemma}


\begin{proof}
First observe that assumption \eqref{cond} already constrains $\pi$ to be injective:  if $S_1 \neq S_2$, then $|\pi(S_1) \cap \pi(S_2)| \leq |S_1 \cap S_2| < \max\{|S_1|, |S_2|\} = \max\{|\pi(S_1)|, |\pi(S_2)|\}$, which implies that $\pi(S_1) \neq \pi(S_2)$.  Next, consider the collection of pairs: \[T_1 := \{(i, S): i \in \pi(S), S \in E\},\] which number $|T_1| = \sum_{S \in E} |\pi(S)| = \sum_{S \in E} |S| = \ell m$ by regularity of $E$.  
Our second observation is that $\bar m \geq m$. Otherwise, if $\bar m < m$, then by pigeonholing the $m \ell \geq \ell (\bar m + 1) \geq [(\ell + 1) - 1] \bar m + 1$ elements of $T_1$ with respect to the set $[\bar m]$ of possible first indices, there must exist at least $\ell + 1$ elements of $T_1$ having the same first index. But by \eqref{cond} and injectivity of $\pi$, we then have more than $\ell$ edges in $E$ with common element, a contradiction to regularity.

Since $|T_1| > \bar m (\ell -1) + (p-1) \geq \bar m (\ell -1)$, the pigeonhole principle also implies that there are at least $\ell$ elements of $T_1$ sharing the same first coordinate. Let $F_1$ be the preimage of this family under $\pi$.  By the assumption \eqref{cond}, the intersection of $F_1$ is nonempty, and in fact must contain exactly $\ell$ elements by regularity. 
%Thus, $F_1$ is the star of some element $i_1 \in [m]$.  
From the SIP, the intersection of sets in $F_1$ is $\{i_1\}$, which by \eqref{cond} implies that the intersection of $\pi(S)$ with $S \in F_1$ also contains exactly one element.  If $p > 1$, then define $T_2 := T_1 \setminus \{(i,S) \in T_1: i = i_1\}$, which contains more than $(\bar m - 1)(\ell -1)$ sets, and produce by the same argument a (necessarily) different index $i_2$. Iterating $p = \bar{m} - \ell(\bar m - m)$ times this loop yields the desired $J = \{i_1, \ldots, i_p\} \subseteq [m]$.
\end{proof}
\begin{proof}[Proof of Lem.\ref{MainLemma}]
We begin by showing that: 
\begin{align}\label{eq2}
d(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}, \text{\rm Span}\{\mathbf{A}_S\} ) = d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}).
\end{align}
Since the right-hand side of \eqref{GapUpperBound} is less than one (by \eqref{delrho} and $r \leq 1$), it follows from \eqref{dimLem} that $|\pi(S)| \geq \dim(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \geq \dim(\text{\rm Span}\{\mathbf{A}_{S}\}) = |S|$, with the equality due to $\mathbf{A}$ satisfying \eqref{SparkCondition}. Since $|S| = |\pi(S)|$, we have $\dim(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) = \dim(\text{\rm Span}\{\mathbf{A}_{S}\})$, and \eqref{eq2} follows using \eqref{eqdim}. Note that the columns of $\mathbf{B}_{\pi(S)}$ are therefore linearly independent, for all $S \in E$.

We next verify \eqref{cond} holds.  Fix $F \subseteq E$. Since $\text{\rm Span}\{\mathbf{B}_{\cap_{S \in F}\pi(S)}\} \subseteq \cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}$, if $\cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\} = \{\textbf{0}\}$, then we must have $|\cap_{S \in F} \pi(S)| = 0$ (as the columns of $\mathbf{B}_{\pi(S)}$ are linearly independent) and \eqref{cond} trivially holds. Suppose then that the intersection is not the zero vector. By Lem.~\ref{SpanIntersectionLemma} and Lem.~\ref{DistanceToIntersectionLemma}, and incorporating \eqref{eq2} and \eqref{GapUpperBound}, we have:
\begin{align}\label{randoml}
d( \text{\rm Span}&\{\mathbf{B}_{\cap_{S \in F}\pi(S)}\}, \text{\rm Span}\{\mathbf{A}_{\cap_{S \in F} S}\}  ) \nonumber \\
&\leq d\left( \cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}, \cap_{S \in F} \text{\rm Span}\{\mathbf{A}_{S}\} \right) \nonumber \\
&\leq \sum_{T \in F} \frac{ d\left( \cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\},\text{\rm Span}\{\mathbf{A}_{T}\} \right) }{ r( \{ \text{\rm Span}\{\mathbf{A}_{S}\}_{S \in F}) } \nonumber \\
&\leq \sum_{T \in F} \frac{ d\left( \text{\rm Span}\{\mathbf{B}_{\pi(T)}\},\text{\rm Span}\{\mathbf{A}_{T}\} \right) }{ r( \{ \text{\rm Span}\{\mathbf{A}_{S}\}_{S \in F}) }\nonumber \\
&\leq \frac{|F| \varepsilon}{r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F})} 
\leq \frac{C_2 \varepsilon}{\max_i\|\mathbf{A}_i\|_2}. 
\end{align}
Since $\varepsilon < L_2(\mathbf{A}) / C_2$, by \eqref{delrho} the right-hand side in \eqref{randoml} is strictly less than one. Hence, $\dim(\text{\rm Span}\{\mathbf{B}_{\cap_{S \in F}\pi(S)}\}) \leq \dim(\text{\rm Span}\{\mathbf{A}_{\cap_{S \in F} S}\})$ by \eqref{dimLem}, and \eqref{cond} follows from the linear independence of the columns of $\mathbf{A}_{S}$ and $\mathbf{B}_{\pi(S)}$ for all $S \in F$.

Now, by Lem.~\ref{NonEmptyLemma}, the association $i \mapsto \cap \pi(F(i))$ defines an injective map $\bar \pi: J \to [\bar m]$ for some $J \subseteq [m]$ of size $\bar m - \ell(\bar m - m)$, and we can be sure that $\mathbf{B}_{\bar \pi(i)} \neq \mathbf{0}$ for all $i \in J$ since the columns of $\mathbf{B}_{\pi(S)}$ are linearly independent for all $S \in E$. It then follows from \eqref{eqdim} and \eqref{randoml} that $d\left( \text{\rm Span}\{\mathbf{A}_i\}, \text{\rm Span}\{ \mathbf{B}_{\bar \pi(i)} \} \right) \leq C_2 \varepsilon / \max_i \|\mathbf{A}_i\|_2$ for all $i \in J$. Fixing $\bar \varepsilon = C_2\varepsilon$ and letting $c_i = \|\mathbf{A}_i\|_2^{-1}$, we thus have that for every basis vector $\mathbf{e}_i \in \mathbb{R}^m$ with $i \in J$ there exists some $\bar{c}_i \in \mathbb{R}$ such that $\|c_i\mathbf{A}\mathbf{e}_i - \bar{c}_i \mathbf{B}\mathbf{e}_{\bar \pi(i)}\|_2 \leq \bar \varepsilon < L_2(\mathbf{A}) \min_{i\in J} |c_i|$.  But this is exactly the supposition in \eqref{1D}, and the result follows from the case $k=1$ in Sec.~\ref{DUT} applied to the submatrix $\mathbf{A}_J$.
\end{proof}

\section{Proofs of Thm.~\ref{robustPolythm} and Cor.~\ref{ProbabilisticCor}}

\begin{proof}[Proof (sketch) of Thm.~\ref{robustPolythm}]
Let $M$ be the matrix with columns $\mathbf{A}\mathbf{x}_i$, $i \in [N]$.  Consider the polynomial in the entries of $\mathbf{A}$ and $\mathbf{x}_i$:
\begin{align*}
g(\mathbf{A}, \{\mathbf{x}_i\}_{i=1}^N) := \prod_{S \in {[n] \choose k}} \sum_{S' \in {[N] \choose k}} (\det M_{S',S})^2,
\end{align*}
with notation as in Sec.~\ref{Results}.  
It can be checked that when $g$ is nonzero for a substitution of real numbers for the indeterminates, all of the genericity requirements on $\mathbf{A}$ and $\mathbf{x}_i$ in our proofs of stability in Thm.~\ref{DeterministicUniquenessTheorem} are satisfied (in particular, the spark condition on $\mathbf{A}$).
\end{proof}

\begin{proof}[Proof (sketch) of Cor.~\ref{ProbabilisticCor}]
First, note that if a set of measure spaces $\{(X_{\ell}, \Sigma_{\ell}, \nu_{\ell})\}_{\ell=1}^p$ has $\nu_{\ell}$ is absolutely continuous with respect to $\mu$ for all $\ell \in [p]$, where $\mu$ is the standard Borel measure on $\mathbb{R}$, then the product measure $\prod_{\ell=1}^p \nu_{\ell}$ is absolutely continuous with respect to the standard Borel product measure on $\mathbb{R}^p$ (e.g.,  \cite{folland2013real}). By Thm.~\ref{robustPolythm}, there is a polynomial that is nonzero whenever $Y$ has a stable $k$-sparse representation in $\mathbb R^m$; in particular, this property (stability) holds with probability one.
\end{proof}

\end{document}