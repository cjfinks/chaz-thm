\documentclass[9pt,twocolumn]{pnas-new}
% Use the lineno option to display guide line numbers if required.
% Note that the use of elements such as single-column equations
% may affect the guide line number alignment. 

%%% CHAZ TODO %%%

%%% CHRIS TODO %%
% steamroll

% IMPORTED PACKAGES
\usepackage{amsmath, amssymb, amsthm} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

%\usepackage[pdftex]{graphicx}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}
%\usepackage{cite}

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} = Template for a one-column mathematics article
% {pnasinvited} = Template for a PNAS invited submission

\title{On the uniqueness and stability of dictionaries for sparse representation of noisy signals}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,1]{Charles J. Garfinkle}
\author[a,1]{Christopher J. Hillar} 

\affil[a]{Redwood Center for Theoretical Neuroscience, Berkeley, CA, USA}

% Please give the surname of the lead author for the running footer
\leadauthor{Garfinkle} 

\significancestatement{Many naturally occurring signals (visual scenery, speech, electrocardiograms, etc.) can be characterized as parsimonious combinations of elementary waveforms drawn from a large dictionary. Absent analytic descriptions of these signal classes, it falls on algorithms from machine learning to extract such structures given many examples from a class. We give conditions guaranteeing when a dictionary of a given size is uniquely and stably determined by data, regardless of the algorithm used to compute it. These results provide theoretical grounding for  the application of dictionary learning to recovering sparse signals from unknown noisy compressive measurements.}

\authordeclaration{The authors declare no conflict of interest.}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: cjg@berkeley.edu, chillar@msri.org}

\keywords{Sparse linear coding, dictionary learning, uniqueness, compressed sensing} 

\begin{abstract}
Dictionary learning for sparse linear coding has exposed underlying structure in many natural signals.
However, there are few universal theorems guaranteeing uniqueness of model estimation independent of implementation.
Here, we prove that for all diverse enough datasets generated from the sparse coding model, latent dictionaries and codes are uniquely determined up to measurement error.  Applications are given to data analysis, engineering, and neuroscience. 
\end{abstract}

% \dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

% Optional adjustment to line up main text (after abstract) of first page with line numbers, when using both lineno and twocolumn options.
% You should only change this length when you've finalised the article contents.
\verticaladjustment{-2pt}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.
\dropcap{B}lind source identification is a classical problem in signal processing \cite{sato1975method}.  
One popular modern formulation of the idea is to find a dictionary of $m$ elementary waveforms, at most $k$ of which need be linearly combined to represent each $n$-dimensional signal in a dataset of size $N$, typically where $k < m \ll N$.  Approximating solutions to this problem have
provided insight into the structure of many signals lacking explicit analytic structure (see \cite{Zhang15} for a comprehensive review). 
In particular, it has been shown that response properties of simple-cell neurons in mammalian visual cortex emerge from learning a dictionary for sparse linear coding of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent}, a major advance in computational neuroscience. A fundamental aspect of this finding is that the latent waveforms (``Gabors'') estimated from data appear to be canonical; i.e., they are found in learned dictionaries independent of algorithmic implementation or natural image training set.

% (e.g., Fourier bases, wavelets, etc.)

Motivated by these discoveries and earlier work in the theory of neural communication \cite{Coulter10, Isely10}, we address when dictionaries and sparse representations are uniquely determined by data.  Answers to this question also have other real-world implications.  For example, a sparse coding analysis of local painting style can be used for forgery detection \cite{hughes2010, Olshausen10}, but only if sparse components of the artwork are independent of implementation idiosyncrasies. Fortunately, algorithms with proven recovery of generating dictionaries under certain 
conditions have recently been proposed (see \cite[Sec.~I-E]{Sun16} for a summary of the state-of-the-art).  However, there are few universal theorems explaining the general phenomenon of uniqueness.  

Here, we prove very generally that uniqueness in sparse linear coding is an expected property of the model.  In other words, whenever enough sparsely represented data generated from a dictionary (satisfying a spark condition) are observed, the original codes and dictionary are uniquely determined up to an error that is commensurate with the measurement noise.  Applications of this result include:  1) a sufficient diversity of sparse codes sent through a random linear compressive channel are unique, 2) the sparse linear coding problem is well-posed in the sense of Hadamard \cite{Hadamard1902}, and 3) an explanation for universal representation of natural signals in neuroscience. 

More formally, let $\mathbf{A} \in \mathbb R^{n \times m}$ be a matrix with columns $\mathbf{A}_j$ ($j = 1,\ldots,m$) and let dataset $Z$ consist of measurements:
\begin{align}\label{LinearModel}
\mathbf{z}_i = \mathbf{A}\mathbf{x}_i + \mathbf{n}_i,\ \ \  \text{$i=1,\ldots,N$},
\end{align}
for $k$-\emph{sparse} $\mathbf{x}_i \in \mathbb{R}^m$ having at most $k$ nonzero entries and \emph{noise} $\mathbf{n}_i \in \mathbb{R}^n$, with bounded norm $\| \mathbf{n}_i \|_2 \leq  \eta$ representing our combined worst-case uncertainty in  measuring $\mathbf{A}\mathbf{x}_i$.
The precise mathematical problem addressed here is the following.

\begin{problem}[Sparse linear coding]\label{InverseProblem}
Find a matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$ such that $\|\mathbf{z}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \eta$ for all $i$.
\end{problem}

Note that any particular solution $(\mathbf{B}, \mathbf{\bar x}_1 \ldots, \mathbf{\bar x}_N)$ to this problem gives rise to an orbit of other solutions $(\mathbf{BPD}, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_1, \ldots, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_N)$, where $\mathbf{P}$ is any permutation matrix and $\mathbf{D}$ any invertible diagonal.  
We therefore consider whether solutions to Prob.~\ref{InverseProblem} are unique only up to this ambiguity.  More specifically, 
we seek conditions for when sparsely coded datasets have a stable unique representation.

\begin{definition}\label{maindef}
Fix $Y = \{ \mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$. We say $Y$ has a \textbf{$k$-sparse linear representation in $m$ dimensions} if there exists a matrix $\mathbf{A}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ such that $\mathbf{y}_i = \mathbf{A}\mathbf{x}_i$ for all $i$. %Equivalently, $\mathbf{Y} = \mathbf{A}\mathbf{X}$ for $\mathbf{Y}$ and $\mathbf{X}$ having the $\mathbf{y}_i$ and $\mathbf{x}_i$ as their columns, respectively. 
This representation is \textbf{stable} if for every $\delta_1, \delta_2 \geq 0$, there exists $\varepsilon(\delta_1, \delta_2) \geq 0$ (with $\varepsilon > 0$ when  $\delta_1, \delta_2 > 0$) such that if $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$ satisfy:
\begin{align*}
\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon,\ \ \   \text{for all $i$},
\end{align*}
%
then there is some permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ such that for all $i, j$:
\begin{align}\label{def1}
\|\mathbf{A}_j - (\mathbf{BPD})_j\|_2 \leq \delta_1 \ \ \text{and} \ \ \|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i\|_1 \leq \delta_2.
\end{align}
\end{definition}

To see how Def. \ref{maindef} directly relates to Prob. \ref{InverseProblem}, suppose that $Y$
% \mbox{$Y = \{\mathbf{A} \mathbf{x}_1, \ldots, \mathbf{A}\mathbf{x}_N\}$} 
has a stable $k$-sparse linear representation and fix $\delta_1, \delta_2$ to be the desired recovery accuracy in \eqref{def1}. Consider now any dataset $Z$ generated as in \eqref{LinearModel} with $\eta \leq \frac{1}{2} \varepsilon(\delta_1, \delta_2)$. Then from the triangle inequality, it follows that any matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N$ solving Prob.~\ref{InverseProblem} are necessarily close to the original dictionary $\mathbf{A}$ and sparse codes $\mathbf{x}_i$. 

Previous theoretical work on Prob.~\ref{InverseProblem} for the noiseless case $\eta =0$ (e.g., \cite{li2004analysis, Georgiev05, Aharon06, Hillar15}) has shown that a solution (when it exists) is indeed unique provided the $\mathbf{x}_i$ are sufficiently diverse and the generating matrix $\mathbf{A}$ satisfies the \textit{spark condition} from compressed sensing (CS):
\begin{align}\label{SparkCondition}
\mathbf{A}\mathbf{x}_1 = \mathbf{A}\mathbf{x}_2 \implies \mathbf{x}_1 = \mathbf{x}_2, \ \ \ \text{for all $k$-sparse } \mathbf{x}_1, \mathbf{x}_2,
\end{align}
%
which would be, in any case, necessary for uniqueness.

Our main technical finding is that a sufficient quantity of measurements generated from any dictionary satisfying the spark condition 
has a stable sparse representation.  More specifically, such dictionaries are uniquely identifiable from as few as \mbox{$N = m(k-1){m \choose k} + m$} sparse linear combinations of their columns up to an error that is linear in the measurement noise (Thm.~\ref{DeterministicUniquenessTheorem}). In fact, provided $(n, m, k)$ satisfy CS inequality \eqref{CScondition}, in almost all cases the dictionary learning problem is well-posed given enough data (Cor.~\ref{ProbabilisticCor}). Importantly, these guarantees hold without assuming the recovered matrix satisfies the spark condition. Our results  also apply when only an upper bound on the number of dictionary elements is known. The explicit, algorithm-independent criteria we provide should be a useful tool in the theory of sparse dictionary learning.  

In the next section, we give precise statements of our main results, which include an explicit form for $\varepsilon(\delta_1, \delta_2)$. We then prove our main theorem (Thm.~\ref{DeterministicUniquenessTheorem}) in Sec.~\ref{DUT} after stating some additional definitions and lemmas required for the proof, including our main tool from combinatorial matrix analysis (Lem.~\ref{MainLemma}). All other proofs are relegated to the Supplemental. 
Finally, we present several applications in Discussion Sec.~\ref{Discussion}.

\section{Results}

Before stating main results, we first identify the combinatorial criteria on support sets of generating codes allowing for stable sparse 
representations.  Let $\{1, \ldots, m\}$ be denoted $[m]$, its power set $2^{[m]}$, and ${[m] \choose k}$ the set of subsets of $[m]$ of size $k$. 
% Let $\text{\rm Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_\ell\}$ be the $\mathbb{R}$-linear span of vectors $\mathbf{v}_1, \ldots, \mathbf{v}_\ell$. 
A vector $\mathbf{x}$ is said to be \emph{supported} on $S \subseteq [m]$ %,
when $\mathbf{x} \in \text{\rm Span} \{\mathbf{e}_j\}_{j\in S}$, where $\mathbf{e}_j$ are the standard basis in $\mathbb R^m$.  We also write
 supp$(\mathbf{x}) = S$ when $S$ indexes the nonzero components of $\mathbf{x}$.
Let $\mathbf{M}_S$ be the submatrix formed by the columns of $\mathbf{M}$ indexed by $S$, where we set $\text{\rm Span}\{\mathbf{M}_\emptyset\} := \{\textbf{0}\}$.

We say a hypergraph $E \subseteq 2^{[m]}$ on vertices $[m]$ is \textit{$k$-uniform} when every $S \in E$ has cardinality $k$. We also say $E$ is \emph{regular} when every element of $[m]$ is contained in exactly $\ell$ elements of $E$ for some $\ell > 0$ (for given $\ell$, we say $E$ is \textit{$\ell$-regular}).

\begin{definition}\label{sip}
Given $E \subseteq 2^{[m]}$, the \textbf{star} $F(i)$ at $i$ are those $S \in E$ with $i \in S$. We say $E$ has the \textbf{singleton intersection property} (\textbf{SIP}) when $\cap_{S \in F(i)} S = \{i\}$ for all $i \in [m]$. 
\end{definition}

It is easy to verify that for every $k < m$, there is a regular $k$-uniform hypergraph that satisfies the SIP; for instance, consecutive intervals of length $k$ in some cyclic order on $[m]$.
%\begin{proposition}
%For every $k < m$, there is a regular $k$-uniform hypergraph that satisfies the singleton intersection property.
%\end{proposition}

%The second ingredient necessary for the statement of Thm.~\ref{DeterministicUniquenessTheorem} 
Next, we describe a quantitative version of the spark condition.  % We first explain how the spark condition \eqref{SparkCondition} relates to t
The \emph{lower bound} \cite{Grcar10} of a matrix $\mathbf{A} \in \mathbb R^{n \times m}$ is the largest number $\alpha$ such that \mbox{$\|\mathbf{A}\mathbf{x}\|_2 \geq \alpha\|\mathbf{x}\|_2$} for all $\mathbf{x} \in \mathbb{R}^m$. By compactness of the unit sphere, every injective linear map has a nonzero lower bound; hence, if $\mathbf{A}$ satisfies \eqref{SparkCondition}, then each submatrix formed from $2k$ of its columns or less has a nonzero lower bound. This motivates the following definition: % We therefore define the following domain-restricted lower bound:
\begin{align*}
L_k(\mathbf{A}) := \frac{1}{\sqrt{k}}\max \{ \alpha : \|\mathbf{A}\mathbf{x}\|_2 \geq \alpha\|\mathbf{x}\|_2 \text{ for all $k$-sparse } \mathbf{x}\}.
\end{align*} 

Clearly, $\sqrt{k} L_k(\mathbf{A}) \geq \sqrt{k'}L_{k'}(\mathbf{A})$ whenever $k < k'$, and for any $\mathbf{A}$ satisfying \eqref{SparkCondition}, we have $L_{k'}(\mathbf{A}) > 0$ for  $k' \leq 2k$. The quantity $1 - \sqrt{k} L_k(\mathbf{A})$ is also known in the CS literature as the (asymmetric) lower restricted isometry constant \cite{Blanchard2011, Foucart2009}.

We say that a set of $k$-sparse vectors is in \emph{general linear position} when any $k$ of them are linearly independent.
The following is the precise statement of our main result. 

\begin{theorem}\label{DeterministicUniquenessTheorem}
%Fix integers $n, \ell$, and $k < m$. 
Let $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ satisfy \eqref{SparkCondition} for $k < m$, and suppose for some $\bar m$ that more than \mbox{$(k-1){\bar m \choose k}$} $k$-sparse \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} are supported in general linear position on each set in an $\ell$-regular $E \subseteq {[m] \choose k}$ with the SIP.  There is a constant $C_1 > 0$ for which the following hold for $\varepsilon < L_2(\mathbf{A}) / C_1$.

1) Every matrix $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{\bar m}$ with  \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$} for all $i$ must have $\bar m \geq m$ and, provided $p = \bar m - \ell(\bar m - m)$ is positive, necessarily satisfy:
\begin{align}\label{Cstable}
\|(\mathbf{A}_J- \mathbf{B}_{\bar J} \mathbf{PD} )_j\|_2 \leq C_1 \varepsilon, \ \  \text{for $j \in J$},
\end{align}
for some $J \in {[m] \choose p}$, $\bar J \in {[\bar m] \choose p}$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.

2) Restricting dictionaries and sparse codes to respective $J, \bar{J}$, we furthermore have that if $\varepsilon < L_{2k}(\mathbf{A}) / C_1$, then $\mathbf{B}$ also satisfies \eqref{SparkCondition} with $L_{2k}(\mathbf{B}\mathbf{PD}) \geq L_{2k}(\mathbf{A}) - C_1 \varepsilon$, and:
\begin{align}\label{b-PDa}
\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i\|_1 &\leq  \left( \frac{ 1+C_1 \|\mathbf{x}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right) \varepsilon,\ \ \   \text{for $i \in [N]$}.
\end{align}
\end{theorem}

%\begin{theorem}\label{DeterministicUniquenessTheorem}
%Fix integers $n, \ell$, and $k < m$. Suppose $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ satisfies the spark condition \eqref{SparkCondition} and that $k$-sparse \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} contain at least \mbox{$(k-1){m \choose k}+1$} vectors supported in general linear position on each set in some $\ell$-regular $E \subseteq {[m] \choose k}$ with the SIP.  There exists a constant $C_1 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_1$.

%Every matrix $\mathbf{B} \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{m}$ with  \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$} for all $i$ necessarily satisfies:
%\begin{align}\label{Cstable}
%\|\mathbf{A}_j - (\mathbf{B}\mathbf{PD})_j\|_2 \leq C_1 \varepsilon,\ \ \   \text{for all $j \in [m]$},
%\end{align}
%for some permutation matrix $\mathbf{P}$ and invertible diagonal $\mathbf{D}$.

%Furthermore, if $\varepsilon < L_{2k}(\mathbf{A}) / C_1$, then $\mathbf{B}$ also satisfies \eqref{SparkCondition} with $L_{2k}(\mathbf{B}\mathbf{PD}) \geq L_{2k}(\mathbf{A}) - C_1 \varepsilon$, and:
%\begin{align}\label{b-PDa}
%\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i\|_1 &\leq  \left( \frac{ 1+C_1 \|\mathbf{x}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right) \varepsilon,\ \ \   \text{for all $i \in [N]$}.
%\end{align}
%\end{theorem}

%\begin{theorem}\label{DeterministicUniquenessTheorem}
%Fix integers $n, \ell$, and $k < m \leq \bar m$. Suppose $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ satisfies the spark condition \eqref{SparkCondition} and that $k$-sparse \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} contain at least \mbox{$(k-1){\bar m \choose k}+1$} vectors supported in general linear position on each set in some $\ell$-regular $E \subseteq {[m] \choose k}$ satisfying the SIP.  There exists a constant $C_1 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_1$.
%
%Every matrix $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{\bar m}$ with  \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$} for all $i$ necessarily satisfies:
%\begin{align}\label{Cstable}
%\|\mathbf{A}_{\phi(j)} - (\mathbf{B}_{J}\mathbf{PD})_j\|_2 \leq C_1 \varepsilon,\ \ \   \text{for $j = 1, \ldots, |J|$},
%\end{align}
%%
%for some $J \subseteq [\bar m]$ of size \mbox{$\lfloor \bar m - \ell(\bar m - m) \rfloor$}, injective map $\phi$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.
%%\footnote{We note that the condition $\varepsilon < L_2(\mathbf{A})$ above is necessary. When \mbox{$\mathbf{A}$ = $I$} and $\mathbf{x}_i = \mathbf{e}_i$, it turns out that $C_1 = 1$ and there is a $\mathbf{B}$ and $1$-sparse $\mathbf{\bar x}_i$ with $\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$ violating \eqref{Cstable}.}
%
%Furthermore, if $\varepsilon < L_{2k}(\mathbf{A}) / C_1$, then $\mathbf{B}_J$ also satisfies \eqref{SparkCondition} with $L_{2k}(\mathbf{B}_J\mathbf{PD}) \geq L_{2k}(\mathbf{A}) - C_1 \varepsilon$, and:
%\begin{align}\label{b-PDa}
%\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i\|_1 &\leq  \left( \frac{ 1+C_1 \|\mathbf{x}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right) \varepsilon,\ \ \   \text{for all $i \in [N]$}.
%\end{align}
%\end{theorem}

%In other words, the smaller the difference $\bar m - m$, the more columns and coefficients of the original $n \times m$ dictionary $\mathbf{A}$ and $m$-dimensional codes $\mathbf{x}_i$ contained (up to noise) in the appropriately scaled $n \times \bar m$ dictionary $\mathbf{B}$ and $\bar m$-dimensional codes $\mathbf{\bar x}_i$. In particular, when $\bar m = m$ all columns of $\mathbf{A}$ and coefficients of each $\mathbf{x}_i$ recoverable, as per Def.~\ref{maindef}. 

An important consequence of Thm.~\ref{DeterministicUniquenessTheorem} is that \eqref{def1} is guaranteed provided $\varepsilon$ in Def.~\ref{maindef} does not exceed: % [*** TODO: verify this is still correct ***]
\begin{align}\label{epsdel}
\varepsilon(\delta_1, \delta_2) := \min \left\{ \frac{\delta_1}{ C_1 }, \frac{ \delta_2 L_{2k}(\mathbf{A})}{ 1 + C_1 \left( \max_{i \in [N]} \|\mathbf{x}_i\|_1  + \delta_2 \right) } \right\}.
\end{align}
We delay defining the constant $C_1$ until Section \ref{DUT} (\eqref{Cdef1}).  % , since it requires additional definitions not critical to the statement of our results.


% Note also that Thm~\ref{DeterminsiticUniquenessTheorem} gives conditions for when a single solution to Problem~\ref{InverseProblem} in fact represents \emph{all} possible solutions, related by transformations of the form $PD$.

\begin{corollary}\label{DeterministicUniquenessCorollary}
Given $n, m$, $k < m$, and a regular hypergraph $E \subseteq {[m] \choose k}$ with the SIP, there are $N =  |E| \left[ (k-1){m \choose k} + 1  \right]$ vectors \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} such that every matrix $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} generates a dataset $Y = \{\mathbf{A}\mathbf{x}_1, \ldots, \mathbf{A}\mathbf{x}_N\}$ with a stable $k$-sparse representation in $m$ dimensions.
\end{corollary}
\begin{proof}
The implication of Thm.\ref{DeterministicUniquenessTheorem} in the case $\bar{m} = m$ is precisely that such a $Y$ has a stable sparse representation (Def.~\ref{maindef}).  It remains, therefore, only to produce sparse codes in general linear position.  However, this is straightforward with a ``Vandermonde'' matrix construction (e.g., see \cite{Hillar15}).
\end{proof}

%\begin{proposition}
%If $E$ is an $\ell$-regular hypergraph of rank $k$ satisfying the singleton intersection property then $|E| \geq \ell m/k$. 
%\end{proposition}
%Note that every $\ell$-regular hypergraph $E \subseteq {[m] \choose k}$ satisfies: \[|E|k = \sum_{S \in E}|S| = \sum_{i \in [m]} \deg(i) = \ell m.\] 
As mentioned above, it is easy to construct a regular hypergraph $E \subseteq {[m] \choose k}$ with the SIP having cardinality $|E| = m$, implying the lower bound for sample size $N$ from the introduction. In many cases, the SIP can be achieved with $|E| < m$; for example, when $k = \sqrt{m}$ one can take $E$ to be the rows and columns formed by arranging $[m]$ in a square grid.  
%In light of Cor.~\ref{DeterministicUniquenessCorollary}, it would be interesting to determine other such $E$. %[ ** TODO ** can we show for every $k, m$ that there is an $E$ satisfying the SIP with $|E| = \ell m/k$ for every $\ell$ such that $\ell m/k$ is an integer? Then lower bound on $N$ comes from the smallest such $\ell$.]

There are other less direct consequences of Thm.~\ref{DeterministicUniquenessTheorem} to stability in sparse linear coding.  The following implication relates the result to more common optimization formulations.


\begin{corollary}\label{SLCopt}
If the assumptions of Thm.\ref{DeterministicUniquenessTheorem} hold with more than $(k-1)\left[ {\bar m \choose k} + |E|k{\bar m \choose k-1}\right]$ vectors $\mathbf{x}_i$ supported on each $S \in E$, then all matrices $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ and $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{\bar m}$ solving:
\begin{align}\label{minsum}
\min \sum_{i = 1}^N \|\mathbf{\bar x}_i\|_0 \ \ 
\text{subject to} \ \ \|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon, \ \text{for all $i$},
\end{align}
%
necessarily satisfy implications 1) and 2) from Thm.~\ref{DeterministicUniquenessTheorem}.
%must have $\bar m \geq m$ and, provided $p = \bar m - \ell(\bar m - m) > 0$, necessarily satisfy \eqref{Cstable}
%%\begin{align}\label{Cstable}
%%\|(\mathbf{A}_J- \mathbf{B}_{\bar J} \mathbf{PD} )_j\|_2 \leq C_1 \varepsilon, \ \  \text{for  $j \in J$},
%%\end{align}
%for some $J \in {[m] \choose p}$, $\bar J \in {[\bar m] \choose p}$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.
\end{corollary}

%\begin{corollary}
%Fix $n, \ell$, and $k < m$. Let $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ satisfy the spark condition \eqref{SparkCondition} and $k$-sparse \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} have at least \mbox{$\left[ (k-1){ m \choose k} + 1 \right] + (k-1)^2{m \choose k-1}$} vectors supported in general linear position on each set in an $\ell$-regular $E \subseteq {[m] \choose k}$ with the SIP. Given any $\delta_1, \delta_2 \geq 0$ and $\varepsilon \leq \varepsilon(\delta_1, \delta_2)$,
%every solution $\mathbf{B}, \mathbf{\bar x}_i, \ldots, \mathbf{\bar x}_N$ to the $\ell_0$ optimization problem:
%\begin{align}\label{minsum}
%\arg \min \sum_{i = 1}^N \|\mathbf{\bar x}_i\|_0 \ \ \text{s.t.} \ \ \mathbf{B}\mathbf{\bar x}_i = \mathbf{A}\mathbf{x}_i, \ \ \text{$i \in [N]$}, %\|\mathbf{A}\end{align}
%necessarily satisfies the recovery inequalities in \eqref{def1}. % \eqref{Cstable} and \eqref{b-PDa}.
%\end{corollary}
%
%\begin{proof}
%We derive a lower bound on the number of $k$-sparse $\mathbf{\bar x}_i$ and then apply Thm.~\ref{DeterministicUniquenessCorollary}. 
%First, note that $\|\mathbf{\bar x}_i\|_0 \neq 0$ for all $i$, since otherwise we would have $\mathbf{A}\mathbf{x}_i = \mathbf{B}\mathbf{\bar x}_i = \mathbf{0} \implies \mathbf{x}_i = \mathbf{0}$ by the spark condition, contradicting the general linear position of the $\mathbf{x}_i$. Let $n_p$ be the number of $\mathbf{\bar x}_i$ with $\|\mathbf{\bar x}_i\|_0 = p$.  %, so that $\sum_{p = 1}^{m} n_p = N$. 
%Since the $\mathbf{x}_i$ are all $k$-sparse, by \eqref{minsum} we have:
%\begin{align}
%k \sum_{p = 1}^{m} n_p = kN \geq \sum_{i=1}^N \|\mathbf{x}_i\|_0 \geq \sum_{i=1}^N \|\mathbf{\bar x}_i\|_0 = \sum_{p=1}^{m} p n_p.
%\end{align}
%%\[\implies \sum_{p = 1}^k (k-p)n_p \geq \sum_{p = k+1}^{m} (p-k) n_p.\]
%Hence,
%\begin{align}
%\sum_{p = k+1}^m n_p \leq \sum_{p = k+1}^m (p-k) n_p \leq \sum_{p = 1}^k (k-p)n_p \leq (k-1) \sum_{p = 1}^{k-1} n_p.
%\end{align}
%%\[ \implies \sum_{p = k+1}^m n_p \leq (k-1)^2 {\bar m \choose k-1} \]
%
%By the spark condition and general linear position of the $\mathbf{x}_i$, every $k$ vectors $\mathbf{Ax}_i$ span a $k$-dimensional space; hence, at most $k-1$ vectors $\mathbf{\bar x}_i$ can share a support of size $k-1$ or less, and the number of $(k-1)$-sparse vectors $\mathbf{\bar x}_i$ is then bounded from above by $(k-1) { m \choose k-1}$. It follows that at least $N - (k-1)^2 {m \choose k-1}$ of the vectors $\mathbf{\bar  x}_i$ are $k$-sparse, and that there must then be some $N' \subseteq [N]$ for which for each $S \in E$ there are $(k-1){m \choose k}+1$ of the vectors $\mathbf{x}_i$ with $i \in [N']$ supported on $S$ and for which $\mathbf{\bar x}_i$ is $k$-sparse for all $i \in [N']$.  The result now follows directly from Thm.~\ref{DeterministicUniquenessCorollary}. % \eqref{Cstable} and \eqref{b-PDa} follow.
%\end{proof}

Another straightforward application of Thm.~\ref{DeterministicUniquenessTheorem} is a probabilistic extension, which takes advantage of the following well-known application of random matrix theory to compressed sensing.  A random $n \times m$ matrix obeys \eqref{SparkCondition} with probability one (or ``high probability'' for discrete variables) 
provided:
\begin{align}\label{CScondition}
n \geq \gamma k\log\left(\frac{m}{k}\right),
\end{align}
in which $\gamma >0$ is a constant that depends on the particular distribution from which the entries of $\mathbf{A}$ are sampled i.i.d. (many ensembles suffice, e.g. see \cite[Sec.~4]{Baraniuk08}). 

In fact, 
% We motivate our next theorem by the following observation:  
the spark condition can be made explicit.  Let $\mathbf{A}$  be the $n \times m$ matrix of $nm$ indeterminates $A_{ij}$. When real numbers are substituted for $A_{ij}$, the resulting matrix satisfies \eqref{SparkCondition} if and only if the following polynomial is nonzero:
\begin{align*}
f(\mathbf{A}) := \prod_{S \in {[m] \choose k}} \sum_{S' \in {[n] \choose k}} (\det \mathbf{A}_{S',S})^2,
\end{align*}
%
where for any $S' \in {[n] \choose k}$ and $S \in {[m] \choose k}$, the symbol $\mathbf{A}_{S',S}$ denotes the submatrix of entries $A_{ij}$ with $(i,j) \in S' \times S$.   We note that the large number of terms in this product is likely necessary due to the NP-hardness of deciding whether a given matrix $A$ satisfies the spark condition \cite{tillmann2014computational}.

Since $f$ is a real analytic function, having one substitution of real numbers with $f(\mathbf{A}) \neq 0$ implies that its zeroes form a set of (Borel) measure zero. Hence, almost every $n \times m$ real matrix $\mathbf{A}$ satisfies \eqref{SparkCondition} provided \eqref{CScondition} holds with a $\gamma$ for some distribution.  We set $\gamma_0$ to be the smallest known such $\gamma$. % We remark that the precise relationship between $m$, $n$, and $k$ guaranteeing that $f$ is not identically zero is a challenging problem in real algebraic geometry. In any case, we set $\gamma_0$ to be the smallest known such $\gamma$.

A similar phenomenon applies to datasets of vectors with a stable sparse representation. As in \cite[Sec.~IV]{Hillar15}, consider the ``symbolic'' dataset $Y = \{\mathbf{A}\mathbf{x}_1,\ldots,\mathbf{A} \mathbf{x}_N\}$ generated by indeterminate $\mathbf{A}$ and indeterminate $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N$.  

\begin{theorem}\label{robustPolythm} %label is fucking up formatting..???
%Fix $n, m$, and $k < m$. 
There is a polynomial in the entries of $\mathbf{A}$ and the $\mathbf{x}_i$ with the following property:  if the polynomial evaluates to a nonzero number and more than \mbox{$(k-1){m \choose k}$} of the resulting vectors $\mathbf{x}_i$ are supported on each $S \in E$ for some regular $E \subseteq {[m] \choose k}$ with the SIP, then $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ (Def.~\ref{maindef}). In particular, either no substitutions impart to $Y$ this property or all but a Borel set of measure zero do. 
\end{theorem}

% this is cool, but let's just save it for later...i can ask around.  -cjh
%An interesting open question in real algebraic geometry is which collections of $(m,n,k)$ determine this last ``or".

\begin{corollary}\label{ProbabilisticCor}
Fix $n, m$, and $k$ satisfying \eqref{CScondition} for $\gamma = \gamma_0$, and let the entries of $\mathbf{A} \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ be drawn independently from probability measures absolutely continuous with respect to the standard Borel measure $\mu$. If more than $(k-1){m \choose k}$ of the vectors $\mathbf{x}_i$ are supported on each $S \in E$ for a regular $E \subseteq {[m] \choose k}$ with the SIP, then $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ with probability one.
\end{corollary}

Thus, choosing dictionary and sparse codes ``randomly'' generate data with stable sparse representations almost certainly.
% , as long as the relevant parameters satisfy \eqref{CScondition}.

%Next, we address the case when only an upper bound $m'$ on the latent dimension $m$ is known (assuming that $\mathbf{B}$ satisfies \eqref{SparkCondition}).

%\begin{theorem}\label{DeterministicUniquenessTheorem2}
%Fix integers $n$ and $k < m \leq m'$ and matrices $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ and $\mathbf{B} \in \mathbb{R}^{n \times m'}$ both satisfying \eqref{SparkCondition}. Suppose \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} include at least \mbox{$(k-1){m' \choose k}+1$} $k$-sparse vectors in general linear position supported on each set in some $k$-uniform $E \subseteq 2^{[m]}$ satisfying the singleton intersection property. Then there exists a constant $C_3 > 0$ for which the following holds.

%If for some $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{m'}$ and $\varepsilon < L_2(\mathbf{A}) / C_3$ we have \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$} for all $i$, then:
%\begin{align}\label{Cstablem'}
%\|\mathbf{A}_j-(\mathbf{B}_J\mathbf{PD})_j\|_2 \leq C_3\varepsilon \ \ \text{for all $j \in [m]$}
%\end{align}
%
%for some $J \in {[m'] \choose m}$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.
%\end{theorem}

%In other words, the columns of $B$ contain (up to noise, after appropriate scaling) the columns of the original dictionary $\mathbf{A}$. Similarly, one can show by the same arguments at the beginning of Sec.~\ref{DUT} that the $\mathbf{\bar x}_i$ contain the original codes $\mathbf{x}_i$. The constant $C_3$ here is expression (\ref{Cdef2}) from the proof of Thm.~\ref{DeterministicUniquenessTheorem2}. Note that, in contrast to Thm.~\ref{DeterministicUniquenessTheorem}, this constant is dependent on $\mathbf{B}$; hence, \eqref{Cstablem'} holds for \emph{all} matrices $\mathbf{B} \in \mathbb{R}^{n \times m'}$ satisfying the spark condition only in the case $\varepsilon = 0$. 

\section{Proof of Theorem~\ref{DeterministicUniquenessTheorem} and Corollary~\ref{SLCopt}}\label{DUT}

% ======== b - PDa =========
First note that $\|\mathbf{x}\|_1 \leq \sqrt{k} \|\mathbf{x}\|_2$ for $k$-sparse $\mathbf{x} \in \mathbb{R}^m$, which by definition of $L_k(A)$ implies the following often used inequality:
\begin{align}\label{delrho}
L_k(\mathbf{A}) \leq \frac{\|\mathbf{A}\mathbf{x}\|_2}{\sqrt{k} \|\mathbf{x}\|_2} %\leq \frac{\|\mathbf{x}\|_1}{\sqrt{k} \|\mathbf{x}\|_2} \max_{i \in [m]}\|\mathbf{A}_i\|_2 
\leq  \max_{j \in [m]}\|\mathbf{A}_j\|_2.
\end{align}

Our first step is to show how dictionary recovery \eqref{Cstable} already implies sparse code recovery \eqref{b-PDa} when $\varepsilon < L_{2k}(\mathbf{A}) / C_1$. For all $2k$-sparse $\mathbf{x} \in \mathbb{R}^m$, the triangle inequality gives \mbox{$\|(\mathbf{A}-\mathbf{BPD})\mathbf{x}\|_2  \leq C_1\varepsilon \|\mathbf{x}\|_1 \leq C_1 \varepsilon \sqrt{2k}  \|\mathbf{x}\|_2$}. Thus, 
\begin{align*}
\|\mathbf{BPD}\mathbf{x}\|_2 
&\geq | \|\mathbf{A}\mathbf{x}\|_2 - \|(\mathbf{A}-\mathbf{BPD})\mathbf{x}\|_2 | \\
&\geq \sqrt{2k} (L_{2k}(\mathbf{A}) -  C_1\varepsilon) \|\mathbf{x}\|_2.
\end{align*}
%
%where we drop the absolute value due to the upper bound on $\varepsilon$. 
Hence, $L_{2k}(\mathbf{BPD}) \geq L_{2k}(\mathbf{A}) - C_1\varepsilon  > 0$ and \eqref{b-PDa} then follows from:
\begin{align*}
\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i \|_1
%&\leq \sqrt{2k} |\mathbf{x}_i - D^{-1}P^{\top}\mathbf{\bar x}_i\|_2 \\
&\leq \frac{\|\mathbf{BPD}(\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i)\|_2}{L_{2k}(\mathbf{BPD})} \\
&\leq \frac{\|\mathbf{B}\mathbf{\bar x}_i - \mathbf{A}\mathbf{x}_i\|_2 + \|(\mathbf{A} - \mathbf{BPD})\mathbf{x}_i\|_2}{L_{2k}(\mathbf{BPD})} \\
&\leq \frac{\varepsilon (1+C_1 \|\mathbf{x}_i\|_1)}{L_{2k}(\mathbf{BPD})}.
%&\leq \frac{\varepsilon}{\sqrt{2k}} \left( \frac{ 1+C_1 \|\mathbf{x}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right).
%\leq \frac{\varepsilon }{\varepsilon_0 - \varepsilon} \left( C_1^{-1}+|\mathbf{x}_i\|_1 \right).
\end{align*}

The heart of the matter is therefore \eqref{Cstable}, which we now finally establish, but first in the important special case $k = 1$.

%In fact, in the case $k=1$ we can relax our assumptions even further and assume that the matrix $\mathbf{B}$ has only at least as many columns as $\mathbf{A}$; as we will see, the conclusions of Thm.~\ref{DeterministicUniquenessTheorem} then hold for some $n \times m$ submatrix of $\mathbf{B}$.  [*** ??? ***]

\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessCorollary} for $k=1$]
The only 1-uniform hypergraph with the SIP is $[m]$; hence, we have $\mathbf{x}_i = c_i \mathbf{e}_i$ for $c_i \in \mathbb{R} \setminus \{\mathbf{0}\}$, $i \in [m]$. In this case, we may take $C_1 = 1/ \min_{\ell \in [m]} |c_{\ell}|$. 

Fix $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} and suppose that for some $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ and $1$-sparse $\mathbf{\bar x}_i \in \mathbb{R}^{\bar m}$ we have  $\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon < L_2(\mathbf{A}) / C_1$ for all $i$. Then, there are $\bar{c}_1, \ldots, \bar{c}_m \in \mathbb{R}$ and a map $\pi: [m] \to [\bar m]$ with:
\begin{align}\label{1D}
\|c_j\mathbf{A}_j - \bar{c}_j\mathbf{B}_{\pi(j)}\|_2 \leq \varepsilon,\ \ \text{for $j \in [m]$}.
\end{align} 
Note that if $\bar{c}_j = 0$, then $\|c_j\mathbf{A}_j \|_2 \leq \varepsilon$ implies from \eqref{delrho} that $|c_j| < \min_{\ell \in [m]} | c_\ell |$, a contradiction.  Thus, $\bar{c}_j \neq 0$, $j \in [m]$.
%Note that $\bar{c}_j \neq 0$, since otherwise (by definition of $L_2(\mathbf{A})$) we reach the contradiction $|c_j| < \min_{\ell \in [m]} | c_\ell |$. %by \eqref{delrho} and definition of $C_1$ we would have $\|c_j\mathbf{A}_j\|_2 < \min_{\ell \in [m]}\|c_{\ell}\mathbf{A}_{\ell}\|_2$.  [*** TODO:  This doesn't seem correct ***]
%\begin{align*}
%|c_j| \sqrt{2} L_2(\mathbf{A}) \leq \|\mathbf{A}(c_j\mathbf{e}_j)\|_2 \leq \varepsilon < L_2(\mathbf{A}) \min_{\ell \in [m]} | c_\ell |.
%\end{align*}

We  now show that $\pi$ is injective (and thus is a permutation if $\bar m = m$). Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell$. Then, $\|c_{j}\mathbf{A}_{j} - \bar{c}_{j}\mathbf{B}_{\ell}\|_2 \leq \varepsilon$ and $\|c_{i}\mathbf{A}_{i} - \bar{c}_{i} \mathbf{B}_{\ell}\|_2  \leq \varepsilon$. Scaling and summing these inequalities by $|\bar{c}_{i}|$ and $|\bar{c}_{j}|$, respectively, and applying the triangle inequality, we obtain:
\begin{align*}%\label{contra}
(|\bar{c}_{i}| + |\bar{c}_{j}|) \varepsilon
&\geq\|\mathbf{A}(\bar{c}_{i}c_{j} \mathbf{e}_{j} - \bar{c}_{j}c_{i}\mathbf{e}_{i})\|_2 \nonumber \\ 
&\geq  \left( |\bar{c}_{i}| + |\bar{c}_{j}| \right) L_2(\mathbf{A}) \min_{\ell \in [m]} |c_\ell |,
\end{align*}
%
which contradicts the bound $\varepsilon < L_2(\mathbf{A})/C_1$. Hence, the map $\pi$ is injective and $\bar m \geq m$. Setting $J = [m]$, $\bar J = \pi([m])$, and letting $P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}\right)$ and $D = \text{diag}(\frac{\bar{c}_1}{c_1},\ldots,\frac{\bar{c}_m}{c_m})$, we see that \eqref{1D} becomes, for all $j \in [m]$:
\begin{align*}%\label{k=1result}
\|\mathbf{A}_j - (\mathbf{B}_J\mathbf{PD})_j\|_2 
= \|\mathbf{A}_j - \frac{\bar{c}_j}{c_j}\mathbf{B}_{\pi(j)}\|_2 
\leq \frac{\varepsilon}{|c_j|} 
\leq C_1\varepsilon.
\end{align*}
\end{proof}

%\begin{remark}
%The above arguments can be easily modified to show that the conclusions of Thm.~\ref{DeterministicUniquenessTheorem} in this case ($k=1$) hold for some $n \times m$ submatrix of $\mathbf{B}$ in the event where only an upper bound on the number of columns in $\mathbf{A}$ is known; i.e., the recovered matrix $\mathbf{B}$ is set to have as many or more columns than $\mathbf{A}$. 
%\end{remark}

%It is easy to see that when $m < m'$, the above result holds for the submatrix of $B$ composed of columns indexed by the image of $\pi$.

We require a few additional tools to extend the proof to the general case $k < m$. These include a general notion of distance (Def.~\ref{dDef}) and angle (Def.~\ref{FriedrichsDefinition}) between subspaces and a stability result in matrix analysis (Lem.~\ref{MainLemma}).

\begin{definition}\label{dDef}
For $\mathbf{u} \in \mathbb R^m$, let $\text{\rm dist}(\mathbf{u}, V) := \inf \{\| \mathbf{u}-\mathbf{v} \|_2: \mathbf{v} \in V\}$, and for vector spaces $U,V \subseteq \mathbb{R}^m$, define:
%\footnote{Although identical, note the reference \cite{Morris10} defines the supremum over the unit ball.}: %[Kato p.197]
\begin{align}\label{d}
d(U,V) := \sup_{\mathbf{u} \in U, \ \|\mathbf{u}\|_2 \leq 1} \text{\rm dist}(\mathbf{u},V).
\end{align}
% Note that $d(U,\{\textbf{0}\}) = 1$ for $U \neq \{\mathbf{0}\}$, whereas if $U = \{\textbf{0}\}$ then $d$ has no meaning; in this case, set $d(\{\textbf{0}\},V) = 0$ for any $V$.
\end{definition}
%Note that the supremum is always achievable by a point in $U$ when $U, V \subseteq \mathbb{R}^m$.

We note the following facts about $d$. For subspaces $U,V \subseteq \mathbb{R}^m$, we have \cite[Cor.~2.6]{Kato2013}:
\begin{align}\label{dimLem}
d(U,V) < 1 \implies \dim(U) \leq \dim(V),
\end{align}
%
and \cite[Lem.~3.2]{Morris10}:
\begin{align}\label{eqdim}
\dim(U) = \dim(V) \implies d(U,V) = d(V,U).
\end{align}

%\begin{lemma}\label{dimLem} %Kato p.200, 
%\cite[Cor.~2.6]{Kato2013} For subspaces $U,V \subseteq \mathbb{R}^m$, $d(U,V) < 1$ implies $\dim(U) \leq \dim(V)$. 
%\end{lemma}

%\begin{lemma}\label{eqdim}
%\cite[Lem.~3.2]{Morris10} If $\dim(U) = \dim(V)$ then $d(U,V) = d(V,U)$. 
%\end{lemma}

Our result in combinatorial matrix theory is the following.

%, which we derive by the following arguments (see the Supplemental for the full proof). First, we show that the aforementioned proximity between $k$-dimensional subspaces implies a proximity between smaller subspaces spanned by columns of $\mathbf{A}$ indexed by the intersections of sets in $E$ and those spanned by as many or fewer columns of $\mathbf{B}$. Another pigeonholing argument here combined with our assumptions on $E$ (e.g., the singleton intersection property) reveals that, in fact, each column of $\mathbf{A}$ spans a subspace proximal to that spanned by some column of $\mathbf{B}$. \eqref{Cstable} is a simple consequence of this last fact, which actually constitutes the proof of the theorem for the case $k=1$. % We present this special case now before restating the above arguments in greater detail.

\begin{lemma}\label{MainLemma}
%Suppose $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfies the spark condition \eqref{SparkCondition} for some $k < m$ and that $E \subseteq {[m] \choose k}$ is $\ell$-regular and satisfies the SIP. There is a constant $C_2 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_2$. 
Suppose $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfies \eqref{SparkCondition} for some $k < m$ and that $E \subseteq {[m] \choose k}$ is $\ell$-regular with the SIP. There exists $C_2 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_2$. 


If for some  $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ and map $\pi: E \mapsto {[\bar m] \choose k}$ we have:
\begin{align}\label{GapUpperBound}
d(\text{\rm Span}\{\mathbf{A}_{S}\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \leq \varepsilon, \ \  \text{for $S \in E$},
\end{align}
%
then $\bar m \geq m$, and if $p = \bar m - \ell(\bar m - m)$ is positive:
\begin{align}\label{MainLemmaBPD}
\|(\mathbf{A}_J - \mathbf{B}_{\bar J}\mathbf{PD})_j\|_2 \leq C_2 \varepsilon, \ \  \text{for } j \in J,
\end{align}
for some $J \in {[m] \choose p}$, $\bar J \in {[\bar m] \choose p}$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.
\end{lemma}

%\begin{lemma}\label{MainLemma}
%Fix integers $n$ and $k < m$, and suppose $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfies the spark condition \eqref{SparkCondition}. There exists a constant $C_2 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_2$. If for some  $\mathbf{B} \in \mathbb{R}^{n \times m}$ and regular $E \subseteq {[m] \choose k}$ with the SIP there exists a map $\pi: E \mapsto {[m] \choose k}$ satisfying:
%\begin{align}\label{GapUpperBound}
%d(\text{\rm Span}\{\mathbf{A}_{S}\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \leq \varepsilon,\ \ \   \text{for all $S \in E$},
%\end{align}
%
%then there exists a permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ with:
%\begin{align}\label{MainLemmaBPD}
%\|\mathbf{A}_j - (\mathbf{BPD})_j\|_2 \leq C_2 \varepsilon, \ \ \  \text{for all } j \in [m].
%\end{align}
%\end{lemma}

Given vectors $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$, we let $\mathbf{X}$ be the $m \times N$ matrix with columns $\mathbf{x}_i$ and $I(S) := \{i \in [N]: \text{supp}(\mathbf{x}_i) \subseteq S\}$. The constant $C_1$ in Thm.~\ref{DeterministicUniquenessTheorem} is then defined by\footnote{Note that $\|\mathbf{AX}_{I(S)}\mathbf{c}\|_2 \geq \sqrt{k} L_k(\mathbf{A})\|\mathbf{X}_{I(S)}\mathbf{c}\|_2 \geq k L_k(\mathbf{A}) L_k(\mathbf{X}_{I(S)})\|\mathbf{c}\|_2$ for $S \in E$ and $k$-sparse $\mathbf{c}$. Therefore, $\sqrt{k} L_k(\mathbf{AX}_{I(S)}) \geq k L_k(\mathbf{A}) L_k(\mathbf{X}_{I(S)}) > 0$ since $L_k(\mathbf{A}), L_k(\mathbf{X}_{I(S)}) > 0$ by \eqref{SparkCondition} and general linear position of the $\mathbf{x}_i$.  Thus, $C_1 > 0$.  }:
\begin{align}\label{Cdef1}
C_1(\mathbf{A}, \{\mathbf{x}_i\}_{i=1}^N, E) := \frac{ C_2(\mathbf{A}, E) } { \min_{S \in E} L_k(\mathbf{AX}_{I(S)}) }.
\end{align}

The constant $C_2 = C_2(\mathbf{A}, E)$ is given below in terms of one used in \cite{Deutsch12} to analyze the convergence of the alternating projections algorithm.
% for projecting a point onto the intersection of a set of subspaces. 
We use it to bound the distance between a point and the intersection of a set of subspaces given an upper bound on its distance from each subspace individually.

\begin{definition}\label{SpecialSupportSet}\label{FriedrichsDefinition}
For subspaces $V_1, \ldots, V_\ell \subseteq \mathbb{R}^m$, set $r := 1$ when $\ell = 1$ and define for $\ell \geq 2$:
\begin{align*}
r(\{V_i\}_{i=1}^\ell) := 1 - \left(1 -  \max \prod_{i=1}^{\ell-1} \sin^2  \theta \left(V_i, \cap_{j>i} V_j \right)  \right)^{1/2},
\end{align*} 
%
where the maximum is taken over all orderings\footnote{We modify the quantity in \cite{Deutsch12} in this way since the subspace ordering is irrelevant to our purpose.} of the $V_i$ and the angle $\theta \in (0,\frac{\pi}{2}]$ is defined implicitly as \cite[Def.~9.4]{Deutsch12}:
\begin{align*}
\cos{\theta(U,W)} := \max\left\{ |\langle \mathbf{u}, \mathbf{w} \rangle|: \substack{ \mathbf{u} \in U \cap (U \cap W)^\perp, \ \|\mathbf{u}\|_2 \leq 1 \\ \mathbf{w} \in W \cap (U \cap W)^\perp, \  \|\mathbf{w}\|_2 \leq 1 } \right\}.
\end{align*}
\end{definition}
Note that $\theta \in (0,\frac{\pi}{2}]$ implies $0 < r \leq 1$.\footnote{We acknowledge the counter-intuitive property that $\theta =  \pi/2$ when $U = V$ or $U \perp V$.}  %\textbf{Note:} we can also define $\theta$ by \cite[Lem.~9.5]{Deutsch12}:
%\begin{align*}
%\cos{\theta(U,W)} := \max\left\{ |\langle \mathbf{u}, \mathbf{w} \rangle|: \mathbf{u} \in U \cap (U \cap W)^\perp, \|\mathbf{u}\|_2 \leq 1, \mathbf{w} \in W \|\mathbf{w}\|_2 \leq 1  \right\}.
%\end{align*}
The constant $C_2$ in Lem.~\ref{MainLemma} can then be expressed as:  % \footnote{Note that $C_2 > 0$ is well-defined since $r > 0$ by definition.}
\begin{align}\label{Cdef2}
C_2(\mathbf{A}, E) := \frac{ 2^{|E|} \max_{j \in [m]} \|\mathbf{A}_j\|_2}{ \min_{F \subseteq E} r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F}) }.
\end{align}
Note that $k=1$ gives the same $C_1$ as that used to prove $k=1$.

\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessCorollary} for $k < m$] 
We shall show that for every $S \in E$ there is some $\bar S \in {[\bar m] \choose k}$, for which the distance $d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\bar S}\})$ is controlled by $\varepsilon$. Applying Lem.~\ref{MainLemma} with the map $\pi$ defined by $S \mapsto \bar S$ then completes the proof.  

Since there are $(k-1){\bar m \choose k}+1$ vectors $\mathbf{x}_i$ supported on $S$, the pigeonhole principle implies that there is some $\bar S \in {[\bar m] \choose k}$ and some set of $k$ indices $K$ such that the supports of all $\mathbf{x}_i$ and $\mathbf{\bar x}_i$ with $i \in K$ are contained in $S$ and $\bar S$, respectively.

%Let $\mathbf{X}$ and $\mathbf{\bar{X}}$ be the $m \times N$ matrices with columns $\mathbf{x}_i$ and $\mathbf{\bar x}_i$, respectively. 
It follows from the general linear position of the $\mathbf{x}_i$ and the linear independence of every $k$ columns of $\mathbf{A}$ that $L_k(\mathbf{AX}_{K}) > 0$; that is, the columns of the $n \times k$ matrix $\mathbf{AX}_K$ form a basis for $\text{\rm Span}\{\mathbf{A}_{S}\}$. Fixing $\mathbf{0} \neq \mathbf{y} \in \text{\rm Span}\{\mathbf{A}_{S}\}$, there exists $\mathbf{0} \neq \mathbf{c} = (c_1, \ldots, c_k) \in \mathbb{R}^k$ such that $\mathbf{y} = \mathbf{AX}_K\mathbf{c}$. Setting \mbox{$\mathbf{\bar{y}} = \mathbf{B\bar{X}}_K\mathbf{c} \in \text{\rm Span}\{\mathbf{B}_{\bar S}\}$}, we have:
\begin{align*}
\|\mathbf{y} - \mathbf{\bar{y}}\|_2 
&= \|\sum_{i=1}^k c_i(\mathbf{AX}_K - \mathbf{B\bar{X}}_K)_i\|_2
\leq \varepsilon \sum_{i=1}^k |c_i| \\
&\leq \varepsilon \sqrt{k}  \|\mathbf{c}\|_2 
%\leq \frac{\varepsilon \sqrt{k}}{L(\mathbf{A}X_{J(S)})} \|\mathbf{A}X_{J(S)}\mathbf{c}\|_2 
\leq \frac{\varepsilon}{L_k(\mathbf{AX}_K)} \|\mathbf{y}\|_2,
\end{align*}
where the last inequality follows directly from the definition of $L_k$. Finally, from Def.~\ref{dDef}, it follows that:
\begin{align}\label{rhs222}
d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\bar S}\}) 
\leq \frac{\varepsilon}{  L_k(\mathbf{AX}_{K}) }.
\end{align}
In particular, with $C_1$ as in \eqref{Cdef1}, the theorem follows.
\end{proof}
% for all $S \in E$:
%\begin{align}\label{rhs222}
%d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\bar S}\}) 
%\leq \frac{\varepsilon}{ \min_{S \in E} L_k(\mathbf{AX}_{I(S)}) }.
%\end{align}
%\end{proof}

%We then show that the map over subsets of column indices defined by this association is such that the number of common elements shared by sets in its domain bounds the number of elements common to their images under this map. 
 
 \begin{proof}[Proof of Cor.~\ref{SLCopt}]
We bound the number of $k$-sparse $\mathbf{\bar x}_i$ and then apply Thm.~\ref{DeterministicUniquenessCorollary}. 
Let $n_p$ be the number of $\mathbf{\bar x}_i$ with $\|\mathbf{\bar x}_i\|_0 = p$.  %, so that $\sum_{p = 1}^{m} n_p = N$. 
Since the $\mathbf{x}_i$ are all $k$-sparse, by \eqref{minsum} we have:
%\begin{align}
\mbox{$k \sum_{p = 0}^{m} n_p \geq \sum_{i=0}^N \|\mathbf{x}_i\|_0 \geq \sum_{i=0}^N \|\mathbf{\bar x}_i\|_0 = \sum_{p=0}^{m} p n_p.$}
%\end{align}
Hence,
\begin{align}\label{eqn}
\sum_{p = k+1}^m n_p \leq \sum_{p = k+1}^m (p-k) n_p \leq \sum_{p = 0}^k (k-p)n_p \leq k \sum_{p = 0}^{k-1} n_p.
\end{align}

We now show that no more than $(k-1)|E|$ of the $\mathbf{\bar x}_i$ share a support of size less than $k$. 
%Let $\mathbf{X}$ and $\mathbf{\bar{X}}$ be the $m \times N$ and $\bar m \times N$ matrices with columns $\mathbf{x}_i$ and $\mathbf{\bar x}_i$, respectively, and let $I(S) = \{i \in [N]: \text{supp}(\mathbf{x}_i) \subseteq S\}$. 
From previous arguments, for every $S \in E$, the columns of each $n \times k$ submatrix of $\mathbf{AX}_{I(S)}$ form a basis for $\text{\rm Span}\{\mathbf{A}_S\}$.
% the general linear position of the $\mathbf{x}_i$ and the linear independence of every $k$ columns of $\mathbf{A}$ that $L_k(\mathbf{AX}_{I(S)}) > 0$ for all $S \in E$; that is, for every $S \in E$, the columns of every $n \times k$ submatrix of $\mathbf{AX}_{I(S)}$ form a basis for $\text{\rm Span}\{\mathbf{A}_S\}$. 
Suppose that more than $(k-1)|E|$ vectors $\mathbf{\bar x}_i$ share a support $\bar S$ of size $|\bar{S}| < k$. By the pigeonhole principle, there is some $S \in E$ supporting $k$ or more of the corresponding $\mathbf{x}_i$; let them be indexed by $K \subseteq I(S)$.  By the argument as in the proof of Thm.\ref{DeterministicUniquenessTheorem}, we also have \eqref{rhs222}. Since our bound on $\varepsilon$ implies that the right-hand side of \eqref{rhs222} is less than one, by \eqref{dimLem}: \[k = \dim(\text{Span}(\mathbf{A}_S)) \leq \dim(\text{Span}\{\mathbf{B}_{\bar S}\}) \leq |\bar S|,\] a contradiction. 

Hence, the number of $(k-1)$-sparse vectors $\mathbf{\bar x}_i$ is no greater than $|E|(k-1){ m \choose k-1}$. Taking \eqref{eqn} into account, it follows that at least $N - |E|k(k-1){ m \choose k-1}$ of the vectors $\mathbf{\bar  x}_i$ are $k$-sparse. Therefore, more than $(k-1){m \choose k}$ of the vectors $\mathbf{x}_i$ supported on each $S \in E$ have corresponding $\mathbf{\bar x}_i$ that are also $k$-sparse. The result now follows directly from Thm.~\ref{DeterministicUniquenessCorollary}.
\end{proof}


 
\section{Discussion}\label{Discussion}


In this note, we generalize the approach of \cite{Hillar15} to prove uniqueness in Prob.~\ref{InverseProblem} for the case of noisy measurements while also reducing the sufficient number of samples.
% FRITZ: Inverse problems instead of Data Analysis. Don't assume that everyone is assuming they are recovering ground truth. Cite results again and state implications! Focus on them (probabilistic ones too).
Our results provide theoretical grounding for the use of sparse linear coding in blind source separation, wherein the goal is to infer the generating dictionary and sparse codes from noisy measurements, \eqref{LinearModel}.  We also collect a set of useful mathematical tools and basic facts for future research.
The main motivation for this work, however, was to understand how seemingly universal and stable representations are discovered by training a sparse linear coding model on natural data, independent of specific algorithmic implementation.    We elaborate more on these applications below after discussing  theoretical aspects.  % We close with several application areas.

%\textbf{Inverse Problems}.  
% from $N=k{m \choose k}^2$ to $N = m(k-1){m \choose k}+m$. 
% CUT OK? Surprisingly, almost all $n \times m$ dictionaries satisfying the standard assumption \eqref{CScondition} from compressed sensing (CS) are identifiable from enough generic noisy $k$-sparse linear combinations of their elements, up to an error linear in the noise. Moreover, if solutions are constrained to satisfy \eqref{SparkCondition}, then only an upper bound on the number of dictionary elements need be taken as given. 

What we have shown here is that as an inverse problem to be approximated by a numerical algorithm, the sparse linear coding model  generally produces \textit{well-posed} optimizations.  This early concept of Hadamard \cite{Hadamard1902} can be paraphrased as the idea that good parameterized models should pose optimization problems with solutions that are \textit{stable}; that is, continuous in the data.  In other words, perturbing a dataset by a small amount should only slightly influence the identifiability or uniqueness of parameters in the model, rather than discontinuously affect the character of optimization involved.  In this regard, we demonstrate a linear relationship, \eqref{epsdel}, between noise in the data and the unique solution in  sparse linear coding, with explicit constants expressed in terms of original latent parameters. 
Nonetheless, it would be of practical utility to determine the best possible dependence of $\varepsilon$ on $\delta_1, \delta_2$ in Def.\ref{maindef} as well as the minimal requirements on the number and diversity of generating codes, and we hope that other researchers continue to improve and extend our results.
% , although they are too computationally expensive to be of practical value.  We hope .

One notable component of our contribution is a combinatorial criteria (regular hypergraphs satisfying the singular intersection property, Def.~\ref{sip}) for choosing support sets for original sparse codes. Fully understanding those combinatorial designs allowing for stable representations of datasets is an interesting research area for the future.  For instance, whether there is a support set size $N$ that is polynomial in $m,k$ gauanteeing the conclusions of Thm.~\ref{DeterministicUniquenessTheorem} has implications for the computational complexity of Prob.\ref{InverseProblem} and related questions \cite{Tillmann15}.  We also remark that our main uniqueness result accounts for deterministic ``worst-case'' noise, whereas the ``effective'' noise might be much smaller when it is sampled from a given distribution; in such cases, the constants  %Thms.~\ref{DeterministicUniquenessTheorem},~\ref{DeterministicUniquenessTheorem2} 
will improve. %We note also that these results extend trivially to cases where point-wise injective nonlinearities are applied to the data. 

A technical difficulty in our main theorem is the absence of a spark condition assumption on alternate sparse data factorizations.
Although mathematically interesting that no such requirement is necessary, there are other reasons to seek out such a theoretical guarantee. For instance, it is difficult to ensure an algorithm maintain a dictionary satisfying \eqref{SparkCondition} at each learning stage; indeed, even certifying  a dictionary has this property is likely intractable given its NP-hardness \cite{tillmann2014computational}.

In fact, though, uniqueness guarantees with minimal assumptions apply to all areas of data science and engineering that utilize learned sparse representations.  For example, several groups have utilized compressed sensing for signal processing tasks: MRI analysis \cite{lustig2008compressed},  image compression \cite{Duarte08}, and, more recently, the design of an ultrafast camera \cite{Gao14}. Given such effective uses of CS, it is only a matter of time before these systems incorporate sparse representational learning to encode and process data (e.g., in a device that learns structure from motion \cite{kong2016prior}).  In these cases, assurances such as those offered by our theorems certify that different devices (with different initialization, samples, etc.) are equivalent as soon as enough data originate from a statistically identical system.
%Data science:
%  
%in a common application of compressed sensing in engineering 

%\textbf{Theoretical Neuroscience}.
Additionally, in the field of theoretical neuroscience, sparse dictionary learning and related methods have recovered characteristic components of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent} and sounds \cite{bellsejnowski1996, smithlewicki2006, Carlson12}, reproducing response properties of cortical neurons. Our results suggest that this correspondence could be due to  sparse representations being ``universal'' in natural data, an early mathematical idea in neural theory \cite{pitts1947}. Furthermore, our guarantees justify the hypothesis of \cite{Coulter10, Isely10} that sparse codes passed through a communication bottleneck in the brain are recovered from random projections via (unsupervised) biologically plausible sparse dictionary learning (e.g., \cite{rehnsommer2007, rozell2007neurally, hu2014hebbian}).  

%In fact, the concept of ``universal'' objects in the brain is . 

%But this is also in data science applications.  For instance, when recovering a rat's position on a linear track from learning a sparse linear model on  local field potentials in Hippocampus \cite{Agarwal14}.

%TODO: add reproduction of response propeties in olfactory cortex?

% Reiterate probabilistic result in the smooth analysis part?
%\textbf{Smoothed Analysis}.
%The main concept in smoothed analysis \cite{Spielman04} is that certain algorithms having exponential worst-case behavior are, nonetheless, efficient if certain (typically, measure zero in the continuous case and with ``low probability" in the discrete case) pathological input sets are avoided. Our results imply that if there is an efficient ``smoothed" algorithm for solving Prob.~\ref{InverseProblem} given enough samples, then for generic inputs this algorithm determines the unique original solution. We note that avoiding certain pathological sets of inputs is often a necessary technicality for dictionary learning \cite{Razaviyayn15, Tillmann15}.

%\textbf{Engineering}.


\acknow{We thank Friedrich Sommer for introducing us to sparse dictionary learning, Darren Rhea for sharing early explorations, and Ian Morris for posting \eqref{eqdim} online.}

\showacknow % Display the acknowledgments section

% \pnasbreak % splits and balances the columns before the references.
% If you see unexpected formatting errors, try commenting out this line
% as it can run into problems with floats and footnotes on the final page.
%\pnasbreak %was causing only first two pages to print..??

% Bibliography
\bibliography{chazthm_pnas}

%% SUPPLEMENTAL INFO? %%%
%\begin{remark}
%Actually, $\mathbf{A}$ need not be injective on all $k$-sparse vectors for Theorem \ref{DeterministicUniquenessTheorem} to hold; rather, it need only be injective the set of all vectors with supports in $E$. [** TODO ** we will probably have to redefine $L_2$ since it is over all $k$-sparse vectors -- may have to instead define $L_E$.] Wait, $E$ need not be regular for $B$ sat. spark cond. Also it need not be $k$-regular!
%\end{remark}

\clearpage

\section{Proof of Main Lemma}

In this supplemental, we prove Lem.\ref{MainLemma} after stating auxiliary lemmas.

\begin{lemma}\label{SpanIntersectionLemma}
Let $\mathbf{M} \in \mathbb{R}^{n \times m}$. If every $2k$ columns of $\mathbf{M}$ are linearly independent, then for any $F \subseteq {[m] \choose k}$:
\begin{align*}
\text{\rm Span}\{\mathbf{M}_{\cap F}\}  = \bigcap_{S \in F} \text{\rm Span}\{\mathbf{M}_S\}.
\end{align*}
\end{lemma}
\begin{proof}
Using induction on $|F|$, it is enough to prove the lemma when $|F| = 2$; but this case follows directly from the assumption.
\end{proof}

% This can be made tighter by using Pythagoras' Thm for first projection instead of triangle inequality. 
\begin{lemma}\label{DistanceToIntersectionLemma}
Fix $k \geq 2$. Let $V_1, \ldots, V_k$ be subspaces of $\mathbb{R}^m$ and set $V = \cap_{i = 1}^k V_i$. For  $\mathbf{x} \in \mathbb{R}^m$, we have (where $r$ is given in Def.~\ref{SpecialSupportSet}):
\begin{align}\label{DTILeq}
\text{\rm dist}(\mathbf{x}, V) \leq \frac{1}{r(\{V_i\}_{i = 1}^k)} \sum_{i=1}^k \text{\rm dist}(\mathbf{x}, V_i).
\end{align}
\end{lemma}
\begin{proof} 
Recall that orthogonal projection onto a subspace $V \subseteq \mathbb{R}^m$ is the mapping $\Pi_V: \mathbb{R}^m \to V$ that associates with each $\mathbf{x}$ its unique nearest point in $V$; i.e., $\|\mathbf{x} - \Pi_V\mathbf{x}\|_2 = \text{\rm dist}(\mathbf{x}, V)$.
% Since subspaces of real vector spaces are closed, we can replace inf with min in def. of orthogonal projection
Next, observe:
%Use Pythagoras' Theorem first?
\begin{align}\label{f}
\|\mathbf{x} - \Pi_V\mathbf{x}\|_2 &\leq \|\mathbf{x} - \Pi_{V_k} \mathbf{x}\|_2 + \|\Pi_{V_k}  \mathbf{x} - \Pi_{V_k}\Pi_{V_{k-1}}\mathbf{x}\|_2 \nonumber \\
&\ \ \ + \cdots + \|\Pi_{V_k} \Pi_{V_{k-1}}\cdots \Pi_{V_1} \mathbf{x} - \Pi_V \mathbf{x}\|_2 \nonumber \\
&\leq \|\Pi_{V_k}\cdots\Pi_{V_{1}} \mathbf{x} - \Pi_V \mathbf{x}\|_2 + \sum_{\ell=1}^k \|\mathbf{x} - \Pi_{V_{\ell}} \mathbf{x}\|_2,
\end{align}
%
using the triangle inequality and that the spectral norm satisfies $\|\Pi_{V_{\ell}}\|_2 \leq 1$ for all $\ell$ (since $\Pi_{V_{\ell}}$ are orthogonal projections).

The desired result, \eqref{DTILeq}, now follows by bounding the second term on the right-hand side using the following fact \cite[Thm.~9.33]{Deutsch12}:
\begin{align}
\|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} \mathbf{x} - \Pi_V\mathbf{x}\|_2 \leq z \|\mathbf{x}\|_2, %  \ \ \  \text{for } \mathbf{x} \in \mathbb{R}^m,
\end{align}
for \mbox{$z^2= 1 - \prod_{\ell =1}^{k-1}(1-z_{\ell}^2)$} and \mbox{$z_{\ell} = \cos\theta\left(V_{\ell}, \cap_{s=\ell+1}^k V_s\right)$}. Together with $\Pi_{V_\ell} \Pi_V = \Pi_V$ for all $\ell \in [k]$ and $\Pi_V^2 = \Pi_V$ yields:
\begin{align*}
\|\Pi_{V_k} \cdots \Pi_{V_1}\mathbf{x}  - \Pi_V \mathbf{x} \|_2 
&= \|\left( \Pi_{V_k} \cdots\Pi_{V_1} - \Pi_V \right) (\mathbf{x} - \Pi_V\mathbf{x})\|_2 \\
&\leq z\|\mathbf{x} - \Pi_V\mathbf{x}\|_2.
\end{align*}

Finally, substituting this into \eqref{f} and rearranging produces \eqref{DTILeq} after replacing $1 - z$ with $r(\{V_i\}_{i=1}^k)$.
% ($z$ may depend on the ordering).
\end{proof}

\begin{lemma}\label{NonEmptyLemma} 
Fix $\ell, m$, and $\bar m$. Suppose $E \subseteq 2^{[m]}$ is $\ell$-regular with the SIP.  If $\pi: E \to 2^{[\bar m]}$ has $\sum_{S \in E} |S| = \sum_{S \in E} |\pi(S)|$ and:
\begin{align}\label{cond}
|\bigcap_{S \in F} \pi(S)| \leq |\bigcap_{S \in F} S |,\ \ \   \text{for } F \subseteq E,
\end{align}
%
then $\bar m \geq m$ and the association $i \mapsto \cap \pi(F(i))$ defines an injective map from $J$ to $[\bar m]$ for some $J \subseteq [m]$ of size $\bar m - \ell(\bar m - m)$. In particular, if $\bar m = m$ then the map $\pi$ is induced by a permutation.
%$\pi(E)$ is $\ell$-regular and satisfies the singleton intersection property. In particular, $|\cap_{S \in F} S| = 1$ if and only if $|\cap_{S \in F} \pi(S)| = 1$. 
\end{lemma}
\begin{proof}
We first show that $\bar m \geq m$. Fix $T = \{(j, S): j \in \pi(S), S \in E\}$. Then $|T| = \sum_{S \in E} |\pi(S)| = \sum_{S \in E} |S| = \sum_{i \in [m]} \deg(i) = \ell m$ by regularity of $E$. If $\bar m < m$ then by pigeonholing the $m \ell \geq \ell (\bar m + 1) \geq [(\ell + 1) - 1] \bar m + 1$ elements of $T$ with respect to the set $[\bar m]$ of possible first indices, we see that there must exist at least $\ell + 1$ elements of $T$ having the same first index. But by \eqref{cond} there can be no more than $\ell$ elements of $T$ with a given first index, since $E$ is $\ell$-regular. Hence $\bar m \geq m$. 
Again, pigeonholing the elements of $T$ with respect to the set $[\bar m]$ of possible first indices, we see that for each $j$ there must be exactly $\ell$ elements of $T$ with $j$ as a first index; hence, $\pi(E)$ is $\ell$-regular. 
Fix $j$ and let $F(j) = \{S \in E: j \in \pi(S)\}$ be the preimage of the star in $\pi(E)$ centered at $j$, which is of size $\ell$ by the above arguments. It follows from \eqref{cond} that $\cap_{S \in F(j)} S$ is nonempty. In fact, we must have $|\cap _{S \in F(j)} S| = 1$, since $E$ is $\ell$-regular and satisfies the singleton intersection property. It follows by \eqref{cond} that $\cap_{S \in F(j)} \pi(S) = \{j\}$. Thus $\pi(E)$ satisfies the singleton intersection property as well, and the preimage of every one of the $m$ stars in $\pi(E)$ is a star in $E$. It remains to show that every star in $E$ maps through $\pi$ to some star in $\pi(E)$. This follows by pigeonholing the $m$ stars of $\pi(E)$ with respect to their $m$ possible preimages if no two stars in $\pi(E)$ share the same preimage. This indeed must be the case, since $F(i) = F(j)$ implies $\{i\} = \cap_{S \in F(i)} \pi(S) = \cap_{S \in F(j)} \pi(S) = \{j\}$.
\end{proof}

\begin{proof}[Proof of Main Lemma]
We first claim $\dim(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) = \dim(\text{\rm Span}\{\mathbf{A}_{S}\})$ for all $S \in E$. To see why, note that the right-hand side of \eqref{GapUpperBound} is then strictly less than one, since $r \leq 1$ and $L_2(\mathbf{A}) \leq 1$ by \eqref{delrho}. By \eqref{dimLem}, it then follows that $|\pi(S)| \geq \dim(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \geq \dim(\text{\rm Span}\{\mathbf{A}_{S}\}) = |S|$, with the equality due to $\mathbf{A}$ satisfying \eqref{SparkCondition}; hence, $\dim(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) = \dim(\text{\rm Span}\{\mathbf{A}_{S}\})$ (since $|S| = |\pi(S)|$). It follows that the columns of $\mathbf{B}_{\pi(S)}$ are linearly independent and, by \eqref{eqdim}:% and $\mathbf{B}_i \neq \textbf{0}$ for all $i$.
\begin{align}\label{eq2}
d(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}, \text{\rm Span}\{\mathbf{A}_S\} ) = d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}).
\end{align}
%We will now show that \eqref{cond} holds.
%\begin{align}\label{fact2}
%|\bigcap_{S \in F} \pi(S)| \leq |\bigcap_{S \in F} S |,\ \ \   \text{for all } \ F \subseteq E.
%\end{align}

Fix $F \subseteq E$. Since $\text{\rm Span}\{\mathbf{B}_{\cap_{S \in F}\pi(S)}\} \subseteq \cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}$, if $\cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\} = \textbf{0}$ then we must have $|\cap_{S \in F} \pi(S)| = 0$ (as $\mathbf{B}_i \neq \textbf{0}$ for all $i$) and \eqref{cond} trivially holds. Suppose then that the intersection is not the zero vector. By Lem.~\ref{SpanIntersectionLemma} and Lem.~\ref{DistanceToIntersectionLemma}, and incorporating \eqref{eq2} and \eqref{GapUpperBound}, we have:
\begin{align}\label{randoml}
d( \text{\rm Span}&\{\mathbf{B}_{\cap_{S \in F}\pi(S)}\}, \text{\rm Span}\{\mathbf{A}_{\cap_{S \in F} S}\}  ) \nonumber \\
&\leq d\left( \cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}, \cap_{S \in F} \text{\rm Span}\{\mathbf{A}_{S}\} \right) \nonumber \\
&\leq \sum_{S \in F} \frac{ d\left( \cap_{T \in F} \text{\rm Span}\{\mathbf{B}_{\pi(T)}\},\text{\rm Span}\{\mathbf{A}_{S}\} \right) }{ r( \{ \text{\rm Span}\{\mathbf{A}_{T}\}_{T \in F}) } \nonumber \\
&\leq \sum_{S \in F} \frac{ d\left( \text{\rm Span}\{\mathbf{B}_{\pi(S)}\},\text{\rm Span}\{\mathbf{A}_{S}\} \right) }{ r( \{ \text{\rm Span}\{\mathbf{A}_{T}\}_{T \in F}) }\nonumber \\
%&\leq \frac{1}{r( \{ \text{\rm Span}\{\mathbf{A}_{S}\}_{S \in F})} \sum_{S \in F} \frac{ \varepsilon }{ L(\mathbf{AX}_{J(S)}) } \nonumber \\
&\leq \frac{|F| \varepsilon}{r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F})} 
%&< L_2(\mathbf{A}) \frac{|F| }{ 2^{|E|}} \left( \frac{ \min_{\substack{F \subseteq E}} r( \{ \text{\rm Span}\{\mathbf{A}_{S}\}_{S \in F}) }{ r( \{ \text{\rm Span}\{\mathbf{A}_{S}\}_{S \in F}) } \right) \left( \frac{\min_{S \in E} L_k(\mathbf{AX}_{I(S)}) }{ \min_{T \in F} L(\mathbf{AX}_{J(T)}) } \right) \nonumber \\
\leq C_2 \varepsilon. 
\end{align}
Note that since $\varepsilon < L_2(\mathbf{A}) / \tilde C_2$, by \eqref{delrho} the right-hand side in \eqref{randoml} is strictly less than one, so \eqref{dimLem} implies that $\dim(\text{\rm Span}\{\mathbf{B}_{\cap_{S \in F}\pi(S)}\}) \leq \dim(\text{\rm Span}\{\mathbf{A}_{\cap_{S \in F} S}\})$ and \eqref{fact2} follows from the linear independence of the columns of $\mathbf{A}_{S}$ and $\mathbf{B}_{\pi(S)}$ for all $S \in F$.
Now, fix $\ell \in [m]$. Since $E$ satisfies the singleton intersection property, $\{\ell\} = \cap F(\ell)$ for the star $F(\ell) \subseteq E$. By \eqref{cond}, we have that $\cap_{S \in F(\ell)} \pi(S)$ is either empty or it contains a single element. Lem.~\ref{NonEmptyLemma} ensures the latter case is the only possibility. Thus, the association $\ell \mapsto \cap_{S \in F(\ell)} \pi(S)$ defines a map $\bar \pi: [m] \to [m]$ and $\dim(\text{\rm Span}\{\mathbf{B}_{\bar \pi(\ell)}\}) = 1$, since $\mathbf{B}_i \neq \textbf{0}$ for all $i$. By \eqref{eqdim}, it follows from \eqref{randoml} that $d\left( \text{\rm Span}\{\mathbf{A}_\ell\}, \text{\rm Span}\{ \mathbf{B}_{\bar \pi(\ell)} \} \right) \leq C_2 \varepsilon$. Since $\ell$ is arbitrary, fixing $\bar \varepsilon = C_2\delta$ it follows from \eqref{randoml} that for every basis vector $\mathbf{e}_\ell \in \mathbb{R}^m$ there exists some $c'_\ell \in \mathbb{R}$ such that $\|\mathbf{A}\mathbf{e}_\ell - c'_\ell \mathbf{B}\mathbf{e}_{\bar \pi(\ell)}\|_2 \leq \bar \varepsilon < L_2(\mathbf{A})$. This is exactly the supposition in \eqref{1D} (letting $c_i = \|\mathbf{A}_i\|_2^{-1}$ for all $i$) and the result follows from the subsequent arguments for the case $k=1$ in Sec.~\ref{DUT}.
\end{proof}

%The above arguments can be easily modified to prove the following variation of Lem.~\ref{MainLemma}, key to proving Thm.~\ref{DeterministicUniquenessTheorem2}. 

%\begin{lemma}\label{MainLemma2}
%Fix integers $n$ and $k < m \leq \bar m$ and suppose $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ and $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ both satisfiy \eqref{SparkCondition}. There exists a constant $C_4 > 0$ for which for all $\varepsilon < L_2(\mathbf{A}) / C_4$ the following holds. If for some $E \subseteq2^{[m]}$ satisfying the singleton intersection property there exists a size-preserving map $\pi: E \mapsto 2^{[\bar m]}$ satisfying:
%\begin{align}\label{GapUpperBound}
%d(\text{\rm Span}\{\mathbf{A}_{S}\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \leq \varepsilon, \ \ \   \text{for all $S \in E$},
%\end{align}
%
%then for some $J \in {[\bar m] \choose m}$, permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$, we have:
%\begin{align}\label{MainLemmaBPD}
%\|\mathbf{A}_j - (\mathbf{BPD})_j\|_2 \leq C_4 \varepsilon, \ \ \  \text{for } j \in [m].
%\end{align}
%The constant $C_4$ is given by:
%\begin{align}\label{Cdefm}
%C_4 := \frac{2^{|E|}}{R} \max_{j \in [m]} \|\mathbf{A}_j\|_2
%\end{align}
%
%with $R$ being the lesser of $\min_{\substack{F \subseteq E}} r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F})$ and $\min_{\substack{F \subseteq {[\bar m] \choose k}}} r( \{ \text{\rm Span}\{\mathbf{B}_{S}\} \}_{S \in F})$.
%\end{lemma}

%\begin{proof}
%The proof is very similar to the case $\bar m = m$, the difference being that now we may not invoke Lem.~\ref{NonEmptyLemma} (which requires $\bar m = m$) to infer from \eqref{fact2} that $| \cap_{S \in F(i)} \pi(S) | = 1$ for all stars $F(i)$. We circumvent this issue by instead assuming the spark condition on $\mathbf{B}$, which allows us to swap the roles of $\mathbf{A}$ and $\mathbf{B}$ in the arguments leading to the derivation of \eqref{fact2} to prove the opposite inequality for every star $F(i)$. Of course, this requires that the constant $C_4$  depend on $\mathbf{B}$. By these arguments we establish that $| \cap_{S \in F(i)} \pi(S) | = 1$ for all stars $F(i)$, yielding an injective map $\bar \pi: [m] \to [\bar m]$. The result then follows from the proof of the case $k=1$. 
%\end{proof}

\section{Proofs of Thm.~\ref{robustPolythm} and Cor.~\ref{ProbabilisticCor}}\label{AppendixB} %Cors.~\ref{DeterministicUniquenessCorollary} \& \ref{ProbabilisticCor}}\label{AppendixB}

%\begin{proof}[Proof of Cor.~\ref{DeterministicUniquenessCorollary}]
%We need only demonstrate how to produce $N$ vectors $\mathbf{a}_i$ such that for some $E \subseteq {[m] \choose k}$ satisfying the singleton intersection property there are \mbox{$(k-1){m \choose k}+1$} vectors supported on each $S \in E$ in general linear position. Let $\gamma_1, \ldots, \gamma_N$ be any distinct numbers. Then the columns of the $k \times N$ matrix $V = (\gamma^i_j)^{k,N}_{i,j=1}$ are in general linear position (since the $\gamma_j$ are distinct, any $k \times k$ ``Vandermonde" sub-determinant is nonzero). Next, fix some $E \subseteq {[m] \choose k}$ satisfying the singleton intersection property. Finally, form the $k$-sparse vectors $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ with supports $S \in E$ (partitioning the $\mathbf{x}_i$ evenly among these supports so that each contains $(k-1){m \choose k}+1$ vectors $\mathbf{x}_i$) by setting the nonzero values $\mathbf{x}_i$ to be those contained in the $i$th column of $V$.
%\end{proof}
% Actually this is even overkill since 

%We now determine classes of datasets $Y$ having a stable sparse representation that are cut out by a single polynomial equation.

\begin{proof}[Proof of Thm.~\ref{robustPolythm}]
% We sketch the argument, leaving the details to the reader. 
Let $M$ be the matrix with columns $\mathbf{A}\mathbf{x}_i$, $i \in [N]$.  Consider the polynomial \cite[Sec.~IV]{Hillar15} in the entries of $\mathbf{A}$ and $\mathbf{x}_i$:
\begin{align*}
g(\mathbf{A}, \{\mathbf{x}_i\}_{i=1}^N) = \prod_{S \in {[n] \choose k}} \sum_{S' \in {[N] \choose k}} (\det M_{S',S})^2,
\end{align*}
with notation as in Sec.~\ref{Results}.  
It can be checked that when $g$ is nonzero for a substitution of real numbers for the indeterminates, all of the genericity requirements on $\mathbf{A}$ and $\mathbf{x}_i$ in our proofs of stability in Thm.~\ref{DeterministicUniquenessTheorem} are satisfied (in particular, the spark condition on $\mathbf{A}$). % The statement of the theorem now follows directly.
\end{proof}

\begin{proof}[Proof of Cor.~\ref{ProbabilisticCor}]
First, note that if a set of measure spaces $\{(X_{\ell}, \Sigma_{\ell}, \nu_{\ell})\}_{\ell=1}^p$ is such that $\nu_{\ell}$ is absolutely continuous with respect to $\mu$ for all $\ell = 1, \ldots, p$, where $\mu$ is the standard Borel measure on $\mathbb{R}$, then the product measure $\prod_{\ell=1}^p \nu_{\ell}$ is absolutely continuous with respect to the standard Borel product measure on $\mathbb{R}^p$ (e.g.,  \cite{folland2013real}). By Thm.~\ref{robustPolythm}, there is a polynomial that is nonzero whenever $Y$ has a stable $k$-sparse representation in $\mathbb R^m$; in particular, this property (stability) holds with probability one.
\end{proof}

%\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessTheorem2}]
%The proof is the same as that of the case $\bar m = m$, only now we establish a map $\pi: E \to {[\bar m] \choose k}$ by pigeonholing $(k-1){\bar m \choose k} + 1$ vectors with respect to holes $[\bar m]$ and eventually applying Lem.~\ref{MainLemma2} instead of Lem.~\ref{MainLemma}. 

%In this case the constant $C_3$ is given by:
%\begin{align}\label{Cdefm}
%C_3 := \frac{2^{|E|} \max_{j \in [m]} \|\mathbf{A}_j\|_2}{ R \min_{S \in E} L_k(\mathbf{AX}_{I(S)}) },
%\end{align}
%
%with $R$ being the lesser of $\min_{\substack{F \subseteq E}} r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F})$ and $\min_{\substack{F \subseteq {[\bar m] \choose k}}} r( \{ \text{\rm Span}\{\mathbf{B}_{S}\} \}_{S \in F})$.
%\end{proof}

\end{document}