\documentclass[9pt,twocolumn]{pnas-new}
% Use the lineno option to display guide line numbers if required.
% Note that the use of elements such as single-column equations may affect the guide line number alignment. 

% TODO %
% *** Put ell1 norm into the denominator of L_k *** ?
% Give example of when we might expect k-uniformity, i.e. x_i in glp on k-dimensional subspaces. Orbit under group action of a k-dimensional subspace?

%%% Subspace Arrangement..Semi-lattices..? %%%
% Describe the connection between dictionary learning and union of subspace learning. Dictionaries are the top elements of the subspace intersection semi-lattice over reverse-inclusion. We identify subspaces generated by the elements of $A$ by showing that every $B$ must include these subspaces in its intersection semi-lattice. The dictionary elements are then inside these subspaces, so the lower the dimensionality of these subspaces the better.
% Shoutout to research on subspace arrangements? Not just "union of subspaces" model?
% Write Main Lemma or Lil lemma as a map between two intersection semi-lattices with a constraint (on the dimension of subspace? or on size of edge?)?
%See pg. 257 of GPCA textbook that alludes to the connection between DL and deep learning -- could se way we take a step toward identifiability theory in deep learning? lol



% IMPORTED PACKAGES
\usepackage{amsmath, amssymb, amsthm} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}


\renewcommand{\eqref}[1]{\textnormal{[\ref{#1}]}}


%\usepackage[pdftex]{graphicx}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}
\usepackage{bm}
%\usepackage{cite}

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} = Template for a one-column mathematics article
% {pnasinvited} = Template for a PNAS invited submission

\title{On the uniqueness and stability of dictionaries for sparse representation of noisy signals}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,b, 1]{Charles J. Garfinkle}
\author[a,1]{Christopher J. Hillar} 

\affil[a]{Redwood Center for Theoretical Neuroscience, Berkeley, CA, USA}
\affil[b]{Helen Wills Neuroscience Institute, UC Berkeley}

% Please give the surname of the lead author for the running footer
\leadauthor{Garfinkle} 

\significancestatement{Many natural signals (visual scenery, speech, EEG, etc.) lacking domain-specific formal models can nonetheless be usefully characterized as linear combinations of few elementary waveforms drawn from a large ``dictionary". We give general conditions guaranteeing when such dictionaries are uniquely and stably determined by data. The result justifies the use of this model as a constraint for blind source separation and helps explain the observed universality of emergent representations in sparse models of certain natural phenomena.
}

% \authordeclaration{The authors declare no conflict of interest.}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: chaz@berkeley.edu, chillar@msri.org}

\keywords{Sparse coding, dictionary learning, matrix factorization, compressed sensing, inverse problems, blind source separation} 

\begin{abstract}
Dictionary learning for sparse linear coding has exposed characteristic properties of natural signals.
However, a universal theorem guaranteeing the consistency of estimation in this model is lacking.
Here, we prove that for all diverse enough datasets generated from the sparse coding model, latent dictionaries and codes are uniquely and stably determined up to measurement error.  Applications are given to data analysis, engineering, and neuroscience. 
\end{abstract}

% \dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

% Optional adjustment to line up main text (after abstract) of first page with line numbers, when using both lineno and twocolumn options.
% You should only change this length when you've finalised the article contents.
\verticaladjustment{-2pt}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.
\dropcap{B}lind source separation is a classical problem in signal processing \cite{sato1975method}.
In one common modern formulation, each of $N$ observed $n$-dimensional signals is a (noisy) linear combination of at most $k$ elementary waveforms drawn from some unknown ``dictionary'' of size $m$, typically with $k < m \ll N$ (see \cite{Zhang15} for a comprehensive review of this and related models).
Approximating solutions to this sparsity-constrained inverse problem have provided insight into the structure of many signal classes lacking domain-specific formal models (e.g., in vision \cite{wang2015sparse}).  In particular, it has been shown that response properties of simple-cell neurons in mammalian visual cortex emerge from optimizing a dictionary to represent small patches of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent}, a major advance in computational neuroscience. A curious aspect of this finding is that the latent waveforms (e.g. `Gabor' wavelets) estimated from data appear to be canonical \cite{donoho2001can};
i.e., they are found in learned dictionaries independent of algorithm or natural image training set.

Motivated by these discoveries and earlier work in the theory of neural communication \cite{Isely10}, we address when dictionaries and the sparse representations they induce are uniquely determined by data.  Answers to this question also have other real-world implications.  For example, a sparse coding analysis of local painting style can be used for forgery detection \cite{hughes2010, Olshausen10}, but only if all dictionaries consistent with training data do not differ appreciably in their ability to sparsely encode new samples. 
Fortunately, algorithms with proven recovery of generating dictionaries under certain 
conditions have recently been proposed (see \cite[Sec.~I-E]{Sun16} for a summary of the state-of-the-art). Few theorems, however, can be cited to explain this uniqueness phenomenon more broadly.

Here, we prove very generally that uniqueness and stability in sparse linear coding is an expected property of the model. 
More specifically, dictionaries that preserve sparse codes (e.g., satisfy a ``spark condition'') are identifiable from as few as \mbox{$N = m(k-1){m \choose k} + m$} noisy sparse linear combinations of their columns up to an error that is linear in the noise (Thm.~\ref{DeterministicUniquenessTheorem}). In fact, provided $n \geq \min(2k,m)$, in almost all cases the dictionary learning problem is well-posed (as per Hadamard \cite{Hadamard1902}) given enough data (Cor.~\ref{ProbabilisticCor}). Moreover, these explicit, algorithm-independent guarantees hold without assuming the recovered matrix satisfies a spark condition, and even when the number $m$ of dictionary elements is unknown.

More formally, let $\mathbf{A} \in \mathbb R^{n \times m}$ be a matrix with columns $\mathbf{A}_j$ ($j = 1,\ldots,m$) and let dataset $Z$ consist of measurements:
\begin{align}\label{LinearModel}
\mathbf{z}_i = \mathbf{A}\mathbf{x}_i + \mathbf{n}_i,\ \ \  \text{$i=1,\ldots,N$},
\end{align}
for $k$-\emph{sparse} $\mathbf{x}_i \in \mathbb{R}^m$ having at most $k$ nonzero entries and \emph{noise} $\mathbf{n}_i \in \mathbb{R}^n$, with bounded norm $\| \mathbf{n}_i \|_2 \leq  \eta$ representing our combined worst-case uncertainty in  measuring $\mathbf{A}\mathbf{x}_i$.
The first mathematical problem addressed here is the following.

\begin{problem}[Sparse linear coding]\label{InverseProblem}
Find a matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$ such that $\|\mathbf{z}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \eta$ for all $i$.
\end{problem}

Note that any particular solution $(\mathbf{B}, \mathbf{\bar x}_1 \ldots, \mathbf{\bar x}_N)$ to this problem gives rise to an orbit of equivalent solutions $(\mathbf{BPD}, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_1, \ldots, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_N)$, where $\mathbf{P}$ is any permutation matrix and $\mathbf{D}$ any invertible diagonal. Previous theoretical work addressing the noiseless case $\eta =0$ (e.g., \cite{li2004analysis, Georgiev05, Aharon06, Hillar15}) has shown that a solution to Prob.~\ref{InverseProblem} (when it exists) is indeed unique up to this inherent ambiguity provided the $\mathbf{x}_i$ are sufficiently diverse and the generating matrix $\mathbf{A}$ satisfies the \textit{spark condition} from compressed sensing:
\begin{align}\label{SparkCondition}
\mathbf{A}\mathbf{x}_1 = \mathbf{A}\mathbf{x}_2 \implies \mathbf{x}_1 = \mathbf{x}_2, \ \ \ \text{for all $k$-sparse } \mathbf{x}_1, \mathbf{x}_2,
\end{align}
%
which would be, in any case, necessary for uniqueness of the $\mathbf{x}_i$. Our concern here is solution stability with respect to noise.

\begin{definition}\label{maindef}
Fix $Y = \{ \mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$. We say $Y$ has a \textbf{$k$-sparse representation in $\mathbb{R}^m$} if there exists a matrix $\mathbf{A}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ such that $\mathbf{y}_i = \mathbf{A}\mathbf{x}_i$ for all $i$. 
This representation is \textbf{stable} if for every $\delta_1, \delta_2 \geq 0$, there exists $\varepsilon(\delta_1, \delta_2) \geq 0$ (with $\varepsilon > 0$ when  $\delta_1, \delta_2 > 0$) such that if $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$ satisfy:
\begin{align*}
\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon,\ \ \   \text{for all $i$},
\end{align*}
then there is some permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ such that for all $i, j$:
\begin{align}\label{def1}
\|\mathbf{A}_j - \mathbf{BPD}_j\|_2 \leq \delta_1 \ \ \text{and} \ \ \|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i\|_1 \leq \delta_2.
\end{align}
\end{definition}

To see how Def. \ref{maindef} directly relates to Prob. \ref{InverseProblem}, suppose that $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ and fix $\delta_1, \delta_2$ to be the desired recovery accuracy in \eqref{def1}. Consider now any dataset $Z$ generated as in \eqref{LinearModel} with $\eta \leq \frac{1}{2} \varepsilon(\delta_1, \delta_2)$. Then from the triangle inequality, it follows that any matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$ solving Prob.~\ref{InverseProblem} are necessarily within $\delta_1, \delta_2$ of the original dictionary $\mathbf{A}$ and sparse codes $\mathbf{x}_i$. 

In the next section, we give precise statements of our main results, which include an explicit form for $\varepsilon(\delta_1, \delta_2)$. We then prove our main theorem (Thm.~\ref{DeterministicUniquenessTheorem}) in Sec.~\ref{DUT} after stating some additional definitions and lemmas required for the proof, including a useful result in combinatorial matrix analysis (Lem.~\ref{MainLemma}). We also provide an argument that extends our guarantees to the following more common optimization formulation of the dictionary learning problem (Cor.~\ref{SLCopt}).

\begin{problem}\label{OptimizationProblem}
Find $\mathbf{B}$ and \mbox{$\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$} that solve:
\begin{align}\label{minsum}
\min \sum_{i = 1}^N \|\mathbf{\bar x}_i\|_0 \ \ 
\text{subject to} \ \ \|\mathbf{z}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \eta, \ \text{for all $i$}.
\end{align}
\end{problem}
Finally, in Sec.~\ref{Discussion}, we explain how our results fit into the literature and present several applications of our findings.

\section{Results}\label{Results}

Before stating our results precisely, we identify criteria on the support sets of the generating codes $\mathbf{x}_i$ that imply stable sparse representations. Letting $\{1, \ldots, m\}$ be denoted $[m]$, its power set $2^{[m]}$, and ${[m] \choose k}$ the set of subsets of $[m]$ of size $k$, we say a hypergraph $\mathcal{H} \subseteq 2^{[m]}$ on vertices $[m]$ is \textit{$k$-uniform} when $\mathcal{H} \subseteq {[m] \choose k}$. The \emph{degree} $\deg(i)$ of a node $i \in [m]$ is the number of sets in $\mathcal{H}$ that contain $i$, and we say $\mathcal{H}$ is \emph{regular} when for some $r$ we have $\deg(i) = r$ for all $i$ (given such an $r$, we say $\mathcal{H}$ is \textit{$r$-regular}). We also write $2\mathcal{H} := \{ S \cup S': S, S' \in \mathcal{H}\}$.

\begin{definition}\label{sip}
Given $\mathcal{H} \subseteq 2^{[m]}$, the \textbf{star} $\sigma(i)$ is the collection of sets in $\mathcal{H}$ containing $i$. We say $\mathcal{H}$ has the \textbf{singleton intersection property} (\textbf{SIP}) when $\cap \sigma(i) = \{i\}$ for all $i \in [m]$.
\end{definition}

Next, we describe a quantitative version of the spark condition. The \emph{lower bound} $L$ of a matrix $\mathbf{M} \in \mathbb R^{n \times m}$ is the largest $\alpha$ such that \mbox{$\|\mathbf{M}\mathbf{x}\|_2 \geq \alpha\|\mathbf{x}\|_2$} for all $\mathbf{x} \in \mathbb{R}^m$ \cite{Grcar10}. By compactness of the sphere, every injective linear map has nonzero lower bound; hence, if $\mathbf{M}$ satisfies \eqref{SparkCondition}, then each submatrix formed from $2k$ of its columns or less has $L > 0$. 

We generalize the  lower bound to a domain-restricted \emph{union of subspaces} model \cite{vidal2005generalized} derived from the hypergraph $\mathcal{H}$. Letting $\mathbf{M}_J$ denote the submatrix formed by the columns of $\mathbf{M}$ indexed by $J$ (with $\mathbf{M}_\emptyset := \mathbf{0}$), we define: 
%\begin{align*} 
%L_\mathcal{H}(\mathbf{M}) := \inf \left\{ \frac{ \|\mathbf{M}(\mathbf{x}_1-\mathbf{x}_2)\|_2 }{ \sqrt{2k} \|\mathbf{x}_1-\mathbf{x}_2\|_2} : \mathbf{x}_1, \mathbf{x}_2 \in \cup_{S \in \mathcal{H}} \bm{\mathcal{M}}_S \right\},
%\end{align*} 
%
%where we write $L_{2k}$ in place of $L_\mathcal{H}$ when $\mathcal{H} = {[m] \choose k}$. Note that if $\mathcal{H}$ covers $[m]$, then $L_2 > L_\mathcal{H}$.\footnote{The reader should beware that $L_2 = L_{[m]}$, whereas $L = L_{\{[m]\}}$. For even $k$, the quantity $1 - \sqrt{k} L_k(\mathbf{M})$ is also known in the compressed sensing literature as the (asymmetric) lower restricted isometry constant \cite{Blanchard2011}.} Clearly, for any $\mathbf{M}$ satisfying \eqref{SparkCondition}, we have $L_{k'}(\mathbf{M}) > 0$ for  $k' \leq 2k$.
%\begin{align*}
%L_\mathcal{H}(\mathbf{M}) := \inf \left\{ \frac{ \|\mathbf{M}_S\mathbf{x}\|_2 }{ \sqrt{k} \|\mathbf{x}\|_2} : S \in \mathcal{H} \right\},
%\end{align*} 
%\begin{align*}
%L_\mathcal{H}(\mathbf{M}) := \max \left\{ \alpha: \|\mathbf{M}_S\mathbf{x}\|_2 \geq \alpha \sqrt{k} \|\mathbf{x}\|_2 : S \in \mathcal{H}, \ \ \mathbf{x} \in \mathbb{R}^m \right\},
%\end{align*} 
\begin{align}\label{Ldef}
L_\mathcal{H}(\mathbf{M}) := \frac{1}{\sqrt{k}} \inf \left\{ \frac{\|\mathbf{M}_S\mathbf{x}\|_2}{ \|\mathbf{x}\|_2} : S \in \mathcal{H}, \ \ \mathbf{x} \in \mathbb{R}^{|S|} \right\},
\end{align} 
%
where we write $L_{k}$ when $\mathcal{H} = {[m] \choose k}$.\footnote{We note that $1 - \sqrt{k} L_k(\mathbf{M})$ is known as the asymmetric lower restricted isometry constant \cite{Blanchard2011}.} Clearly, for any $\mathbf{M}$ satisfying \eqref{SparkCondition}, we have $L_{k'}(\mathbf{M}) > 0$ for  $k' \leq 2k$. Note that if $\mathcal{H}$ covers $[m]$ t(i.e., $\cup \mathcal{H} = [m]$), then $L_2 \geq L_{2\mathcal{H}}$.
 
A vector $\mathbf{x}$ is said to be \emph{supported} in $S \subseteq [m]$ when $\mathbf{x} \in \text{\rm span}\{\mathbf{e}_j: j\in S\}$, where $\mathbf{e}_j$ are the standard basis in $\mathbb R^m$. 
%Denote by $\mathbf{x}^J$ the subvector formed from the entries of $\mathbf{x}$ indexed by $J$. 
A set of $k$-sparse vectors is said to be in \emph{general linear position} when any $k$ of them are linearly independent. The following is a precise statement of our main result.  Only the quantity $C_1$ is undefined until later (in Sec.~\ref{DUT}, Eq.~\eqref{Cdef1}).

\begin{theorem}\label{DeterministicUniquenessTheorem}
Fix an $n \times m$ matrix $\mathbf{A}$ with $L_{2\mathcal{H}}(\mathbf{A}) > 0$ for an $r$-regular $\mathcal{H} \subseteq {[m] \choose k}$ with the SIP. If \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} contains, for each $S \in \mathcal{H}$, more than $(k-1){m \choose k}$ $k$-sparse vectors in general linear position supported in $S$, then there is a constant $C_1 > 0$ with the following holding for all\footnote{The condition $\varepsilon < L_2(\mathbf{A}) /C_1$ is necessary; otherwise,  with \mbox{$\mathbf{A}$ = $\mathbf{I}$} and $\mathbf{a}_i = \mathbf{e}_i$, there is a $\mathbf{B}$ and $1$-sparse $\mathbf{b}_i$ with $|\mathbf{A}\mathbf{a}_i - \mathbf{B}\mathbf{b}_i|_2 \leq \varepsilon$ violating \eqref{Cstable}.} $\varepsilon < L_{2}(\mathbf{A}) / C_1$:

Every $n \times \bar m$ matrix $\mathbf{B}$ for which there exist $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N$ satisfying \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$} for all $i$ has $\bar m \geq m$ and, provided $\bar m < mr/(r-1)$,
\begin{align}\label{Cstable}
\|\mathbf{A}_j- \mathbf{B}_{\bar J} \mathbf{PD}_j\|_2 \leq C_1 \varepsilon, \ \ \text{for all } j \in J,
\end{align}
%
for some nonempty $J \subseteq [m]$ and $\bar J \subseteq [m]$ of size $\bar m - r(\bar m - m)$, permutation matrix $\mathbf{P}$, and and invertible diagonal matrix $\mathbf{D}$. 

Moreover, if in addition $\varepsilon < L_{2k}(\mathbf{A}) / C_1$ (provided $L_{2k}(\mathbf{A}) > 0$), then $L_{2k}(\mathbf{B}\mathbf{PD}) \geq L_{2k}(\mathbf{A}) - C_1 \varepsilon$ and:
\begin{align}\label{b-PDa}
%\|\mathbf{x}^J_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}^{\bar J}_i\|_1 &\leq  \left( \frac{ 1+C_1 \|\mathbf{x}^{J}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right) \varepsilon \ \  \text{for $i \in [N]$}.
\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i\|_1 &\leq  \left( \frac{ 1+C_1 \|\mathbf{x}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right) \varepsilon, \ \  \text{for $i \in [N]$},
\end{align}
%
where $\mathbf{x}_i$ and $\mathbf{\bar x}_i$ here represent subvectors formed from restricting to entries indexed by $J$ and $\bar J$, respectively. 
\end{theorem}

%We delay defining the explicit constant $C_1$ until Section \ref{DUT} (\eqref{Cdef1}).
%To be clear, the implication of Thm.~\ref{DeterministicUniquenessTheorem} is that $Y = \{\mathbf{Ax}_1, \ldots, \mathbf{Ax}_N\}$ has a stable $k$-sparse representation in $\mathbb{R}^m$, with \eqref{def1} guaranteed provided $\varepsilon$ in Def.~\ref{maindef} does not exceed: 
To be clear, Thm.~\ref{DeterministicUniquenessTheorem} says that the smaller the difference $\bar m - m$, the more columns and coefficients of the original dictionary $\mathbf{A}$ and codes $\mathbf{x}_i$ are contained (up to noise) in the appropriately scaled dictionary $\mathbf{B}$ and codes $\mathbf{\bar x}_i$. When $\bar m = m$, the theorem implies that  $Y = \{\mathbf{Ax}_1, \ldots, \mathbf{Ax}_N\}$ has a stable $k$-sparse representation in $\mathbb{R}^m$, with inequality \eqref{def1} guaranteed provided $\varepsilon$ in Def.~\ref{maindef} does not exceed: 
\begin{align}\label{epsdel}
\varepsilon(\delta_1, \delta_2) := \min \left\{ \frac{\delta_1}{ C_1 }, \frac{ \delta_2 L_{2k}(\mathbf{A})}{ 1 + C_1 \left( \max_{i \in [N]} \|\mathbf{x}_i\|_1  + \delta_2 \right) } \right\}.
\end{align}

A strong practical implication of this result is the following: there is an effective procedure by which one can affirm if one's proposed solution $(\mathbf{B}, \mathbf{\bar x}_1, \ldots, \mathbf{x}_N)$ to Prob.~\ref{InverseProblem} is indeed unique (up to noise and inherent ambiguities). One simply need check that $\mathbf{B}$ and the $\mathbf{\bar x}_i$ satisfy the assumptions of Thm.~\ref{DeterministicUniquenessTheorem} on $\mathbf{A}$ and the $\mathbf{x}_i$, respectively.

%In fact, a more general result (stated clearly in the next section) can be gleaned from our method of proving Thm.~\ref{DeterministicUniquenessTheorem}. Briefly, in cases where $\mathbf{B}$ has $\bar m \neq m$ columns, or $\mathcal{H}$ is not regular or only partially satisfying the SIP, a relation between $\bar m$ and the degree sequence of nodes in $\mathcal{H}$ gives indices $J \subseteq [m]$ defining a submatrix $\mathbf{A}_J$ and subvectors $\mathbf{x}_i^J$ that are recoverable in the sense of \eqref{Cstable} and \eqref{b-PDa}. For example, if $\mathcal{H}$ is $\ell$-regular with the SIP but $m \leq \bar m < m\ell/(\ell - 1)$ then we have nonzero $|J| = \bar m - \ell(\bar m - m)$. The implication here is that the smaller the difference $\bar m - m$, the more columns and code entries of the original $n \times m$ dictionary $\mathbf{A}$ and codes $\mathbf{x}_i$ contained (up to noise) in the appropriately scaled $n \times \bar m$ dictionary $\mathbf{B}$ and codes $\mathbf{\bar x}_i$. When $\bar m = m$, we recover Thm.~\ref{DeterministicUniquenessTheorem}.

%In fact, even if $\mathcal{H}$ is not regular or only partially satisfies the SIP, a relation between $\bar m$ and the degree sequence of nodes in $\mathcal{H}$ may give the indices $J \subseteq [m]$. For sake of brevity, we delay to the next section a clear statement of this more general result.

Regarding the assumptions of Thm.~\ref{DeterministicUniquenessTheorem}, it so happens that sparse codes $\mathbf{x}_i$ in general linear position are straightforward to produce with a ``Vandermonde'' matrix construction \cite{Hillar15}, leading to the following.

\begin{corollary}\label{DeterministicUniquenessCorollary}
Given a regular hypergraph $\mathcal{H} \subseteq {[m] \choose k}$ with the SIP, there are $N =  |\mathcal{H}| \left[ (k-1){m \choose k} + 1  \right]$ vectors \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} such that every matrix $\mathbf{A}$ with $L_{2\mathcal{H}}(\mathbf{A}) > 0$ generates a dataset $Y = \{\mathbf{A}\mathbf{x}_1, \ldots, \mathbf{A}\mathbf{x}_N\}$ with a stable $k$-sparse representation in $\mathbb{R}^m$.
\end{corollary}

One can also easily verify that for every $k < m$, there is a regular $k$-uniform hypergraph that satisfies the SIP; for instance, take $\mathcal{H}$ to be the consecutive intervals of length $k$ in some cyclic order on $[m]$, for which Cor.~\ref{DeterministicUniquenessCorollary} implies the lower bound for sample size $N$ from the introduction. In many cases, however, the SIP is achievable with far fewer supports; for example, when $k = \sqrt{m}$, take $\mathcal{H}$ to be the $2k$ rows and columns formed by arranging $[m]$ in a square grid. 

%As mentioned above, there exist $k$-uniform regular hypergraphs $\mathcal{H}$ with the SIP having cardinality $|\mathcal{H}| = m$, 

We furthermore note that unlike the requirements of previous works \cite{li2004analysis, Georgiev05, Aharon06, Hillar15}, matrix $\mathbf{A}$ need not satisfy the spark condition to be recoverable from data. As a simple example for $k=2$, let $\mathbf{A} = [ \mathbf{e}_1, \ldots, \mathbf{e}_5, \mathbf{v}]$ where $\mathbf{v} = \mathbf{e}_1 + \mathbf{e}_3 + \mathbf{e}_5$, and take $\mathcal{H}$ to be the set of all consecutive pairs of $[m]$ arranged in cyclic order. Then, $\mathbf{A}$ satisfies the assumptions of Thm.~\ref{DeterministicUniquenessTheorem} but not the spark condition. %(since $\{ \mathbf{A}_1, \mathbf{A}_3, \mathbf{A}_5, \mathbf{A}_6\}$ is not a linearly independent set).

There are other less direct consequences of Thm.~\ref{DeterministicUniquenessTheorem}, such as the following implication for optimization Prob.~\ref{SLCopt}.

\begin{corollary}\label{SLCopt}
If the assumptions of Thm.\ref{DeterministicUniquenessTheorem} hold, only now with more than $(k-1)\left[ {\bar m \choose k} + |\mathcal{H}|k{\bar m \choose k-1}\right]$ vectors $\mathbf{x}_i$ supported in each $S \in \mathcal{H}$, then all solutions to Prob.~\ref{OptimizationProblem} necessarily satisfy implications \eqref{Cstable} and \eqref{b-PDa} of Thm.~\ref{DeterministicUniquenessTheorem}.
\end{corollary}

Another extension of Thm.~\ref{DeterministicUniquenessTheorem} arises from the following analytic characterization of the spark condition.  Let $\mathbf{A}$  be the $n \times m$ matrix of $nm$ indeterminates $A_{ij}$. When real numbers are substituted for $A_{ij}$, the resulting matrix satisfies \eqref{SparkCondition} if and only if the following polynomial is nonzero:
\begin{align*}
f(\mathbf{A}) := \prod_{S \in {[m] \choose k}} \sum_{S' \in {[n] \choose k}} (\det \mathbf{A}_{S',S})^2,
\end{align*}
%
where for any $S' \in {[n] \choose k}$ and $S \in {[m] \choose k}$, the symbol $\mathbf{A}_{S',S}$ denotes the submatrix of entries $A_{ij}$ with $(i,j) \in S' \times S$.   We note that the large number of terms in this product is likely necessary due to the NP-hardness of deciding whether a given matrix $\mathbf{A}$ satisfies the spark condition \cite{tillmann2014computational}.

Since $f$ is analytic, having a single substitution of a real matrix $\mathbf{A}$ with $f(\mathbf{A}) \neq 0$ necessarily implies that the zeroes of $f$, in fact, form a set of measure zero. Fortunately, such a matrix $\mathbf{A}$ is easily constructed by adding rows of zeroes to any $\min(2k,m) \times m$ Vandermonde matrix $[\gamma_i^j]_{i,j=1}^{k,m}$ with distinct $\gamma_i$ (so that each term in the product above for $f$ is nonzero). Hence, almost every real $n \times m$ matrix with $n \geq \min(2k,m)$ satisfies \eqref{SparkCondition}.

A similar phenomenon applies to datasets of vectors with a stable sparse representation. As in \cite[Sec.~IV]{Hillar15}, consider the ``symbolic'' dataset $Y = \{\mathbf{A}\mathbf{x}_1,\ldots,\mathbf{A} \mathbf{x}_N\}$ generated by indeterminate $\mathbf{A}$ and indeterminate $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N$.  

\begin{theorem}\label{robustPolythm}
There is a polynomial $g$ in the entries of $\mathbf{A}$ and $\mathbf{x}_i$ with the following property:  if $g$ evaluates to a nonzero number and more than \mbox{$(k-1){m \choose k}$} of the resulting $\mathbf{x}_i$ are supported in each $S \in \mathcal{H}$ for some regular $\mathcal{H} \subseteq {[m] \choose k}$ with the SIP, then $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ (Def.~\ref{maindef}). In particular, all -- except for a Borel set of measure zero -- substitutions impart to $Y$ this property.
\end{theorem}

\begin{corollary}\label{ProbabilisticCor}
Fix $k < m$, $n \geq \min(2k, m)$, and let the entries of $\mathbf{A} \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ be drawn independently from probability measures absolutely continuous with respect to the standard Borel measure $\mu$. If more than $(k-1){m \choose k}$ of the vectors $\mathbf{x}_i$ are supported in each $S \in \mathcal{H}$ for a regular $\mathcal{H} \subseteq {[m] \choose k}$ with the SIP, then $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ with probability one.
\end{corollary}

Thus, choosing the dictionary and sparse codes ``randomly'' almost certainly generates data with a stable sparse representation.

We remark that these results have a strong application to theoretical neuroscience, mathematically justifying one of the few hypothesized theories of compressive communication between sparsely active neural populations \cite{Isely10, ganguli2012compressed}. 

\begin{proposition}
Random noisy (linear) compression of sparse neural activities is decodable by any neurally-plausible unsupervised sparse (linear) feature learning algorithm that solves Prob.~\ref{DeterministicUniquenessTheorem} or \ref{SLCopt} (e.g. \cite{rehnsommer2007, rozell2007neurally}).
\end{proposition}
%\cite{rehnsommer2007, rozell2007neurally, ganguli2012compressed, hu2014hebbian}

\section{Proofs of Theorem~\ref{DeterministicUniquenessTheorem} and Corollary~\ref{SLCopt}}\label{DUT}

%%%%%%%%%%%%%%
% need to cut maybe: 
%As mentioned in the previous section, Thm.~\ref{DeterministicUniquenessTheorem} is a particular case of a more general result that forgoes the assumption that $\mathcal{H} \subseteq {[m] \choose k}$ is regular and satisfies the SIP. If instead we require only that the stars $\cap h(i)$ intersect at singletons for all $i \leq q$ (assuming that the nodes of $\mathcal{H}$ are labeled in some order of non-increasing degree), we have that $\bar m \geq k|\mathcal{H}| / \deg(1)$ and, provided $\bar m < k|\mathcal{H}| / (\deg(1) - 1)$, the nonempty submatrix $J$ is of size equal to the largest number $p$ satisfying:
%\begin{align}\label{pcond}
%\sum_{i=\ell}^{m} \deg(i) > (\bar m + 1 - \ell) (\deg(\ell) - 1) \ \ \text{for all } \ell \leq p \leq q.
%\end{align}
%Specifically, $J$ contains all nodes of degree exceeding $\deg(p)$ and some subset of those with degreee equal to $\deg(p)$. For the benefit of the reader, we do not prove explicitly this more general result below; it can be discerned from how exactly Lemma \ref{NonEmptyLemma} is incorporated into the proof of Lem.~\ref{MainLemma}.
%%%%%%%%%%%%%%%%


%As mentioned in the previous section, Thm.~\ref{DeterministicUniquenessTheorem} is a particular case of a more general result, which requires a looser set of constraints on the hypergraph $\mathcal{H}$ and which applies to $n \times \bar m$ matrices $\mathbf{B}$ (and $\bar m$-dimensional codes $\mathbf{\bar x}_i$) with $\bar m \neq m$:

%Fix $\bar m$ and suppose the assumptions of Thm.~\ref{DeterministicUniquenessTheorem} hold, only now with the constraints on $\mathcal{H} \subseteq {[m] \choose k}$ being just that $|\cap H(i)| = 1$ for all $i \leq q$ (assuming w.l.o.g. that the nodes of $\mathcal{H}$ are labeled in some order of non-increasing degree), and with more than $(k-1){\bar m \choose k}$ vectors $\mathbf{x}_i$ supported in g.l.p. on each $S \in \mathcal{H}$. Then we must have $\bar m \geq k|\mathcal{H}| / \deg(1)$ and, provided $\bar m < k|\mathcal{H}| / (\deg(1) - 1)$, the guarantee \eqref{Cstable} holds for a submatrix $\mathbf{A}_J$, where $J \subseteq[m]$ is nonempty and of a size equal to the largest number $p$ satisfying:
%\begin{align}\label{pcond}
%\sum_{i=\ell}^{m} \deg(i) > (\bar m + 1 - \ell) (\deg(\ell) - 1) \ \ \text{for all } \ell \leq p \leq q.
%\end{align}
%Specifically, $J$ consists of the union of the set of all nodes of degree exceeding $\deg(p)$ and some subset of those nodes with degrees equal to $\deg(p)$. 

%For the benefit of the reader, we prove below the case where we forgo only the constraint that $\bar m = m$. This yields the implication $\bar m \geq m$ and \eqref{pcond} reduces to $|J| = \bar m - r(\bar m - m)$. The extension to the general result above can be seen by examining how exactly Lemma \ref{NonEmptyLemma} is incorporated into the overall proof. 

% ======== b - PDa =========

We now begin our proof of Thm.~\ref{DeterministicUniquenessTheorem} by showing how dictionary recovery \eqref{Cstable} already implies sparse code recovery \eqref{b-PDa} when \mbox{$\varepsilon < L_{2k}(\mathbf{A}) / C_1$} (provided $L_{2k}(\mathbf{A}) > 0$), temporarily assuming (without loss of generality) that $\bar m = m$. First, note that since $\|\mathbf{A}\mathbf{x}\|_2 \leq \max_j\|\mathbf{A}_j\|_2\|\mathbf{x}\|_1$ and $\|\mathbf{x}\|_1 \leq \sqrt{k} \|\mathbf{x}\|_2$ for $k$-sparse $\mathbf{x}$, by \eqref{Ldef} we have the following frequently applied inequality:
\begin{align}\label{delrho}
L_{2\mathcal{H}}(\mathbf{A}) %\leq \frac{\|\mathbf{A}\mathbf{x}\|_2}{\sqrt{2k} \|\mathbf{x}\|_2} 
\leq  \max_j\|\mathbf{A}_j\|_2.
\end{align}
For $2k$-sparse $\mathbf{x} \in \mathbb{R}^m$, the triangle inequality gives \mbox{$\|(\mathbf{A}-\mathbf{BPD})\mathbf{x}\|_2  \leq C_1\varepsilon \|\mathbf{x}\|_1 \leq C_1 \varepsilon \sqrt{2k}  \|\mathbf{x}\|_2$}. Thus:
\begin{align*}
\|\mathbf{BPD}\mathbf{x}\|_2 
&\geq | \|\mathbf{A}\mathbf{x}\|_2 - \|(\mathbf{A}-\mathbf{BPD})\mathbf{x}\|_2 | \\
&\geq \sqrt{2k} (L_{2k}(\mathbf{A}) -  C_1\varepsilon) \|\mathbf{x}\|_2,
\end{align*}
%
since $\varepsilon < L_{2k}(\mathbf{A}) / C_1$. Hence, $L_{2k}(\mathbf{BPD}) \geq L_{2k}(\mathbf{A}) - C_1\varepsilon  > 0$, and \eqref{b-PDa} then follows from:
\begin{align*}
\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i \|_1
&\leq \frac{\|\mathbf{BPD}(\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i)\|_2}{L_{2k}(\mathbf{BPD})} \\
&\leq \frac{\|(\mathbf{BPD} - \mathbf{A})\mathbf{x}_i\|_2 + \|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2}{L_{2k}(\mathbf{BPD})} \\
&\leq \frac{\varepsilon (1 + C_1 \|\mathbf{x}_i\|_1)}{L_{2k}(\mathbf{BPD})}.
\end{align*}

The heart of the matter is therefore \eqref{Cstable}, which we now finally establish, first in the important special case $k = 1$

\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessCorollary} for $k=1$]
Since the only 1-uniform hypergraph with the SIP is $[m]$, we have $\mathbf{x}_i = c_i \mathbf{e}_i$ for $c_i \in \mathbb{R} \setminus \{\mathbf{0}\}$, $i \in [m]$. In this case, we may take any $C_1 \geq 1/ \min_{\ell \in [m]} |c_{\ell}|$. 

Fix $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} and suppose that for some $\mathbf{B}$ and $1$-sparse $\mathbf{\bar x}_i \in \mathbb{R}^{\bar m}$ we have  $\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon < L_2(\mathbf{A}) / C_1$ for all $i$. Then, there exist $\bar{c}_1, \ldots, \bar{c}_m \in \mathbb{R}$ and a map $\pi: [m] \to [\bar m]$ such that:
\begin{align}\label{1D}
\|c_j\mathbf{A}_j - \bar{c}_j\mathbf{B}_{\pi(j)}\|_2 \leq \varepsilon,\ \ \text{for $j \in [m]$}.
\end{align} 
Note that if $\bar{c}_j = 0$, then $\|c_j\mathbf{A}_j \|_2 \leq \varepsilon$ implies from \eqref{delrho} that $|c_j| < \min_{\ell \in [m]} | c_\ell |$, a contradiction.  Thus, $\bar{c}_j \neq 0$.

We  now show that $\pi$ is injective (in particular, a permutation if $\bar m = m$). Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell$. Then, $\|c_{j}\mathbf{A}_{j} - \bar{c}_{j}\mathbf{B}_{\ell}\|_2 \leq \varepsilon$ and $\|c_{i}\mathbf{A}_{i} - \bar{c}_{i} \mathbf{B}_{\ell}\|_2  \leq \varepsilon$. Scaling and summing these inequalities by $|\bar{c}_{i}|$ and $|\bar{c}_{j}|$, respectively, and applying the triangle inequality, we obtain:
\begin{align*}
(|\bar{c}_{i}| + |\bar{c}_{j}|) \varepsilon
&\geq\|\mathbf{A}(\bar{c}_{i}c_{j} \mathbf{e}_{j} - \bar{c}_{j}c_{i}\mathbf{e}_{i})\|_2 \nonumber \\ 
&\geq  \left( |\bar{c}_{i}| + |\bar{c}_{j}| \right) L_2(\mathbf{A}) \min_{\ell \in [m]} |c_\ell |,
\end{align*}
which contradicts the bound $\varepsilon < L_2(\mathbf{A})/C_1$. Hence, the map $\pi$ is injective and therefore $\bar m \geq m$. Setting $\bar J = \pi([m])$ and letting $P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}\right)$ and $D = \text{diag}(\frac{\bar{c}_1}{c_1},\ldots,\frac{\bar{c}_m}{c_m})$, we see that \eqref{1D} becomes, for all $j \in [m]$:
\begin{align*}
\|\mathbf{A}_j - \mathbf{B}_{\bar J}\mathbf{PD}_j\|_2 
= \|\mathbf{A}_j - \frac{\bar{c}_j}{c_j}\mathbf{B}_{\pi(j)}\|_2 
\leq \frac{\varepsilon}{|c_j|} 
\leq C_1\varepsilon.
\end{align*}
\end{proof}

We require a few additional tools to extend the proof to the general case $k < m$. These include a generalized notion of distance (Def.~\ref{dDef}) and angle (Def.~\ref{FriedrichsDefinition}) between subspaces as well as a stability result in combinatorial matrix analysis (Lem.~\ref{MainLemma}).

\begin{definition}\label{dDef}
For $\mathbf{u} \in \mathbb R^m$ and vector spaces $U,V \subseteq \mathbb{R}^m$, let $\text{\rm dist}(\mathbf{u}, V) := \inf \{\| \mathbf{u}-\mathbf{v} \|_2: \mathbf{v} \in V\}$ and define:
\begin{align}
d(U,V) := \sup_{\mathbf{u} \in U, \ \|\mathbf{u}\|_2 \leq 1} \text{\rm dist}(\mathbf{u},V).
\end{align}
\end{definition}

We note the following facts about $d$. For subspaces $U \subseteq U', V \subseteq \mathbb{R}^m$, we have $d(U,V) \leq d(U',V)$ and \cite[Cor.~2.6]{Kato2013}:
\begin{align}\label{dimLem}
d(U,V) < 1 \implies \dim(U) \leq \dim(V).
\end{align}
Also, from \cite[Lem.~3.2]{Morris10}, we have:
\begin{align}\label{eqdim}
\dim(U) = \dim(V) \implies d(U,V) = d(V,U).
\end{align}

Our result in combinatorial matrix analysis is the following. Let $\bm{\mathcal{M}}$ denote the column-span of a matrix $\mathbf{M}$.

\begin{lemma}\label{MainLemma}
Suppose $\mathbf{A} \in \mathbb{R}^{n \times m}$ has $L_{2\mathcal{H}}(\mathbf{A}) > 0$ for some $r$-regular $\mathcal{H} \subseteq {[m] \choose k}$ with the SIP. There exists $C_2 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_2$:

If for some  $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ and map $\pi: E \mapsto {[\bar m] \choose k}$ we have:
\begin{align}\label{GapUpperBound}
d(\bm{\mathcal{A}}_S, \bm{\mathcal{B}}_{\pi(S)}) \leq \varepsilon, \ \  \text{for $S \in \mathcal{H}$},
\end{align}
then $\bar m \geq m$, and provided $\bar m < mr /(r-1)$, there is a permutation matrix $\mathbf{P}$ and invertible diagonal $\mathbf{D}$ such that:
\begin{align}\label{MainLemmaBPD}
\|\mathbf{A}_j - \mathbf{B}_{\bar J} \mathbf{PD}_j\|_2 \leq C_2 \varepsilon, \ \  \text{for } j \in J,
\end{align}
for some nonempty $J, \bar J$ of size $\bar m - r(\bar m - m)$.
\end{lemma}

%\begin{lemma}\label{MainLemma}
%Fix $\mathbf{A} \in \mathbb{R}^{n \times m}$ with $L_\mathcal{H}(\mathbf{A}) > 0$ for some regular $\mathcal{H} \subseteq {[m] \choose k}$ with the SIP. There exists a constant $C_2 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_2$:

%If a matrix $\mathbf{B} \in \mathbb{R}^{n \times m}$ and map $\pi: E \mapsto {m \choose k}$ satisfy:
%\begin{align}\label{GapUpperBound}
%d(\text{\rm span}\{\mathbf{A}_{S}\}, \bm{\mathcal{B}}_{\pi(S)}) \leq \varepsilon, \ \ \text{for $S \in \mathcal{H}$},
%\end{align}
%then there exist a permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ such that:
%\begin{align}\label{MainLemmaBPD}
%\|\mathbf{A}_j - \mathbf{BPD}_j\|_2 \leq C_2 \varepsilon, \ \  \text{for } j \in [m],
%\end{align}
%\end{lemma}

The constant $C_1 > 0$ in Thm.~\ref{DeterministicUniquenessTheorem} is then given by\footnote{Note that $\|\mathbf{AX}_{I(S)}\mathbf{c}\|_2 \geq \sqrt{k} L_\mathcal{H}(\mathbf{A})\|\mathbf{X}_{I(S)}\mathbf{c}\|_2 \geq k L_\mathcal{H}(\mathbf{A}) L_k(\mathbf{X}_{I(S)})\|\mathbf{c}\|_2$ for $S \in \mathcal{H}$ and $k$-sparse $\mathbf{c}$. Therefore, $L_k(\mathbf{AX}_{I(S)}) \geq \sqrt{k} L_\mathcal{H}(\mathbf{A}) L_k(\mathbf{X}_{I(S)}) > 0$ since $L_{2\mathcal{H}}(\mathbf{A})> 0$ and $L_k(\mathbf{X}_{I(S)}) > 0$ by general linear position of the $\mathbf{x}_i$. Thus, $C_1 > 0$.}:
\begin{align}\label{Cdef1}
C_1(\mathbf{A}, \{\mathbf{x}_i\}_{i=1}^N, \mathcal{H}) := \frac{ C_2(\mathbf{A}, \mathcal{H}) } { \min_{S \in \mathcal{H}} L_k(\mathbf{AX}_{I(S)}) }.
\end{align}
where, given vectors $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$, we denote by $\mathbf{X}$ the $m \times N$ matrix with columns $\mathbf{x}_i$ and by $I(S)$ the set of indices $i$ for which $\mathbf{x}_i$ is supported in $S$.

The constant $C_2 = C_2(\mathbf{A}, \mathcal{H})$ is given in turn below, in terms of one used in \cite{Deutsch12} to analyze the convergence of the alternating projections algorithm for projecting a point onto an intersection of subspaces. 
We use it to bound the distance between a point and the intersection of subspaces given an upper bound on its distance from each individual subspace.

\begin{definition}\label{SpecialSupportSet}\label{FriedrichsDefinition}
For subspaces $V_1, \ldots, V_\ell \subseteq \mathbb{R}^m$, set $r := 1$ when $\ell = 1$ and define for $\ell \geq 2$:
\begin{align*}
r(\{V_i\}_{i=1}^\ell) := 1 - \left(1 -  \max \prod_{i=1}^{\ell-1} \sin^2  \theta \left(V_i, \cap_{j>i} V_j \right)  \right)^{1/2},
\end{align*} 
%
where the maximum is taken over all orderings\footnote{We modify the quantity in \cite{Deutsch12} in this way since the subspace ordering is irrelevant to our purpose.} of the $V_i$ and the angle $\theta \in (0,\frac{\pi}{2}]$ is defined implicitly as \cite[Def.~9.4]{Deutsch12}:
\begin{align*}
\cos{\theta(U,W)} := \max\left\{ |\langle \mathbf{u}, \mathbf{w} \rangle|: \substack{ \mathbf{u} \in U \cap (U \cap W)^\perp, \ \|\mathbf{u}\|_2 \leq 1 \\ \mathbf{w} \in W \cap (U \cap W)^\perp, \  \|\mathbf{w}\|_2 \leq 1 } \right\}.
\end{align*}
\end{definition}
Note that $\theta \in (0,\frac{\pi}{2}]$ implies $0 < r \leq 1$.\footnote{We acknowledge the counter-intuitive property that $\theta =  \pi/2$ when $U = V$ or $U \perp V$.}  
The constant $C_2$ in Lem.~\ref{MainLemma} can then be expressed as:  
\begin{align}\label{Cdef2}
C_2(\mathbf{A}, \mathcal{H}) := \frac{ 2^{|\mathcal{H}|} \max_{j \in [m]} \|\mathbf{A}_j\|_2}{ \min_{\mathcal{G} \subseteq \mathcal{H}} r( \{ \bm{\mathcal{A}}_S \}_{S \in \mathcal{G}}) },
\end{align}
%
which we remark is consistent with the assumption on $C_1$ in the proof of the case $k=1$ at the beginning of this section.

\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessCorollary} for $k < m$] 
We demonstrate the existence of a map $\pi: \mathcal{H} \to 2^{[\bar m]}$ for which the distance $d(\bm{\mathcal{A}}_S, \bm{\mathcal{B}}_{\pi(S)})$ is controlled by $\varepsilon$. Applying Lem.~\ref{MainLemma} then completes the proof. 

%We shall show that for every $S \in \mathcal{H}$ there is some $\bar S \in {[\bar m] \choose k}$ for which the distance $d(\bm{\mathcal{A}}_S, \bm{\mathcal{B}}_{\bar S})$ is controlled by $\varepsilon$. Applying Lem.~\ref{MainLemma} with the map $\pi$ defined by $S \mapsto \bar S$ then completes the proof.

Since there are more than $(k-1){\bar m \choose k}$ vectors $\mathbf{x}_i$ supported in $S$, the pigeonhole principle gives $\bar S \in {[\bar m] \choose k}$ and a set of $k$ indices $K \subseteq I(S)$ with all $\mathbf{\bar x}_i$, $i \in K$, supported in $\bar S$.

It follows from $L_{2\mathcal{H}}(\mathbf{A}) > 0$ and the general linear position of the $\mathbf{x}_i$ that $L(\mathbf{AX}_{K}) > 0$; that is, the columns of the $n \times k$ matrix $\mathbf{AX}_K$ form a basis for $\bm{\mathcal{A}}_S$. Fixing $\mathbf{0} \neq \mathbf{y} \in \bm{\mathcal{A}}_S$, there then exists $\mathbf{0} \neq \mathbf{c} = (c_1, \ldots, c_k) \in \mathbb{R}^k$ such that $\mathbf{y} = \mathbf{AX}_K\mathbf{c}$. Setting \mbox{$\mathbf{\bar{y}} = \mathbf{B\bar{X}}_K\mathbf{c} \in \bm{\mathcal{B}}_{\bar S}$}, we have:
\begin{align*}
\|\mathbf{y} - \mathbf{\bar{y}}\|_2 
&= \|\sum_{i=1}^k c_i(\mathbf{AX}_K - \mathbf{B\bar{X}}_K)_i\|_2
\leq \varepsilon \sum_{i=1}^k |c_i| \\
&\leq \varepsilon \sqrt{k}  \|\mathbf{c}\|_2 
\leq \frac{\varepsilon}{L(\mathbf{AX}_K)} \|\mathbf{y}\|_2,
\end{align*}
where the last inequality follows directly from \eqref{Ldef}. From Def.~\ref{dDef} we have:
\begin{align}\label{rhs222}
d(\bm{\mathcal{A}}_S, \bm{\mathcal{B}}_{\bar S}) 
\leq \frac{\varepsilon}{  L(\mathbf{AX}_{K}) } \leq \varepsilon \frac{C_1}{C_2},
\end{align}
%
where the second inequality is due to $L(\mathbf{AX}_{K}) \geq L_k(\mathbf{AX}_{I(S)})$ and \eqref{Cdef1}. Finally, apply Lem.~\ref{MainLemma} with $\varepsilon < L_2(\mathbf{A})/C_1$.
\end{proof}

\begin{proof}[Proof of Cor.~\ref{SLCopt}]
We bound the number of $k$-sparse $\mathbf{\bar x}_i$ and then apply Thm.~\ref{DeterministicUniquenessCorollary}. 
%We now apply this fact to bound the number of $k$-sparse $\mathbf{\bar x}_i$. 
Let $n_p$ be the number of $\mathbf{\bar x}_i$ with $\|\mathbf{\bar x}_i\|_0 = p$.
Since the $\mathbf{x}_i$ are all $k$-sparse, by \eqref{minsum} we have:
\mbox{$k \sum_{p = 0}^{\bar m} n_p \geq \sum_{i=0}^N \|\mathbf{x}_i\|_0 \geq \sum_{i=0}^N \|\mathbf{\bar x}_i\|_0 = \sum_{p=0}^{\bar m} p n_p.$}
Hence,
\begin{align}\label{eqn}
\sum_{p = k+1}^{\bar m} n_p \leq \sum_{p = k+1}^{\bar m} (p-k) n_p \leq \sum_{p = 0}^k (k-p)n_p \leq k \sum_{p = 0}^{k-1} n_p,
\end{align}
%
demonstrating that the number vectors $\mathbf{\bar x}_i$ that are \emph{not} $k$-sparse is controlled by how many are $(k-1)$-sparse. 

Next, observe that no more than $(k-1)|\mathcal{H}|$ of the $\mathbf{\bar x}_i$ share a support $\bar S$ of size less than $k$; otherwise, by the pigeonhole principle, at least $k$ of these indices $i$ belong to the same $K \subseteq I(S)$ for some $S \in \mathcal{H}$ and (as argued previously) \eqref{rhs222} follows. Since the right-hand side of \eqref{rhs222} is less than one, by \eqref{dimLem} we have the contradiction $k = \dim(\bm{\mathcal{A}}_S) \leq \dim(\bm{\mathcal{B}}_{\bar S}) \leq |\bar S|.$ 

The total number of $(k-1)$-sparse vectors $\mathbf{\bar x}_i$ can thus not exceed $|\mathcal{H}|(k-1){ \bar m \choose k-1}$. By \eqref{eqn} no more than $|\mathcal{H}|k(k-1){ \bar m \choose k-1}$ vectors $\mathbf{\bar x}_i$ are not $k$-sparse. Since for every $S \in \mathcal{H}$ there are over $(k-1)\left[ {\bar m \choose k} + |\mathcal{H}|k{ \bar m \choose k-1} \right]$ vectors $\mathbf{x}_i$ supported there, it follows that more than $(k-1){\bar m \choose k}$ of them have corresponding $\mathbf{\bar x}_i$ that are $k$-sparse. The result now follows directly from Thm.~\ref{DeterministicUniquenessCorollary}.
\end{proof}

\section{Discussion}\label{Discussion}
%[mention dictionary size independence]

% A significant implication is that the constant $C_1$ of Thm.~\ref{DeterministicUniquenessTheorem} can be computed efficiently if we restrict the class of possible supports for the $\mathbf{\bar x}_i$. 

[*** ADD:  Ganguli, Deep Learning, constants, tightness, ***]

In this note, we prove the stability of unique solutions to Probs.~\ref{InverseProblem} and \ref{OptimizationProblem} while significantly reducing the known sample complexity. Our results justify the application of the sparse linear coding model to blind source separation problems, wherein the goal is to infer the generating dictionary and sparse codes from noisy measurements, \eqref{LinearModel}. We also collect a set of useful mathematical tools and basic facts for future research. The main motivation for this work, however, was to understand how seemingly universal representations emerge from sparse coding models fit to natural data by a variety of methodologies. We elaborate on these applications below.

What we have shown here is that the sparse linear coding model generally produces a \textit{well-posed} inverse problem to be approximately solved by a numerical algorithm. This early concept of Hadamard \cite{Hadamard1902} can be paraphrased as the idea that inferences from observations should be robust to the inevitable uncertainty in measurement. In other words, a small perturbation of the data should result in only slightly different inferred parameters. In this regard, for the sparse linear coding model we demonstrate a linear relationship, \eqref{epsdel}, between measurement noise and the deviation of any solution from the true parameters, with explicit constants expressed in terms of these parameters. Moreover, we show that even if the meta-parameter for the number of dictionary elements is overestimated, a subset of parameters may still be identifiable up to noise. It would therefore be of practical utility to determine the best possible dependence of $\varepsilon$ on $\delta_1, \delta_2$ in Def.\ref{maindef} as well as the minimal requirements on the number and diversity of generating codes, and we hope that other researchers continue to improve and extend our results. We remark that our constants have been derived for deterministic ``worst-case'' noise, whereas the ``effective'' noise might be smaller when sampled from a distribution.  % ; in such cases, the constants will improve as well.

One notable component of our contribution is a combinatorial criteria (regular hypergraphs satisfying the singleton intersection property, Def.~\ref{sip}) for the support sets of sparse codes key to the identification of the dictionary. Fully understanding those combinatorial designs allowing for stable sparse representations is an interesting research area for the future. For instance, whether there is a sample size $N$ that is polynomial in $m,k$ gauranteeing the conclusions of Thm.~\ref{DeterministicUniquenessTheorem} has implications for the computational complexity of Probs.~\ref{InverseProblem} and \ref{OptimizationProblem} and related questions \cite{Tillmann15}. A reexamination of pigeonholing argument in the proof of Thm.~\ref{DeterministicUniquenessTheorem} demonstrates that this can in fact be achieved if one restricts the supports of the $\mathbf{\bar x}_i$ to a class that grows polynomially in $m, k$. 


A technical difficulty in proving Thm.~\ref{DeterministicUniquenessTheorem} was the absence of any kind of assumption on solutions to Prob.~\ref{InverseProblem}. Although mathematically interesting that no such requirement is necessary, there are other reasons to seek out such a theoretical guarantee. For instance, it is difficult to ensure that an algorithm maintain a dictionary satisfying \eqref{SparkCondition} at each iteration; indeed, even certifying a dictionary has this property is likely intractable given its NP-hardness \cite{tillmann2014computational}.

In fact, uniqueness guarantees with minimal assumptions apply to all areas of data science and engineering that utilize learned sparse representations. For example, several groups have applied compressed sensing to signal processing tasks: MRI analysis \cite{lustig2008compressed}, image compression \cite{Duarte08}, and, more recently, the design of an ultrafast camera \cite{Gao14}. Given such effective uses of compressed sensing, it is only a matter of time before these systems incorporate dictionary learning to encode and process data (e.g., in a device that learns structure from motion \cite{kong2016prior}). In these cases, assurances such as those offered by our theorems certify that different devices (with different initialization, samples, etc.) will learn equivalent representations given enough data from statistically identical systems.

In the field of theoretical neuroscience in particular, dictionary learning for sparse coding and related methods have recovered characteristic components of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent} and sounds \cite{bellsejnowski1996, smithlewicki2006, Carlson12} that reproduce response properties of cortical neurons. Our results suggest that this correspondence could be due to the ``universality'' of sparse representations in natural data, an early mathematical idea in neural theory \cite{pitts1947}. %Furthermore, they justify the hypothesis of \cite{Isely10} that sparse codes passed through information bottlenecks in the brain are recovered from random projections via (unsupervised) biologically plausible sparse coding (e.g., \cite{rehnsommer2007, rozell2007neurally, ganguli2012compressed, hu2014hebbian}).

Deep learning noise issues: \cite{goodfellow2014explaining}.

\acknow{We thank Friedrich Sommer and Darren Rhea for early thoughts, and Ian Morris for posting \eqref{eqdim} online.}
\showacknow % Display the acknowledgments section

\bibliography{chazthm_pnas}

\section{Appendix}\label{proofs}

We prove Lem.~\ref{MainLemma} after stating auxiliary lemmas and then sketch the proofs of Thm.~\ref{robustPolythm} and Cor.~\ref{ProbabilisticCor}.

%\begin{lemma}\label{spanIntersectionLemma}
%Let $\mathbf{M} \in \mathbb{R}^{n \times m}$. If every $2k$ columns of $\mathbf{M}$ are linearly independent, then for any $F \subseteq {[m] \choose k}$:
%\begin{align*}
%\text{\rm span}\{\mathbf{M}_{\cap F}\}  = \bigcap_{S \in F} \text{\rm span}\{\mathbf{M}_S\}.
%\end{align*}
%\end{lemma}
%\begin{proof}
%Using induction on $|F|$, it is enough to prove the lemma when $|F| = 2$; but this case follows directly from the assumption.
%\end{proof}

\begin{lemma}\label{spanIntersectionLemma}
If $f: U \to V$ is an injective function then $f\left(\cap_{i=1}^\ell U_i \right) =  \cap_{i=1}^\ell f\left(U_i\right)$ for any $U_1, \ldots, U_\ell \subseteq U$.
%In particular, if for some $E \in 2^{[m]}$ the map $M \in \mathbb{R}^{n \times m}$ is injective on $\cup_{S \in \mathcal{H}} \text{span}\{e_i\}_{i \in S}$ then $\text{span}\{ M_{\cap_{S \in \mathcal{H}} S} \} = \cap_{S \in \mathcal{H}} \text{span}\{M_S\}$.
\end{lemma}
\begin{proof}
By induction it is enough to prove the case $|n| = 2$, but this case follows directly from the assumtion. %Clearly, for any map $f$, if $w \in f(U \cap V)$ then $w \in f(U)$ and $w \in f(V)$, hence $w \in f(U) \cap f(V)$. If $w \in f(U) \cap f(V)$ then $w \in f(U)$ and $w \in f(V)$, hence $w = f(u) = f(v)$ for some $u \in U$ and $v \in V$, implying $u = v$ by injectivity of $f$. Hence $u \in U \cap V$, and $w \in f(U \cap V)$.
\end{proof}

\begin{lemma}\label{DistanceToIntersectionLemma}
Fix $k \geq 2$. Let $V_1, \ldots, V_k$ be subspaces of $\mathbb{R}^m$ and set $V = \cap_{i = 1}^k V_i$. For  $\mathbf{x} \in \mathbb{R}^m$, we have (where $r$ is given in Def.~\ref{SpecialSupportSet}):
\begin{align}\label{DTILeq}
\text{\rm dist}(\mathbf{x}, V) \leq \frac{1}{r(\{V_i\}_{i = 1}^k)} \sum_{i=1}^k \text{\rm dist}(\mathbf{x}, V_i).
\end{align}
\end{lemma}
\begin{proof} 
Recall the orthogonal projection onto a subspace $V \subseteq \mathbb{R}^m$ is the mapping $\Pi_V: \mathbb{R}^m \to V$ that associates with each $\mathbf{x}$ its unique nearest point in $V$; i.e., $\|\mathbf{x} - \Pi_V\mathbf{x}\|_2 = \text{\rm dist}(\mathbf{x}, V)$.
Next, observe:
\begin{align}\label{f}
\|\mathbf{x} - \Pi_V\mathbf{x}\|_2 &\leq \|\mathbf{x} - \Pi_{V_k} \mathbf{x}\|_2 + \|\Pi_{V_k}  \mathbf{x} - \Pi_{V_k}\Pi_{V_{k-1}}\mathbf{x}\|_2 \nonumber \\
&\ \ \ + \cdots + \|\Pi_{V_k} \Pi_{V_{k-1}}\cdots \Pi_{V_1} \mathbf{x} - \Pi_V \mathbf{x}\|_2 \nonumber \\
&\leq \|\Pi_{V_k}\cdots\Pi_{V_{1}} \mathbf{x} - \Pi_V \mathbf{x}\|_2 + \sum_{\ell=1}^k \|\mathbf{x} - \Pi_{V_{\ell}} \mathbf{x}\|_2,
\end{align}
using the triangle inequality and that the spectral norm satisfies $\|\Pi_{V_{\ell}}\|_2 \leq 1$ for all $\ell$ (since $\Pi_{V_{\ell}}$ are orthogonal projections).

The desired result, \eqref{DTILeq}, now follows by bounding the second term on the right-hand side using the following fact \cite[Thm.~9.33]{Deutsch12}:
\begin{align}
\|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} \mathbf{x} - \Pi_V\mathbf{x}\|_2 \leq z \|\mathbf{x}\|_2,
\end{align}
for \mbox{$z^2= 1 - \prod_{\ell =1}^{k-1}(1-z_{\ell}^2)$} and \mbox{$z_{\ell} = \cos\theta\left(V_{\ell}, \cap_{s=\ell+1}^k V_s\right)$}, recalling $\theta$ from Def.~\ref{FriedrichsDefinition}. Together with $\Pi_{V_\ell} \Pi_V = \Pi_V$ for all $\ell \in [k]$ and $\Pi_V^2 = \Pi_V$, this implies that $\|\Pi_{V_k} \cdots \Pi_{V_1}\mathbf{x}  - \Pi_V \mathbf{x} \|_2$ satisfies:
\[\| ( \Pi_{V_k} \cdots\Pi_{V_1} - \Pi_V ) (\mathbf{x} - \Pi_V\mathbf{x})\|_2 \leq z\|\mathbf{x} - \Pi_V\mathbf{x}\|_2.\]

Finally, substituting this into \eqref{f} and rearranging produces \eqref{DTILeq} after replacing $1 - z$ with $r(\{V_i\}_{i=1}^k)$.
\end{proof}

\begin{lemma}\label{NonEmptyLemma}
Fix a hypergraph $\mathcal{H} \subseteq 2^{[m]}$ with nodes labeled in order of non-increasing degree and for which $|\cap h(i)| = 1$ for all $i \leq q$. Fix $\bar m$ and let $p \leq q$ be the largest number satisfying:
\begin{align}\label{pcond}
\sum_{i=\ell}^{m} \deg(i) > (\bar m + 1 - \ell) (\deg(\ell) - 1) \ \ \text{for all } \ell \leq p.
\end{align}

If the map $\pi: E \to 2^{[\bar m]}$ has $\sum_{S \in \mathcal{H}} (|\pi(S)| -|S|) \geq 0$ and:
\begin{align}\label{cond}
|\cap \pi(\mathcal{G})| \leq |\cap \mathcal{G} | \text{ for all } \mathcal{G} \subseteq \mathcal{H},
\end{align}
then $\bar m \geq\sum_i \deg(i) / \deg(1)$, and if $\bar m < \sum_i \deg(i) / (\deg(1) - 1)$ then the association $i \mapsto \cap \pi(h(i))$ defines an injective map to $[\bar m]$ from some nonempty $J \subseteq [m]$ of size $p$ consisting of the union of the set of all nodes of degree exceeding $\deg(p)$ and some set of nodes all having degree equal to $\deg(p)$. In particular, if $\mathcal{H}$ is $r$-regular then $|J| = \bar m - r(\bar m - m)$. 
\end{lemma}

\begin{proof}
Consider the collection of pairs: $T_1 := \{(i, S): i \in \pi(S), S \in \mathcal{H}\}$, which number $|T_1| = \sum_{S \in \mathcal{H}} |\pi(S)| \geq \sum_{S \in \mathcal{H}} |S| = \sum_{i \in [m]} \deg(i)$. Note that assumption \eqref{cond} implies $\bar m \geq |T_1| / \deg(1)$, since otherwise pigeonholing the elements of $T_1$ with respect to their set of possible first indices $[\bar m]$ would contradict that there are more than $\deg(1)$ sets in $\mathcal{H}$ sharing a common element. %QUESTION: Do we really need to say this..?

By \eqref{pcond} and the upper bound on $\bar m$ we have $|T_1| >  \bar m (\deg(1) - 1)$, which implies, again by the pigeonhole principle, that there must be at least $\deg(1)$ elements of $T_1$ sharing the same first index. By \eqref{cond}, the intersection of the set $\mathcal{G}_1$ consisting of their second indices is nonempty. As $p \leq q$ and $\deg(1) \geq \deg(i)$ for all $i$, it must be that $|\cap \mathcal{G}_1| = 1$. Since $\cap \pi(\mathcal{G}_1)$ is nonempty, applying \eqref{cond} again implies $\cap \pi(\mathcal{G}_1) = \{i_1\}$ for some $i_1 \in [m]$. If $p=1$ then we are done. Otherwise, define $T_2 := T_1 \setminus \{(i,S) \in T_1: i = i_1\}$, which contains $|T_2| = |T_1| - \deg(1) = \sum_{i=2}^m \deg(i)$ ordered pairs having $\bar m - 1$ distinct first indices. By \eqref{pcond} we have $|T_2| > (\bar m - 1)(\deg(2) - 1)$ and reiterating the above arguments produces a (necessarily) distinct index $i_2$. Iterating the arguments $p$ times yields the set of singletons \mbox{$J = \{\cap \mathcal{G}_1, \ldots, \cap \mathcal{G}_p\} \subseteq [m]$}.
%TODO: Let T_1 be the set of all pairs $T_1 := \{(R, S): R \subseteq \pi(S), S \in \mathcal{H}\}$ 
\end{proof}

\begin{proof}[Proof of Lem.~\ref{MainLemma}]
We will show that the bound \eqref{GapUpperBound} trickles through the intersection semi-lattices of $\{\bm{\mathcal{A}}_S\}_{S \in \mathcal{H}}$ and $\{\bm{\mathcal{B}}_{\bar S}\}_{\bar S \in \pi(\mathcal{H})}$ to yield \eqref{MainLemmaBPD} by virtue of the SIP.  

We begin by showing that $\dim(\bm{\mathcal{B}}_{\pi(S)}) = \dim(\bm{\mathcal{A}}_S)$ for all $S \in \mathcal{H}$. Since the right-hand side of \eqref{GapUpperBound} is less than one (by \eqref{delrho} and $r \leq 1$), it follows from \eqref{dimLem} that $|\pi(S)| \geq \dim(\bm{\mathcal{B}}_{\pi(S)}) \geq \dim(\bm{\mathcal{A}}_S) = |S|$, with the equality by injectivity of $\mathbf{A}$. Since $|S| = |\pi(S)|$, the required fact follows. Note that the columns of $\mathbf{B}_{\pi(S)}$ are therefore linearly independent, for all $S \in \mathcal{H}$.

We next show that \eqref{cond} holds.  Fix $\mathcal{G} \subseteq \mathcal{H}$. Since $\bm{\mathcal{B}}_{\cap \pi(\mathcal{G})} \subseteq \cap_{S \in \mathcal{G}} \bm{\mathcal{B}}_{\pi(S)}$, if $\cap_{S \in \mathcal{G}} \bm{\mathcal{B}}_{\pi(S)} = \{\textbf{0}\}$, then we must have $|\cap_{S \in \mathcal{G}} \pi(S)| = 0$ (as the columns of $\mathbf{B}_{\pi(S)}$ are linearly independent) and \eqref{cond} is trivially true. Suppose then that the intersection is not the zero vector. Keeping in mind that $d(U,V) \leq d(U',V)$ whenever $U \subseteq U'$ and applying (in order) Lem.~\ref{spanIntersectionLemma}, Lem.~\ref{DistanceToIntersectionLemma}, \eqref{eqdim}, \eqref{GapUpperBound} and finally \eqref{Cdef2} gives:
\begin{align}\label{randoml}
d(\bm{\mathcal{B}}_{\cap \pi(\mathcal{G})}&, \bm{\mathcal{A}}_{\cap \mathcal{G}}  ) 
\leq d\left( \cap_{S \in \mathcal{G}} \bm{\mathcal{B}}_{\pi(S)}, \cap_{S \in \mathcal{G}} \bm{\mathcal{A}}_S \right) \nonumber \\
&\leq \sum_{T \in \mathcal{G}} \frac{ d\left( \cap_{S \in \mathcal{G}} \bm{\mathcal{B}}_{\pi(S)},\bm{\mathcal{A}_{T}} \right) }{ r(  \{\bm{\mathcal{A}}_S\}_{S \in \mathcal{G}}) }
\leq \sum_{T \in \mathcal{G}} \frac{ d\left( \bm{\mathcal{B}}_{\pi(T)},\bm{\mathbf{A}}_{T} \right) }{ r( \{\bm{\mathcal{A}}_S\}_{S \in \mathcal{G}}) }\nonumber \\
&\leq \frac{|\mathcal{G}| \varepsilon}{r( \{\bm{\mathcal{A}}_S\}_{S \in \mathcal{G}})} 
\leq \frac{C_2 \varepsilon}{\max_i\|\mathbf{A}_i\|_2}. 
\end{align}

Since $\varepsilon < L_2(\mathbf{A}) / C_2$, by \eqref{delrho} the right-hand side in \eqref{randoml} is strictly less than one. Hence, $\dim(\bm{\mathcal{B}}_{\cap \pi(\mathcal{G})}) \leq \dim(\bm{\mathcal{A}}_{\cap \mathcal{G}})$ by \eqref{dimLem}, and \eqref{cond} follows from the linear independence of the columns of $\mathbf{A}_{S}$ and $\mathbf{B}_{\pi(S)}$ for all $S \in \mathcal{G}$.

Now, by Lem.~\ref{NonEmptyLemma}, the association $i \mapsto \cap \pi(h(i))$ defines an injective map $\bar \pi: J \to [\bar m]$ for some $J \in {[m] \choose p}$ with $p$ given by \eqref{pcond}, and we can be sure that $\mathbf{B}_{\bar \pi(i)} \neq \mathbf{0}$ for all $i \in J$ since the columns of $\mathbf{B}_{\pi(S)}$ are linearly independent for all $S \in \mathcal{H}$. It then follows from \eqref{eqdim} and \eqref{randoml} that $d\left( \bm{\mathcal{A}}_i, \bm{\mathcal{B}}_{\bar \pi(i)} \right) \leq C_2 \varepsilon / \max_i \|\mathbf{A}_i\|_2$ for all $i \in J$. Fixing $\bar \varepsilon = C_2\varepsilon$ and letting $c_i = \|\mathbf{A}_i\|_2^{-1}$, we thus have that for every basis vector $\mathbf{e}_i \in \mathbb{R}^m$ with $i \in J$ there exists some $\bar{c}_i \in \mathbb{R}$ such that $\|c_i\mathbf{A}\mathbf{e}_i - \bar{c}_i \mathbf{B}\mathbf{e}_{\bar \pi(i)}\|_2 \leq \bar \varepsilon < L_2(\mathbf{A}) \min_{i\in J} |c_i|$.  But this is exactly the supposition in \eqref{1D}, and the result follows from the case $k=1$ in Sec.~\ref{DUT} applied to the submatrix $\mathbf{A}_J$.
\end{proof}

%\section{Proofs of Thm.~\ref{robustPolythm} and Cor.~\ref{ProbabilisticCor}}

\begin{proof}[Proof (sketch) of Thm.~\ref{robustPolythm}]
Let $M$ be the matrix with columns $\mathbf{A}\mathbf{x}_i$, $i \in [N]$.  Consider the polynomial in the entries of $\mathbf{A}$ and $\mathbf{x}_i$:
\begin{align*}
g(\mathbf{A}, \{\mathbf{x}_i\}_{i=1}^N) := \prod_{S \in {[N] \choose k}} \sum_{S' \in {[n] \choose k}} (\det M_{S',S})^2,
\end{align*}
with notation as in Sec.~\ref{Results}.  
It can be checked that when $g$ is nonzero for a substitution of real numbers for the indeterminates, all of the genericity requirements on $\mathbf{A}$ and $\mathbf{x}_i$ in our proofs of stability in Thm.~\ref{DeterministicUniquenessTheorem} are satisfied (in particular, the spark condition on $\mathbf{A}$).
\end{proof}

\begin{proof}[Proof (sketch) of Cor.~\ref{ProbabilisticCor}]
First, note that if a set of measure spaces $\{(X_{\ell}, \Sigma_{\ell}, \nu_{\ell})\}_{\ell=1}^p$ has that $\nu_{\ell}$ is absolutely continuous with respect to $\mu$ for all $\ell \in [p]$, where $\mu$ is the standard Borel measure on $\mathbb{R}$, then the product measure $\prod_{\ell=1}^p \nu_{\ell}$ is absolutely continuous with respect to the standard Borel product measure on $\mathbb{R}^p$. % for space:  (e.g.,  \cite{folland2013real}). 
By Thm.~\ref{robustPolythm}, there is a polynomial that is nonzero when $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$; in particular, stability holds with probability one.
\end{proof}

%\clearpage

%\section{$\ell_1$-norm Extension}
%
%\begin{problem}\label{OptimizationProblemL1}
%Find a matrix $\mathbf{B}$ and vectors \mbox{$\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$} that solve:
%\begin{align}\label{minsum}
%\min \sum_{i = 1}^N \|\mathbf{\bar x}_i\|_1 \ \ 
%\text{subject to} \ \ \|\mathbf{z}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \eta, \ \text{for all $i$}.
%\end{align}
%\end{problem}

%\begin{corollary}\label{SLCopt}
%Fix $\alpha > 0$ and suppose all of the assumptions of Thm.\ref{DeterministicUniquenessTheorem} hold with the following modifications:
%\begin{enumerate}
%\item The coefficients of all $k$-sparse $\mathbf{x}_i$ are drawn from $[-\beta, \beta]$, where $\beta < \frac{\alpha(k+1)}{k}$.
%\item More than $(k-1){\bar m \choose k} + q$ vectors $\mathbf{x}_i$ are supported in each $S \in \mathcal{H}$, where
%\[ q = \left[\frac{\alpha}{\beta}(k+1) - k\right]^{-1}k|\mathcal{H}|(k-1){\bar m \choose k-1}. \]
%\end{enumerate}
%Then all solutions to Prob.~\ref{OptimizationProblemL1} for which the nonzero elements of all $\mathbf{\bar x}_i$ lie outside $[-\alpha, \alpha]$ necessarily satisfy the implications \eqref{Cstable} and \eqref{b-PDa} of Thm.~\ref{DeterministicUniquenessTheorem}.
%\end{corollary}

%\begin{proof}[Proof of Cor.~\ref{SLCopt}]
%We bound the number of $k$-sparse $\mathbf{\bar x}_i$ and then apply Thm.~\ref{DeterministicUniquenessCorollary}. First, observe that no more than $(k-1)|\mathcal{H}|$ of the $\mathbf{\bar x}_i$ share a support $\bar S$ of size less than $k$; otherwise, by the pigeonhole principle, at least $k$ of these indices $i$ belong to the same $K \subseteq I(S)$ for some $S \in \mathcal{H}$ and (as argued previously) \eqref{rhs222} follows. Since the right-hand side of \eqref{rhs222} is less than one, by \eqref{dimLem} we have the contradiction $k = \dim(\bm{\mathcal{A}}_S) \leq \dim(\bm{\mathcal{B}}_{\bar S}) \leq |\bar S|.$ The total number of $(k-1)$-sparse vectors $\mathbf{\bar x}_i$ can thus not exceed $|\mathcal{H}|(k-1){ \bar m \choose k-1}$. 

%Next, observe that the assumptions of the lemma imply:
%\begin{align*}
%\alpha \sum_i \|\mathbf{\bar x}_i\|_0 \leq \sum_i \|\mathbf{\bar x}_i\|_1 \leq \sum_i \|\mathbf{x}_i\|_1 \leq \beta \sum_i \|\mathbf{x}_i\|_0.
%\end{align*}

%We now apply these facts to bound the number of $k$-sparse $\mathbf{\bar x}_i$. Let $n_p$ be the number of $\mathbf{\bar x}_i$ with $\|\mathbf{\bar x}_i\|_0 = p$. Since the $\mathbf{x}_i$ are all $k$-sparse, we have:
%\begin{align*}
%k \sum_{p = 0}^{\bar m} n_p = k N \geq \sum_{i=0}^N \|\mathbf{x}_i\|_0 \geq \frac{\alpha}{\beta} \sum_{i=0}^N \|\mathbf{\bar x}_i\|_0 = \frac{\alpha}{\beta} \sum_{p=0}^{\bar m} p n_p.
%\end{align*}
%Hence, 
%\begin{align*}
%\left[ \frac{\alpha(k+1)}{\beta} - k\right] \sum_{p = k+1}^{\bar m} n_p 
%&\leq \sum_{p = k+1}^{\bar m} (\frac{\alpha}{\beta} p-k) n_p
%\leq \sum_{p = 0}^k (k - \frac{\alpha}{\beta} p)n_p \\
%&\leq k \sum_{p = 0}^k n_p 
%\leq k(1-\frac{\alpha}{\beta})n_k + k|\mathcal{H}|(k-1){ \bar m \choose k-1}
%\end{align*}

%[Shit, we need $\alpha = \beta$.] Therefore, no more than $q$ vectors $\mathbf{\bar x}_i$ are \emph{not} $k$-sparse. Since for every $S \in \mathcal{H}$ there are over $(k-1){\bar m \choose k} + q$ vectors $\mathbf{x}_i$ supported there, it follows that more than $(k-1){\bar m \choose k}$ of them have corresponding $\mathbf{\bar x}_i$ that are $k$-sparse. The result now follows directly from Thm.~\ref{DeterministicUniquenessCorollary}.
%\end{proof}

%\pagebreak

%\begin{corollary}\label{SLCoptL1}
%If all of the assumptions of Thm.\ref{DeterministicUniquenessTheorem} hold, only now with more than $(k-1)\left[ {\bar m \choose k} + |\mathcal{H}|k{\bar m \choose k-1}\right]$ vectors $\mathbf{x}_i$ supported in each $S \in \mathcal{H}$, then every solution to Prob.~\ref{OptimizationProblemL1} for which the average absolute nonzero coefficient of the $\mathbf{\bar x}_i$ is bounded below by that of the $\mathbf{x}_i$ necessarily satisfies the implications \eqref{Cstable} and \eqref{b-PDa} of Thm.~\ref{DeterministicUniquenessTheorem}.
%\end{corollary}

%\begin{proof}[Proof of Cor.~\ref{SLCoptL1}]
%Since the average absolute nonzero coefficient of the $\mathbf{x}_i$ is given by $\alpha = \sum_i \|\mathbf{x}_i\|_1 / \sum_i \|\mathbf{x}_i\|_0$, we have:
%\begin{align*}
%\alpha \sum_i \|\mathbf{\bar x}_i\|_0 \leq \sum_i \|\mathbf{\bar x}_i\|_1 \leq \sum_i \|\mathbf{x}_i\|_1 = \alpha \sum_i \|\mathbf{x}_i\|_0.
%\end{align*}
%
%and the result follows by the same arguments as in the proof of Cor.~\ref{SLCopt}.
%\end{proof}

\end{document}