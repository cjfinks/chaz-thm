\documentclass[9pt,twocolumn]{pnas-new}
% Use the lineno option to display guide line numbers if required.
% Note that the use of elements such as single-column equations
% may affect the guide line number alignment. 

%%% CHAZ TODO %%%

%%% CHRIS TODO %%
% steamroll

% IMPORTED PACKAGES
\usepackage{amsmath, amssymb, amsthm} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

%\usepackage[pdftex]{graphicx}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}
%\usepackage{cite}

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} = Template for a one-column mathematics article
% {pnasinvited} = Template for a PNAS invited submission

\title{On the uniqueness and stability of dictionaries for sparse representation of noisy signals}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,b, 1]{Charles J. Garfinkle}
\author[a,1]{Christopher J. Hillar} 

\affil[a]{Redwood Center for Theoretical Neuroscience, Berkeley, CA, USA}
\affil[b]{Helen Wills Neuroscience Institute, UC Berkeley}

% Please give the surname of the lead author for the running footer
\leadauthor{Garfinkle} 

\significancestatement{Many naturally occurring signals (visual scenery, speech, EEG, etc.) lacking domain-specific formal models can nonetheless be usefully characterized as linear combinations of few elementary waveforms drawn from a large `dictionary'. We give general conditions guaranteeing when such dictionaries are uniquely and stably determined by data. The result justifies the use of this model as a constraint for blind source separation, and may help explain the observed universality of emergent representations in sparse models of some natural phenomena.
}

\authordeclaration{The authors declare no conflict of interest.}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: chaz@berkeley.edu, chillar@msri.org}

\keywords{Sparse coding, dictionary learning, matrix factorization, compressive sensing, inverse problems} 

\begin{abstract}
Dictionary learning for sparse linear coding has exposed some characteristic properties of many natural signals.
However, universal theorems which guarantee the consistency of estimation in this model are lacking.
Here, we prove that for all diverse enough datasets generated from the sparse coding model, latent dictionaries and codes are uniquely and stably determined up to measurement error.  Applications are given to data analysis, engineering, and neuroscience. 
\end{abstract}

% \dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

% Optional adjustment to line up main text (after abstract) of first page with line numbers, when using both lineno and twocolumn options.
% You should only change this length when you've finalised the article contents.
\verticaladjustment{-2pt}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.
\dropcap{B}lind source separation is a classical problem in signal processing \cite{sato1975method}.
A common modern assumption is that each of $N$ observed $n$-dimensional signals is a (noisy) linear combination of at most $k$ elementary waveforms drawn from some unknown ``dictionary'' of size $m$, typically with $k < m \ll N$ (see \cite{Zhang15} for a comprehensive review of this and related models).
Approximating solutions to this sparsity-constrained inverse problem have provided insight into the structure of many signal classes lacking domain-specific formal models (e.g., in vision \cite{wang2015sparse}).  In particular, it has been shown that response properties of simple-cell neurons in mammalian visual cortex emerge from optimizing a dictionary to represent small patches of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent}, a major advance in computational neuroscience. A curious aspect of this finding is that the latent waveforms (e.g. `Gabor' wavelets) estimated from data appear to be canonical \cite{donoho2001can};
i.e., they are found in learned dictionaries independent of algorithm or natural image training set.

Motivated by these discoveries and earlier work in the theory of neural communication \cite{Coulter10, Isely10}, we address when dictionaries and the sparse representations they induce are uniquely determined by data.  Answers to this question also have other real-world implications.  For example, a sparse coding analysis of local painting style can be used for forgery detection \cite{hughes2010, Olshausen10}, but only if all dictionaries consistent with training data do not differ appreciably in their ability to sparsely encode new samples. 
Fortunately, algorithms with proven recovery of generating dictionaries under certain 
conditions have recently been proposed (see \cite[Sec.~I-E]{Sun16} for a summary of the state-of-the-art). Few theorems, however, can be cited to explain this uniqueness phenomenon more generally.

Here, we prove very generally that uniqueness and stability in sparse linear coding is an expected property of the model. 
More specifically, dictionaries that preserve sparse codes (i.e., satisfy a `spark condition') are identifiable from as few as \mbox{$N = m(k-1){m \choose k} + m$} noisy sparse linear combinations of their columns up to an error linear in the noise (Thm.~\ref{DeterministicUniquenessTheorem}). In fact, provided $n \geq \min(2k,m)$, in almost all cases the dictionary learning problem is well-posed (as per Hadamard \cite{Hadamard1902}) given enough data (Cor.~\ref{ProbabilisticCor}). Moreover, these guarantees hold without assuming the recovered matrix satisfies a spark condition, even when the number $m$ of dictionary elements is unknown. The explicit, algorithm-independent criteria we provide should be a useful theoretical tool.  %  in the theory of sparse representations.

More formally, let $\mathbf{A} \in \mathbb R^{n \times m}$ be a matrix with columns $\mathbf{A}_j$ ($j = 1,\ldots,m$) and let dataset $Z$ consist of measurements:
\begin{align}\label{LinearModel}
\mathbf{z}_i = \mathbf{A}\mathbf{x}_i + \mathbf{n}_i,\ \ \  \text{$i=1,\ldots,N$},
\end{align}
for $k$-\emph{sparse} $\mathbf{x}_i \in \mathbb{R}^m$ having at most $k$ nonzero entries and \emph{noise} $\mathbf{n}_i \in \mathbb{R}^n$, with bounded norm $\| \mathbf{n}_i \|_2 \leq  \eta$ representing our combined worst-case uncertainty in  measuring $\mathbf{A}\mathbf{x}_i$.
The precise mathematical problem addressed here is the following.

\begin{problem}[Sparse linear coding]\label{InverseProblem}
Find a matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$ such that $\|\mathbf{z}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \eta$ for all $i$.
\end{problem}

Note that any particular solution $(\mathbf{B}, \mathbf{\bar x}_1 \ldots, \mathbf{\bar x}_N)$ to this problem gives rise to an orbit of equivalent solutions $(\mathbf{BPD}, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_1, \ldots, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_N)$, where $\mathbf{P}$ is any permutation matrix and $\mathbf{D}$ any invertible diagonal. Previous theoretical work addressing the noiseless case $\eta =0$ (e.g., \cite{li2004analysis, Georgiev05, Aharon06, Hillar15}) has shown that a solution to Prob.~\ref{InverseProblem} (when it exists) is indeed unique up to this inherent ambiguity provided the $\mathbf{x}_i$ are sufficiently diverse and the generating matrix $\mathbf{A}$ satisfies the \textit{spark condition} from compressive sensing:
\begin{align}\label{SparkCondition}
\mathbf{A}\mathbf{x}_1 = \mathbf{A}\mathbf{x}_2 \implies \mathbf{x}_1 = \mathbf{x}_2, \ \ \ \text{for all $k$-sparse } \mathbf{x}_1, \mathbf{x}_2,
\end{align}
%
which would be, in any case, necessary for uniqueness. Our concern here is solution stability with respect to noise.

\begin{definition}\label{maindef}
Fix $Y = \{ \mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$. We say $Y$ has a \textbf{$k$-sparse representation in $\mathbb{R}^m$} if there exists a matrix $\mathbf{A}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ such that $\mathbf{y}_i = \mathbf{A}\mathbf{x}_i$ for all $i$. 
This representation is \textbf{stable} if for every $\delta_1, \delta_2 \geq 0$, there exists $\varepsilon(\delta_1, \delta_2) \geq 0$ (with $\varepsilon > 0$ when  $\delta_1, \delta_2 > 0$) such that if $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$ satisfy:
\begin{align*}
\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon,\ \ \   \text{for all $i$},
\end{align*}
%
then there is some permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ such that for all $i, j$:
\begin{align}\label{def1}
\|\mathbf{A}_j - (\mathbf{BPD})_j\|_2 \leq \delta_1 \ \ \text{and} \ \ \|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i\|_1 \leq \delta_2.
\end{align}
\end{definition}

\pagebreak

To see how Def. \ref{maindef} directly relates to Prob. \ref{InverseProblem}, suppose that $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ and fix $\delta_1, \delta_2$ to be the desired recovery accuracy in \eqref{def1}. Consider now any dataset $Z$ generated as in \eqref{LinearModel} with $\eta \leq \frac{1}{2} \varepsilon(\delta_1, \delta_2)$. Then from the triangle inequality, it follows that any matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^m$ solving Prob.~\ref{InverseProblem} are necessarily close to the original dictionary $\mathbf{A}$ and sparse codes $\mathbf{x}_i$. 

In the next section, we give precise statements of our main results, which include an explicit form for $\varepsilon(\delta_1, \delta_2)$. We then prove our main theorem (Thm.~\ref{DeterministicUniquenessTheorem}) in Sec.~\ref{DUT} after stating some additional definitions and lemmas required for the proof, including a useful result in combinatorial matrix analysis (Lem.~\ref{MainLemma}). We also give a simple argument extending our guarantees to the following more common optimization formulation of the dictionary learning problem (Cor.~\ref{SLCopt}).

\begin{problem}\label{OptimizationProblem}
Find a matrix $\mathbf{B} \in \mathbb{R}^{\bar m}$ and vectors \mbox{$\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N \in \mathbb{R}^{\bar m}$} that solve:
\begin{align}\label{minsum}
\min \sum_{i = 1}^N \|\mathbf{\bar x}_i\|_0 \ \ 
\text{subject to} \ \ \|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon, \ \text{for all $i$}.
\end{align}
\end{problem}

All other proofs are relegated to the Section \ref{proofs}. 
Finally, we present several applications in Discussion Sec.~\ref{Discussion}. 

\section{Results}\label{Results}

Before stating our main results, we first identify combinatorial criteria on the support sets of generating codes that imply stable sparse representations.  Letting $\{1, \ldots, m\}$ be denoted $[m]$, its power set $2^{[m]}$, and ${[m] \choose k}$ the set of subsets of $[m]$ of size $k$,
we say a hypergraph $E \subseteq 2^{[m]}$ on vertices $[m]$ is \textit{$k$-uniform} when in fact $E \subseteq {[m] \choose k}$. We also say $E$ is \emph{regular} when every element of $[m]$ is contained in exactly $\ell$ elements of $E$ for some $\ell > 0$ (for given $\ell$, we say $E$ is \textit{$\ell$-regular}).

\begin{definition}\label{sip}
Given $E \subseteq 2^{[m]}$, the \textbf{star} $H(i)$ at $i$ is the set of $S \in E$ with $i \in S$. We say $E$ has the \textbf{singleton intersection property} (\textbf{SIP}) when $\cap H(i) = \{i\}$ for all $i \in [m]$. %We say $E$ has the SIP \textbf{of order $p$} when $\cap F(i) = \{i\}$ for every node $i$ for which $\text{deg(i)} \geq d_p$, where $d_1 \geq \ldots \geq d_m$ is the degree sequence of $E$.
\end{definition}

Next, we describe a quantitative version of the spark condition.  
The \emph{lower bound} $L$ of a matrix $\mathbf{M} \in \mathbb R^{n \times m}$ is the largest number $\alpha$ such that \mbox{$\|\mathbf{M}\mathbf{x}\|_2 \geq \alpha\|\mathbf{x}\|_2$} for all $\mathbf{x} \in \mathbb{R}^m$ \cite{Grcar10}. By compactness of the unit sphere, every injective linear map has a nonzero lower bound; hence, if $\mathbf{M}$ satisfies \eqref{SparkCondition}, then each submatrix formed from $2k$ of its columns or less has a nonzero lower bound. This motivates the following definition: 
%\begin{align*}
%L_k(\mathbf{M}) := \frac{1}{\sqrt{k}}\max \{ \alpha : \|\mathbf{M}\mathbf{x}\|_2 \geq \alpha\|\mathbf{x}\|_2 \text{ for all $k$-sparse } \mathbf{x}\}.
%\end{align*}
\begin{align*}
L_E(\mathbf{M}) := \inf \left\{ \frac{ \|\mathbf{M}_{S \cup S'}\mathbf{x}\|_2 }{ \sqrt{2k} \|\mathbf{x}\|_2} : \mathbf{x} \in \mathbb{R}^{|S\cup S'|}, \ \ S,S' \in E \right\}
%\frac{1}{\sqrt{k}}\max \{ \alpha : \|\mathbf{M}\mathbf{x}\|_2 \geq \alpha\|\mathbf{x}\|_2 \text{ for all $k$-sparse } \mathbf{x}\}.
\end{align*} 
%
and we write $L_{2k}:= L_E$ when $E = {[m] \choose k}$. Note that $ L = L_{\{[m]\}}$ whereas $L_{[m]} = L_2$. [TODO: note that $L_2$ implied by $L_E$]

Clearly, for any $\mathbf{M}$ satisfying \eqref{SparkCondition}, we have $L_{k'}(\mathbf{M}) > 0$ for  $k' \leq 2k$. We note that the quantity $1 - \sqrt{k} L_k(\mathbf{M})$ is also known in the compressive sensing literature as the (asymmetric) lower restricted isometry constant \cite{Blanchard2011}.
 
A vector $\mathbf{x}$ is said to be \emph{supported} on $S \subseteq [m]$ when $\mathbf{x} \in \text{\rm Span}( \{\mathbf{e}_j\}_{j\in S})$, where $\mathbf{e}_j$ are the standard basis in $\mathbb R^m$. For any index set $J$, denote by by $\mathbf{x}^J$ the subvector formed from the entries of $\mathbf{x}$ indexed by $J$, and similarly by $\mathbf{M}_J$ the submatrix formed by the columns of $\mathbf{M}$ indexed by $J$, where we set $\text{\rm Span}\{\mathbf{M}_\emptyset\} := \{\textbf{0}\}$. A set of $k$-sparse vectors is said to be in \emph{general linear position} when any $k$ of them are linearly independent. The following is a precise statement of our main result.

\begin{theorem}\label{DeterministicUniquenessTheorem}
Fix a matrix $\mathbf{A} \in \mathbb{R}^{n \times m}$ with $L_E(\mathbf{A}) > 0$ for some regular $E \subseteq {[m] \choose k}$ with the SIP. If the sequence \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} contains, for each $S \in E$, more than $(k-1){m \choose k}$ vectors in general linear position supported on $S$, then there is a constant\footnote{We delay defining the constant $C_1$ until Section \ref{DUT} (\eqref{Cdef1}).} $C_1 > 0$ for which the following holds for all $\varepsilon < L_{2}(\mathbf{A}) / C_1$:

Every real $n \times \bar m$ matrix $\mathbf{B}$ for which there exist $k$-sparse $\mathbf{\bar x}_1, \ldots, \mathbf{\bar x}_N$ satisfying \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon$} for all $i$ has $\bar m \geq m$ and, provided $\bar m < mr/(r-1)$, an associated permutation $\mathbf{P}$ and invertible diagonal $\mathbf{D}$ such that:
\begin{align}\label{Cstable}
\|(\mathbf{A_J})_j- \mathbf{B}_{\bar J} \mathbf{PD}_j\|_2 \leq C_1 \varepsilon \ \ \text{for all } j \in J,
\end{align}
%
for some non-empty $J, \bar J \subseteq [m]$ of size $\bar m - r(\bar m - m)$. 

Moreover, if in fact $L_{2k}(\mathbf{A}) > 0$ and $\varepsilon < L_{2k}(\mathbf{A}) / C_1$ then $L_{2k}(\mathbf{B}\mathbf{PD}) \geq L_{2k}(\mathbf{A}) - C_1 \varepsilon$ and:
\begin{align}\label{b-PDa}
\|\mathbf{x}^J_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}^{\bar J}_i\|_1 &\leq  \left( \frac{ 1+C_1 \|\mathbf{x}^{J}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right) \varepsilon,\ \ \   \text{for $i \in [N]$}.
\end{align}
\end{theorem}
% You can recover matrices that don't satisfy the spark condition, but to uniquely recover k-sparse codes (since no limitation on their supports!) you need to have the spark condition. i guess...? Or can we use our knowledge of the particular permutation P to get rid of spark condition?

%To be clear, the implication of Thm.~\ref{DeterministicUniquenessTheorem} is that $Y = \{\mathbf{Ax}_1, \ldots, \mathbf{Ax}_N\}$ has a stable $k$-sparse representation in $\mathbb{R}^m$, with \eqref{def1} guaranteed provided $\varepsilon$ in Def.~\ref{maindef} does not exceed: 
To be clear, Thm.~\ref{DeterministicUniquenessTheorem} says that the smaller the difference $\bar m - m$, the more columns and coefficients of the original dictionary $\mathbf{A}$ and codes $\mathbf{x}_i$ contained (up to noise) in the appropriately scaled dictionary $\mathbf{B}$ and codes $\mathbf{\bar x}_i$. The implication in the case $\bar m = m$ is that $Y = \{\mathbf{Ax}_1, \ldots, \mathbf{Ax}_N\}$ has a stable $k$-sparse representation in $\mathbb{R}^m$, with \eqref{def1} guaranteed provided $\varepsilon$ in Def.~\ref{maindef} does not exceed: 
\begin{align}\label{epsdel}
\varepsilon(\delta_1, \delta_2) := \min \left\{ \frac{\delta_1}{ C_1 }, \frac{ \delta_2 L_{2k}(\mathbf{A})}{ 1 + C_1 \left( \max_{i \in [N]} \|\mathbf{x}_i\|_1  + \delta_2 \right) } \right\}.
\end{align}

%In fact, a more general result (stated clearly in the next section) can be gleaned from our method of proving Thm.~\ref{DeterministicUniquenessTheorem}. Briefly, in cases where $\mathbf{B}$ has $\bar m \neq m$ columns, or $E$ is not regular or only partially satisfying the SIP, a relation between $\bar m$ and the degree sequence of nodes in $E$ gives indices $J \subseteq [m]$ defining a submatrix $\mathbf{A}_J$ and subvectors $\mathbf{x}_i^J$ that are recoverable in the sense of \eqref{Cstable} and \eqref{b-PDa}. For example, if $E$ is $\ell$-regular with the SIP but $m \leq \bar m < m\ell/(\ell - 1)$ then we have nonzero $|J| = \bar m - \ell(\bar m - m)$. The implication here is that the smaller the difference $\bar m - m$, the more columns and code entries of the original $n \times m$ dictionary $\mathbf{A}$ and codes $\mathbf{x}_i$ contained (up to noise) in the appropriately scaled $n \times \bar m$ dictionary $\mathbf{B}$ and codes $\mathbf{\bar x}_i$. When $\bar m = m$, we recover Thm.~\ref{DeterministicUniquenessTheorem}.
Regarding the assumptions of Thm.~\ref{DeterministicUniquenessTheorem}, it is easy to verify that for every $k < m$, there is a regular $k$-uniform hypergraph that satisfies the SIP; for instance, consecutive intervals of length $k$ in some cyclic order on $[m]$.
In fact, even if $E$ is not regular or only partially satisfies the SIP, a relation between $\bar m$ and the degree sequence of nodes in $E$ may give the indices $J \subseteq [m]$. For sake of brevity, we delay to the next section a clear statement of this more general result.

It also happens that producing sparse codes $\mathbf{x}_i$ in general linear position is straightforward with a ``Vandermonde'' matrix construction (e.g., see \cite{Hillar15}). We therefore have:

\begin{corollary}\label{DeterministicUniquenessCorollary}
Given $n, m$, $k < m$, and a regular hypergraph $E \subseteq {[m] \choose k}$ with the SIP, there are $N =  |E| \left[ (k-1){m \choose k} + 1  \right]$ vectors \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} such that every matrix $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} generates a dataset $Y = \{\mathbf{A}\mathbf{x}_1, \ldots, \mathbf{A}\mathbf{x}_N\}$ with a stable $k$-sparse representation in $\mathbb{R}^m$.
\end{corollary}

As already mentioned, there exist $k$-uniform regular hypergraphs $E$ with the SIP having cardinality $|E| = m$, implying the lower bound for sample size $N$ from the introduction. In many cases, however, the SIP can be achieved with far fewer supports; for example, when $k = \sqrt{m}$, take $E$ to be the $2k$ rows and columns formed by arranging $[m]$ in a square grid. 

There are other less direct consequences of Thm.~\ref{DeterministicUniquenessTheorem}. For instance, it has the following implications for the optimization problem posed in Prob.~\ref{SLCopt}:

\begin{corollary}\label{SLCopt}
If all of the assumptions of Thm.\ref{DeterministicUniquenessTheorem} hold with more than $(k-1)\left[ {\bar m \choose k} + |E|k{\bar m \choose k-1}\right]$ vectors $\mathbf{x}_i$ supported on each $S \in E$, then all solutions to Prob.~\ref{OptimizationProblem} necessarily satisfy the implications \eqref{Cstable} and \eqref{b-PDa} of Thm.~\ref{DeterministicUniquenessTheorem}.
\end{corollary}

%\pagebreak

Another extension of Thm.~\ref{DeterministicUniquenessTheorem} follows from the following analytic characterization of the spark condition.  Let $\mathbf{A}$  be the $n \times m$ matrix of $nm$ indeterminates $A_{ij}$. When real numbers are substituted for $A_{ij}$, the resulting matrix satisfies \eqref{SparkCondition} if and only if the following polynomial is nonzero:
\begin{align*}
f(\mathbf{A}) := \prod_{S \in {[m] \choose k}} \sum_{S' \in {[n] \choose k}} (\det \mathbf{A}_{S',S})^2,
\end{align*}
%
where for any $S' \in {[n] \choose k}$ and $S \in {[m] \choose k}$, the symbol $\mathbf{A}_{S',S}$ denotes the submatrix of entries $A_{ij}$ with $(i,j) \in S' \times S$.   We note that the large number of terms in this product is likely necessary due to the NP-hardness of deciding whether a given matrix $\mathbf{A}$ satisfies the spark condition \cite{tillmann2014computational}.

Since $f$ is an analytic function, if there exists one substitution of a real matrix $\mathbf{A}$ such that $f(\mathbf{A}) \neq 0$ then the zeroes of $f$ in fact form a set of measure zero. Fortunately, such a matrix $\mathbf{A}$ is easily constructed by adding rows of zeroes to any $\min(2k,m) \times m$ Vandermonde matrix $[\gamma_i^j]_{i,j=1}^{k,m}$ with distinct $\gamma_i$ (so that each term in the product above is nonzero). Hence, almost every real $n \times m$ matrix with $n \geq \min(2k,m)$ satisfies \eqref{SparkCondition}.

A similar phenomenon applies to datasets of vectors with a stable sparse representation. As in \cite[Sec.~IV]{Hillar15}, consider the ``symbolic'' dataset $Y = \{\mathbf{A}\mathbf{x}_1,\ldots,\mathbf{A} \mathbf{x}_N\}$ generated by indeterminate $\mathbf{A}$ and indeterminate $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N$.  

\begin{theorem}\label{robustPolythm}
There is a polynomial $g$ in the entries of $\mathbf{A}$ and $\mathbf{x}_i$ with the following property:  if $g$ evaluates to a nonzero number and more than \mbox{$(k-1){m \choose k}$} of the resulting $\mathbf{x}_i$ are supported on each $S \in E$ for some regular $E \subseteq {[m] \choose k}$ with the SIP, then $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ (Def.~\ref{maindef}). In particular, all -- except for a Borel set of measure zero -- substitutions impart to $Y$ this property.
\end{theorem}

\begin{corollary}\label{ProbabilisticCor}
Fix $k < m$ and $n \geq \min(2k, m)$ and let the entries of $\mathbf{A} \in \mathbb{R}^{n \times m}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ be drawn independently from probability measures absolutely continuous with respect to the standard Borel measure $\mu$. If more than $(k-1){m \choose k}$ of the vectors $\mathbf{x}_i$ are supported on each $S \in E$ for a regular $E \subseteq {[m] \choose k}$ with the SIP, then $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ with probability one.
\end{corollary}

Thus, choosing the dictionary and sparse codes ``randomly'' almost certainly generates data with stable sparse representations.

\section{Proofs of Theorem~\ref{DeterministicUniquenessTheorem} and Corollary~\ref{SLCopt}}\label{DUT}

As mentioned in the previous section, Thm.~\ref{DeterministicUniquenessTheorem} is a particular case of a more general result that forgoes the assumptions of regularity and SIP on the hypergraph $E \subseteq {[m] \choose k}$. If instead we require only that the stars $\cap H(i)$ intersect at singletons for all $i \leq q$ (assuming that the nodes of $E$ are labeled in some order of non-increasing degree), we have that $\bar m \geq k|E| / \deg(1)$ and, provided $\bar m < k|E| / (\deg(1) - 1)$, the non-empty submatrix $J$ is of size equal to the largest number $p$ satisfying:
\begin{align}\label{pcond}
\sum_{i=\ell}^{m} \deg(i) > (\bar m + 1 - \ell) (\deg(\ell) - 1) \ \ \text{for all } \ell \leq p \leq q.
\end{align}
Specifically, $J$ consists of the union of the set of all nodes of degree exceeding $\deg(p)$ and some subset of those nodes with degrees equal to $\deg(p)$. For the benefit of the reader, we do not prove this more general result below; it can be gleaned by examining how exactly Lemma \ref{NonEmptyLemma} is incorporated into the proof of Lem.~\ref{MainLemma}.
%As mentioned in the previous section, Thm.~\ref{DeterministicUniquenessTheorem} is a particular case of a more general result, which requires a looser set of constraints on the hypergraph $E$ and which applies to $n \times \bar m$ matrices $\mathbf{B}$ (and $\bar m$-dimensional codes $\mathbf{\bar x}_i$) with $\bar m \neq m$:

%Fix $\bar m$ and suppose the assumptions of Thm.~\ref{DeterministicUniquenessTheorem} hold, only now with the constraints on $E \subseteq {[m] \choose k}$ being just that $|\cap H(i)| = 1$ for all $i \leq q$ (assuming w.l.o.g. that the nodes of $E$ are labeled in some order of non-increasing degree), and with more than $(k-1){\bar m \choose k}$ vectors $\mathbf{x}_i$ supported in g.l.p. on each $S \in E$. Then we must have $\bar m \geq k|E| / \deg(1)$ and, provided $\bar m < k|E| / (\deg(1) - 1)$, the guarantee \eqref{Cstable} holds for a submatrix $\mathbf{A}_J$, where $J \subseteq[m]$ is nonempty and of a size equal to the largest number $p$ satisfying:
%\begin{align}\label{pcond}
%\sum_{i=\ell}^{m} \deg(i) > (\bar m + 1 - \ell) (\deg(\ell) - 1) \ \ \text{for all } \ell \leq p \leq q.
%\end{align}
%Specifically, $J$ consists of the union of the set of all nodes of degree exceeding $\deg(p)$ and some subset of those nodes with degrees equal to $\deg(p)$. 

%For the benefit of the reader, we prove below the case where we forgo only the constraint that $\bar m = m$. This yields the implication $\bar m \geq m$ and \eqref{pcond} reduces to $|J| = \bar m - r(\bar m - m)$. The extension to the general result above can be seen by examining how exactly Lemma \ref{NonEmptyLemma} is incorporated into the overall proof. 

% ======== b - PDa =========

We now begin our proof of Thm.~\ref{DeterministicUniquenessTheorem} by showing how dictionary recovery \eqref{Cstable} already implies sparse code recovery \eqref{b-PDa} when $L_{2k}(\mathbf{A}) > 0$ and \mbox{$\varepsilon < L_{2k}(\mathbf{A}) / C_1$}, assuming without loss of generality that $\bar m = m$. First, note that $\|\mathbf{x}\|_1 \leq \sqrt{k} \|\mathbf{x}\|_2$ for $k$-sparse $\mathbf{x} \in \mathbb{R}^m$, which by definition of $L_E$ implies the following frequently applied inequality valid for any $E \subseteq 2^{[m]}$:
\begin{align}\label{delrho}
L_{E}(\mathbf{A}) \leq \frac{\|\mathbf{A}\mathbf{x}\|_2}{\sqrt{2k} \|\mathbf{x}\|_2} 
\leq  \max_{j \in [m]}\|\mathbf{A}_j\|_2,
\end{align}
For $k$-sparse $\mathbf{x} \in \mathbb{R}^m$, the triangle inequality gives \mbox{$\|(\mathbf{A}-\mathbf{BPD})\mathbf{x}\|_2  \leq C_1\varepsilon \|\mathbf{x}\|_1 \leq C_1 \varepsilon \sqrt{2k}  \|\mathbf{x}\|_2$}. Thus, 
\begin{align*}
\|\mathbf{BPD}\mathbf{x}\|_2 
&\geq | \|\mathbf{A}\mathbf{x}\|_2 - \|(\mathbf{A}-\mathbf{BPD})\mathbf{x}\|_2 | \\
&\geq \sqrt{2k} (L_{2k}(\mathbf{A}) -  C_1\varepsilon) \|\mathbf{x}\|_2.
\end{align*}

Hence, $L_{2k}(\mathbf{BPD}) \geq L_{2k}(\mathbf{A}) - C_1\varepsilon  > 0$. \eqref{b-PDa} then follows from:
\begin{align*}
\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i \|_1
&\leq \frac{\|\mathbf{BPD}(\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\bar x}_i)\|_2}{L_{2k}(\mathbf{BPD})} \\
&\leq \frac{\|(\mathbf{BPD} - \mathbf{A})\mathbf{x}_i\|_2 + \|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2}{L_{2k}(\mathbf{BPD})} \\
&\leq \frac{\varepsilon (1 + C_1 \|\mathbf{x}_i\|_1)}{L_{2k}(\mathbf{BPD})}.
\end{align*}

The heart of the matter is therefore \eqref{Cstable}, which we now finally establish, first in the important special case $k = 1$.

\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessCorollary} for $k=1$]
Since the only 1-uniform hypergraph with the SIP is $[m]$, we have $\mathbf{x}_i = c_i \mathbf{e}_i$ for $c_i \in \mathbb{R} \setminus \{\mathbf{0}\}$, $i \in [m]$. In this case, we may take any $C_1 \geq 1/ \min_{\ell \in [m]} |c_{\ell}|$. 

Fix $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} and suppose that for some $\mathbf{B}$ and $1$-sparse $\mathbf{\bar x}_i \in \mathbb{R}^{\bar m}$ we have  $\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\bar x}_i\|_2 \leq \varepsilon < L_2(\mathbf{A}) / C_1$ for all $i$. Then, there exist $\bar{c}_1, \ldots, \bar{c}_m \in \mathbb{R}$ and a map $\pi: [m] \to [\bar m]$ such that:
\begin{align}\label{1D}
\|c_j\mathbf{A}_j - \bar{c}_j\mathbf{B}_{\pi(j)}\|_2 \leq \varepsilon,\ \ \text{for $j \in [m]$}.
\end{align} 
Note that if $\bar{c}_j = 0$, then $\|c_j\mathbf{A}_j \|_2 \leq \varepsilon$ implies from \eqref{delrho} that $|c_j| < \min_{\ell \in [m]} | c_\ell |$, a contradiction.  Thus, $\bar{c}_j \neq 0$, $j \in [m]$.

We  now show that $\pi$ is injective (in particular, a permutation if $\bar m = m$). Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell$. Then, $\|c_{j}\mathbf{A}_{j} - \bar{c}_{j}\mathbf{B}_{\ell}\|_2 \leq \varepsilon$ and $\|c_{i}\mathbf{A}_{i} - \bar{c}_{i} \mathbf{B}_{\ell}\|_2  \leq \varepsilon$. Scaling and summing these inequalities by $|\bar{c}_{i}|$ and $|\bar{c}_{j}|$, respectively, and applying the triangle inequality, we obtain:
\begin{align*}
(|\bar{c}_{i}| + |\bar{c}_{j}|) \varepsilon
&\geq\|\mathbf{A}(\bar{c}_{i}c_{j} \mathbf{e}_{j} - \bar{c}_{j}c_{i}\mathbf{e}_{i})\|_2 \nonumber \\ 
&\geq  \left( |\bar{c}_{i}| + |\bar{c}_{j}| \right) L_2(\mathbf{A}) \min_{\ell \in [m]} |c_\ell |,
\end{align*}
which contradicts the bound $\varepsilon < L_2(\mathbf{A})/C_1$. Hence, the map $\pi$ is injective and therefore $\bar m \geq m$. Setting $\bar J = \pi([m])$ and letting $P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}\right)$ and $D = \text{diag}(\frac{\bar{c}_1}{c_1},\ldots,\frac{\bar{c}_m}{c_m})$, we see that \eqref{1D} becomes, for all $j \in [m]$:
\begin{align*}
\|(\mathbf{A} - \mathbf{B}_{\bar J}\mathbf{PD})_j\|_2 
= \|\mathbf{A}_j - \frac{\bar{c}_j}{c_j}\mathbf{B}_{\pi(j)}\|_2 
\leq \frac{\varepsilon}{|c_j|} 
\leq C_1\varepsilon.
\end{align*}
\end{proof}

We require a few additional tools to extend the proof to the general case $k < m$. These include a generalized notion of distance (Def.~\ref{dDef}) and angle (Def.~\ref{FriedrichsDefinition}) between subspaces as well as a stability result in combinatorial matrix analysis (Lem.~\ref{MainLemma}).

\begin{definition}\label{dDef}
For $\mathbf{u} \in \mathbb R^m$ and vector spaces $U,V \subseteq \mathbb{R}^m$, let $\text{\rm dist}(\mathbf{u}, V) := \inf \{\| \mathbf{u}-\mathbf{v} \|_2: \mathbf{v} \in V\}$ and define:
\begin{align}\label{d}
d(U,V) := \sup_{\mathbf{u} \in U, \ \|\mathbf{u}\|_2 \leq 1} \text{\rm dist}(\mathbf{u},V).
\end{align}
\end{definition}

We note the following facts about $d$. For subspaces $U \subseteq U', V \subseteq \mathbb{R}^m$, we have $d(U,V) \leq d(U',V)$ and \cite[Cor.~2.6]{Kato2013}:
\begin{align}\label{dimLem}
d(U,V) < 1 \implies \dim(U) \leq \dim(V).
\end{align}
Also, from \cite[Lem.~3.2]{Morris10}, we have:
\begin{align}\label{eqdim}
\dim(U) = \dim(V) \implies d(U,V) = d(V,U).
\end{align}

Our result in combinatorial matrix analysis is the following.

\begin{lemma}\label{MainLemma}
Suppose $\mathbf{A} \in \mathbb{R}^{n \times m}$ has $L_E(\mathbf{A}) > 0$ for some $r$-regular $E \subseteq {[m] \choose k}$ with the SIP. There exists $C_2 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_2$:

If for some  $\mathbf{B} \in \mathbb{R}^{n \times \bar m}$ and map $\pi: E \mapsto {[\bar m] \choose k}$ we have:
\begin{align}\label{GapUpperBound}
d(\text{\rm Span}\{\mathbf{A}_{S}\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \leq \varepsilon, \ \  \text{for $S \in E$},
\end{align}
then $\bar m \geq m$, and provided $\bar m < mr /(r-1)$, there exists a permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ such that:
\begin{align}\label{MainLemmaBPD}
\|(\mathbf{A}_J)_j - \mathbf{BPD}_j\|_2 \leq C_2 \varepsilon, \ \  \text{for } j \in J,
\end{align}
for some non-empty $J$ of size $\bar m - r(\bar m - m)$.
\end{lemma}

%\begin{lemma}\label{MainLemma}
%Fix $\mathbf{A} \in \mathbb{R}^{n \times m}$ with $L_E(\mathbf{A}) > 0$ for some regular $E \subseteq {[m] \choose k}$ with the SIP. There exists a constant $C_2 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_2$:

%If a matrix $\mathbf{B} \in \mathbb{R}^{n \times m}$ and map $\pi: E \mapsto {m \choose k}$ satisfy:
%\begin{align}\label{GapUpperBound}
%d(\text{\rm Span}\{\mathbf{A}_{S}\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \leq \varepsilon, \ \ \text{for $S \in E$},
%\end{align}
%then there exist a permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ such that:
%\begin{align}\label{MainLemmaBPD}
%\|\mathbf{A}_j - \mathbf{BPD}_j\|_2 \leq C_2 \varepsilon, \ \  \text{for } j \in [m],
%\end{align}
%\end{lemma}

The constant $C_1 > 0$ in Thm.~\ref{DeterministicUniquenessTheorem} is then defined by\footnote{Note that $\|\mathbf{AX}_{I(S)}\mathbf{c}\|_2 \geq \sqrt{k} L_k(\mathbf{A})\|\mathbf{X}_{I(S)}\mathbf{c}\|_2 \geq k L_k(\mathbf{A}) L_k(\mathbf{X}_{I(S)})\|\mathbf{c}\|_2$ for $S \in E$ and $k$-sparse $\mathbf{c}$. Therefore, $\sqrt{k} L_k(\mathbf{AX}_{I(S)}) \geq k L_k(\mathbf{A}) L_k(\mathbf{X}_{I(S)}) > 0$ since $L_k(\mathbf{A}), L_k(\mathbf{X}_{I(S)}) > 0$ by \eqref{SparkCondition} and general linear position of the $\mathbf{x}_i$.  Thus, $C_1 > 0$.}:
\begin{align}\label{Cdef1}
C_1(\mathbf{A}, \{\mathbf{x}_i\}_{i=1}^N, E) := \frac{ C_2(\mathbf{A}, E) } { \min_{S \in E} L_k(\mathbf{AX}_{I(S)}) }.
\end{align}
where, given vectors $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$, we denote by $\mathbf{X}$ the $m \times N$ matrix with columns $\mathbf{x}_i$ and by $I(S)$ the set of indices $i$ for which the support of $\mathbf{x}_i$ is contained in $S$.

The constant $C_2 = C_2(\mathbf{A}, E)$ is given in turn below, in terms of one used in \cite{Deutsch12} to analyze the convergence of the alternating projections algorithm for projecting a point onto an intersection of subspaces. 
We use it to bound the distance between a point and the intersection of subspaces given an upper bound on its distance from each individual subspace.

\begin{definition}\label{SpecialSupportSet}\label{FriedrichsDefinition}
For subspaces $V_1, \ldots, V_\ell \subseteq \mathbb{R}^m$, set $r := 1$ when $\ell = 1$ and define for $\ell \geq 2$:
\begin{align*}
r(\{V_i\}_{i=1}^\ell) := 1 - \left(1 -  \max \prod_{i=1}^{\ell-1} \sin^2  \theta \left(V_i, \cap_{j>i} V_j \right)  \right)^{1/2},
\end{align*} 
%
where the maximum is taken over all orderings\footnote{We modify the quantity in \cite{Deutsch12} in this way since the subspace ordering is irrelevant to our purpose.} of the $V_i$ and the angle $\theta \in (0,\frac{\pi}{2}]$ is defined implicitly as \cite[Def.~9.4]{Deutsch12}:
\begin{align*}
\cos{\theta(U,W)} := \max\left\{ |\langle \mathbf{u}, \mathbf{w} \rangle|: \substack{ \mathbf{u} \in U \cap (U \cap W)^\perp, \ \|\mathbf{u}\|_2 \leq 1 \\ \mathbf{w} \in W \cap (U \cap W)^\perp, \  \|\mathbf{w}\|_2 \leq 1 } \right\}.
\end{align*}
\end{definition}
Note that $\theta \in (0,\frac{\pi}{2}]$ implies $0 < r \leq 1$.\footnote{We acknowledge the counter-intuitive property that $\theta =  \pi/2$ when $U = V$ or $U \perp V$.}  
The constant $C_2$ in Lem.~\ref{MainLemma} can then be expressed as:  
\begin{align}\label{Cdef2}
C_2(\mathbf{A}, E) := \frac{ 2^{|E|} \max_{j \in [m]} \|\mathbf{A}_j\|_2}{ \min_{F \subseteq E} r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F}) },
\end{align}
%
which we remark is consistent with the assumption on $C_1$ in the proof of the case $k=1$ at the beginning of this section.

\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessCorollary} for $k < m$] 
We shall show that for every $S \in E$ there is some $\bar S \in {[\bar m] \choose k}$ for which the distance $d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\bar S}\})$ is controlled by $\varepsilon$. Applying Lem.~\ref{MainLemma} with the map $\pi$ defined by $S \mapsto \bar S$ then completes the proof.  

Since there are more than $(k-1){\bar m \choose k}$ vectors $\mathbf{x}_i$ supported on $S$, by the pigeonhole principle there must be some $\bar S \in {[\bar m] \choose k}$ and set of $k$ indices $K \subseteq I(S)$ such that the supports of all $\mathbf{\bar x}_i$ with $i \in K$ are subsets of $\bar S$.

It follows from the general linear position of the $\mathbf{x}_i$ and the linear independence of the columns of $\mathbf{A}_S$ that $L(\mathbf{AX}_{K}) > 0$; that is, the columns of the $n \times k$ matrix $\mathbf{AX}_K$ form a basis for $\text{\rm Span}\{\mathbf{A}_{S}\}$. Fixing $\mathbf{0} \neq \mathbf{y} \in \text{\rm Span}\{\mathbf{A}_{S}\}$, there then exists $\mathbf{0} \neq \mathbf{c} = (c_1, \ldots, c_k) \in \mathbb{R}^k$ such that $\mathbf{y} = \mathbf{AX}_K\mathbf{c}$. Setting \mbox{$\mathbf{\bar{y}} = \mathbf{B\bar{X}}_K\mathbf{c} \in \text{\rm Span}\{\mathbf{B}_{\bar S}\}$}, we have:
\begin{align*}
\|\mathbf{y} - \mathbf{\bar{y}}\|_2 
&= \|\sum_{i=1}^k c_i(\mathbf{AX}_K - \mathbf{B\bar{X}}_K)_i\|_2
\leq \varepsilon \sum_{i=1}^k |c_i| \\
&\leq \varepsilon \sqrt{k}  \|\mathbf{c}\|_2 
\leq \frac{\varepsilon}{L(\mathbf{AX}_K)} \|\mathbf{y}\|_2,
\end{align*}
where the last inequality follows directly from the definition of $L$. From Def.~\ref{dDef} we have:
\begin{align}\label{rhs222}
d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\bar S}\}) 
\leq \frac{\varepsilon}{  L(\mathbf{AX}_{K}) } \leq \varepsilon \frac{C_1}{C_2},
\end{align}
%
where the second inequality follows from $L(\mathbf{AX}_{K}) \geq L_k(\mathbf{AX}_{I(S)})$ and \eqref{Cdef1}. Since $\varepsilon < L_2(\mathbf{A})/C_1$, the result follows by Lem.~\ref{MainLemma}, .
\end{proof}

\begin{proof}[Proof of Cor.~\ref{SLCopt}]
We bound the number of $k$-sparse $\mathbf{\bar x}_i$ and then apply Thm.~\ref{DeterministicUniquenessCorollary}. 
Let $n_p$ be the number of $\mathbf{\bar x}_i$ with $\|\mathbf{\bar x}_i\|_0 = p$.
Since the $\mathbf{x}_i$ are all $k$-sparse, by \eqref{minsum} we have:
\mbox{$k \sum_{p = 0}^{\bar m} n_p \geq \sum_{i=0}^N \|\mathbf{x}_i\|_0 \geq \sum_{i=0}^N \|\mathbf{\bar x}_i\|_0 = \sum_{p=0}^{\bar m} p n_p.$}
Hence,
\begin{align}\label{eqn}
\sum_{p = k+1}^{\bar m} n_p \leq \sum_{p = k+1}^{\bar m} (p-k) n_p \leq \sum_{p = 0}^k (k-p)n_p \leq k \sum_{p = 0}^{k-1} n_p.
\end{align}

We now show that no more than $(k-1)|E|$ of the $\mathbf{\bar x}_i$ share a support of size less than $k$. 
From previous arguments, for every $S \in E$, the columns of each $n \times k$ submatrix of $\mathbf{AX}_{I(S)}$ form a basis for $\text{\rm Span}\{\mathbf{A}_S\}$.
Suppose that more than $(k-1)|E|$ vectors $\mathbf{\bar x}_i$ share a support $\bar S$ of size $|\bar{S}| < k$. By the pigeonhole principle, there is some $S \in E$ supporting $k$ or more of the corresponding $\mathbf{x}_i$; let them be indexed by $K \subseteq I(S)$.  By the same argument as in the proof of Thm.\ref{DeterministicUniquenessTheorem}, we also have \eqref{rhs222}. Our bound on $\varepsilon$ implies the right-hand side of \eqref{rhs222} is less than one; hence, by \eqref{dimLem} we have the contradiction: 
\[k = \dim(\text{Span}(\mathbf{A}_S)) \leq \dim(\text{Span}\{\mathbf{B}_{\bar S}\}) \leq |\bar S|.\]

The total number of $(k-1)$-sparse vectors $\mathbf{\bar x}_i$ can thus no greater than $|E|(k-1){ \bar m \choose k-1}$. Taking \eqref{eqn} into account, no more than $|E|k(k-1){ \bar m \choose k-1}$ vectors $\mathbf{\bar x}_i$ are \emph{not} $k$-sparse. Since for every $S \in E$ there are over $(k-1)\left[ {\bar m \choose k} + |E|k{ \bar m \choose k-1} \right]$ vectors $\mathbf{x}_i$ supported there, it follows that more than $(k-1){\bar m \choose k}$ of them must have corresponding $\mathbf{\bar x}_i$ that are also $k$-sparse. The result now follows directly from Thm.~\ref{DeterministicUniquenessCorollary}.
\end{proof}

\section{Discussion}\label{Discussion}
[ TODO ]
%[mention dictionary size independence]
%In this note, we generalized the approach of \cite{Hillar15} to prove the stability of unique solutions to Probs.~\ref{InverseProblem} and \ref{OptimizationProblem} while significantly reducing the known sample complexity. Our results justify the application of the sparse linear coding model to blind source separation problems, wherein the goal is to infer the generating dictionary and sparse codes from noisy measurements, \eqref{LinearModel}. We also collect a set of useful mathematical tools and basic facts for future research. The main motivation for this work, however, was to understand how seemingly universal representations emerge from sparse coding models fit to natural data by a variety of methodologies. We elaborate on these applications below after discussing theoretical aspects. 

%What we have shown here is that the sparse linear coding model generally produces a \textit{well-posed} inverse problem to be approximately solved by a numerical algorithm. This early concept of Hadamard \cite{Hadamard1902} can be paraphrased as the idea that inferences from observations should be robust to the inevitable uncertainty in measurement. In other words, a small perturbation of the data should result in only slightly different inferred parameters. In this regard, for the sparse linear coding model we demonstrate a linear relationship, \eqref{epsdel}, between measurement noise and the deviation of any solution from the true parameters, with explicit constants expressed in terms of these parameters. Moreover, we show that even if the meta-parameter for the number of dictionary elements is overestimated, a subset of parameters may still be identifiable up to noise. It would therefore be of practical utility to determine the best possible dependence of $\varepsilon$ on $\delta_1, \delta_2$ in Def.\ref{maindef} as well as the minimal requirements on the number and diversity of generating codes, and we hope that other researchers continue to improve and extend our results. We remark that our constants have been derived for deterministic ``worst-case'' noise, whereas the ``effective'' noise might be smaller when sampled from a distribution; in such cases, the constants will improve as well.

%One notable component of our contribution is a combinatorial criteria (regular hypergraphs satisfying the singleton intersection property, Def.~\ref{sip}) for the support sets of sparse codes key to the identification of the dictionary. Fully understanding those combinatorial designs allowing for stable sparse representations is an interesting research area for the future. For instance, whether there is a support set size $N$ that is polynomial in $m,k$ gauranteeing the conclusions of Thm.~\ref{DeterministicUniquenessTheorem} has implications for the computational complexity of Probs.~\ref{InverseProblem} and \ref{OptimizationProblem} and related questions \cite{Tillmann15}. 

%A technical difficulty in proving Thm.~\ref{DeterministicUniquenessTheorem} was the absence of a spark condition assumption on solutions to Prob.~\ref{InverseProblem}. Although mathematically interesting that no such requirement is necessary, there are other reasons to seek out such a theoretical guarantee. For instance, it is difficult to ensure that an algorithm maintain a dictionary satisfying \eqref{SparkCondition} at each iteration; indeed, even certifying a dictionary has this property is likely intractable given its NP-hardness \cite{tillmann2014computational}.

%In fact, uniqueness guarantees with minimal assumptions apply to all areas of data science and engineering that utilize learned sparse representations. For example, several groups have applied compressive sensing to signal processing tasks: MRI analysis \cite{lustig2008compressed}, image compression \cite{Duarte08}, and, more recently, the design of an ultrafast camera \cite{Gao14}. Given such effective uses of compressive sensing, it is only a matter of time before these systems incorporate dictionary learning to encode and process data (e.g., in a device that learns structure from motion \cite{kong2016prior}). In these cases, assurances such as those offered by our theorems certify that different devices (with different initialization, samples, etc.) will learn equivalent representations given enough data from statistically identical systems.

%In the field of theoretical neuroscience in particular, dictionary learning for sparse coding and related methods have recovered characteristic components of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent} and sounds \cite{bellsejnowski1996, smithlewicki2006, Carlson12} that reproduce response properties of cortical neurons. Our results suggest that this correspondence could be due to the ``universality'' of sparse representations in natural data, an early mathematical idea in neural theory \cite{pitts1947}. Furthermore, they justify the hypothesis of \cite{Coulter10, Isely10} that sparse codes passed through information bottlenecks in the brain are recovered from random projections via (unsupervised) biologically plausible sparse coding (e.g., \cite{rehnsommer2007, rozell2007neurally, hu2014hebbian}).

\acknow{We thank Friedrich Sommer and Darren Rhea for early thoughts, and Ian Morris for posting \eqref{eqdim} online.}
\showacknow % Display the acknowledgments section

\bibliography{chazthm_pnas}

\pagebreak

\section{Appendix}\label{proofs}

We prove Lem.~\ref{MainLemma} after stating auxiliary lemmas and then sketch the proofs of Thm.~\ref{robustPolythm} and Cor.~\ref{ProbabilisticCor}.

%\begin{lemma}\label{SpanIntersectionLemma}
%Let $\mathbf{M} \in \mathbb{R}^{n \times m}$. If every $2k$ columns of $\mathbf{M}$ are linearly independent, then for any $F \subseteq {[m] \choose k}$:
%\begin{align*}
%\text{\rm Span}\{\mathbf{M}_{\cap F}\}  = \bigcap_{S \in F} \text{\rm Span}\{\mathbf{M}_S\}.
%\end{align*}
%\end{lemma}
%\begin{proof}
%Using induction on $|F|$, it is enough to prove the lemma when $|F| = 2$; but this case follows directly from the assumption.
%\end{proof}

\begin{lemma}\label{SpanIntersectionLemma}
If $f: U \to V$ is an injective function then:
\begin{align*}
f\left(\cap_{i=1}^\ell U_i \right) =  \cap_{i=1}^\ell f\left(U_i\right) \ \ \text{for any } U_1, \ldots, U_\ell \subseteq U.
\end{align*}
%In particular, if for some $E \in 2^{[m]}$ the map $M \in \mathbb{R}^{n \times m}$ is injective on $\cup_{S \in E} \text{Span}\{e_i\}_{i \in S}$ then $\text{Span}\{ M_{\cap_{S \in E} S} \} = \cap_{S \in E} \text{Span}\{M_S\}$.
\end{lemma}
\begin{proof}
By induction it is enough to prove the case $|n| = 2$, but this case follows directly from the assumtion. %Clearly, for any map $f$, if $w \in f(U \cap V)$ then $w \in f(U)$ and $w \in f(V)$, hence $w \in f(U) \cap f(V)$. If $w \in f(U) \cap f(V)$ then $w \in f(U)$ and $w \in f(V)$, hence $w = f(u) = f(v)$ for some $u \in U$ and $v \in V$, implying $u = v$ by injectivity of $f$. Hence $u \in U \cap V$, and $w \in f(U \cap V)$.
\end{proof}

\begin{lemma}\label{DistanceToIntersectionLemma}
Fix $k \geq 2$. Let $V_1, \ldots, V_k$ be subspaces of $\mathbb{R}^m$ and set $V = \cap_{i = 1}^k V_i$. For  $\mathbf{x} \in \mathbb{R}^m$, we have (where $r$ is given in Def.~\ref{SpecialSupportSet}):
\begin{align}\label{DTILeq}
\text{\rm dist}(\mathbf{x}, V) \leq \frac{1}{r(\{V_i\}_{i = 1}^k)} \sum_{i=1}^k \text{\rm dist}(\mathbf{x}, V_i).
\end{align}
\end{lemma}
\begin{proof} 
Recall the orthogonal projection onto a subspace $V \subseteq \mathbb{R}^m$ is the mapping $\Pi_V: \mathbb{R}^m \to V$ that associates with each $\mathbf{x}$ its unique nearest point in $V$; i.e., $\|\mathbf{x} - \Pi_V\mathbf{x}\|_2 = \text{\rm dist}(\mathbf{x}, V)$.
Next, observe:
\begin{align}\label{f}
\|\mathbf{x} - \Pi_V\mathbf{x}\|_2 &\leq \|\mathbf{x} - \Pi_{V_k} \mathbf{x}\|_2 + \|\Pi_{V_k}  \mathbf{x} - \Pi_{V_k}\Pi_{V_{k-1}}\mathbf{x}\|_2 \nonumber \\
&\ \ \ + \cdots + \|\Pi_{V_k} \Pi_{V_{k-1}}\cdots \Pi_{V_1} \mathbf{x} - \Pi_V \mathbf{x}\|_2 \nonumber \\
&\leq \|\Pi_{V_k}\cdots\Pi_{V_{1}} \mathbf{x} - \Pi_V \mathbf{x}\|_2 + \sum_{\ell=1}^k \|\mathbf{x} - \Pi_{V_{\ell}} \mathbf{x}\|_2,
\end{align}
using the triangle inequality and that the spectral norm satisfies $\|\Pi_{V_{\ell}}\|_2 \leq 1$ for all $\ell$ (since $\Pi_{V_{\ell}}$ are orthogonal projections).

The desired result, \eqref{DTILeq}, now follows by bounding the second term on the right-hand side using the following fact \cite[Thm.~9.33]{Deutsch12}:
\begin{align}
\|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} \mathbf{x} - \Pi_V\mathbf{x}\|_2 \leq z \|\mathbf{x}\|_2,
\end{align}
for \mbox{$z^2= 1 - \prod_{\ell =1}^{k-1}(1-z_{\ell}^2)$} and \mbox{$z_{\ell} = \cos\theta\left(V_{\ell}, \cap_{s=\ell+1}^k V_s\right)$}, recalling $\theta$ from Def.~\ref{FriedrichsDefinition}. Together with $\Pi_{V_\ell} \Pi_V = \Pi_V$ for all $\ell \in [k]$ and $\Pi_V^2 = \Pi_V$, this yields:
\begin{align*}
\|\Pi_{V_k} \cdots \Pi_{V_1}\mathbf{x}  - \Pi_V \mathbf{x} \|_2 
&= \|\left( \Pi_{V_k} \cdots\Pi_{V_1} - \Pi_V \right) (\mathbf{x} - \Pi_V\mathbf{x})\|_2 \\
&\leq z\|\mathbf{x} - \Pi_V\mathbf{x}\|_2.
\end{align*}

Finally, substituting this into \eqref{f} and rearranging produces \eqref{DTILeq} after replacing $1 - z$ with $r(\{V_i\}_{i=1}^k)$.
\end{proof}

\begin{lemma}\label{NonEmptyLemma}
Fix a hypergraph $E \subseteq 2^{[m]}$ with nodes labeled in order of non-increasing degree and for which $|\cap H(i)| = 1$ for all $i \leq q$. Fix $\bar m$ and let $p \leq q$ be the largest number satisfying:
\begin{align}\label{pcond}
\sum_{i=\ell}^{m} \deg(i) > (\bar m + 1 - \ell) (\deg(\ell) - 1) \ \ \text{for all } \ell \leq p.
\end{align}

If the map $\pi: E \to 2^{[\bar m]}$ has $\sum_{S \in E} (|\pi(S)| -|S|) \geq 0$ and:
\begin{align}\label{cond}
|\cap \pi(F)| \leq |\cap F | \text{ for all } F \subseteq E,
\end{align}
then $\bar m \geq\sum_i \deg(i) / \deg(1)$, and if $\bar m < \sum_i \deg(i) / (\deg(1) - 1)$ then the association $i \mapsto \cap \pi(F(i))$ defines an injective map to $[\bar m]$ from some $J \subseteq [m]$ of size $p \geq 1$ consisting of the union of the set of all nodes of degree exceeding $\deg(p)$ and some set of nodes all having degree equal to $\deg(p)$. In particular, if $E$ is $d$-regular then $p = \bar m - d(\bar m - m)$. 
\end{lemma}

\begin{proof}
Consider the collection of pairs: $T_1 := \{(i, S): i \in \pi(S), S \in E\}$, which number $|T_1| = \sum_{S \in E} |\pi(S)| \geq \sum_{S \in E} |S| = \sum_{i \in [m]} \deg(i)$. Note that assumption \eqref{cond} implies $\bar m \geq |T_1| / \deg(1)$, since otherwise pigeonholing the elements of $T_1$ with respect to their set of possible first indices $[\bar m]$ would lead us to conclude that there are more than $\deg(1)$ sets in $E$ sharing a common element. %QUESTION: Do we really need to say this..?

By \eqref{pcond} and the upper bound on $\bar m$ we have $|T_1| >  \bar m (\deg(1) - 1)$, which implies, again by the pigeonhole principle, that there must be at least $\deg(1)$ elements of $T_1$ sharing the same first index. By \eqref{cond}, the intersection of the set $F_1$ consisting of their second indices is non-empty. As $p \leq q$ and $\deg(1) \geq \deg(i)$ for all $i$, it must be that $\cap F_1| = 1$. Since $\cap \pi(F_1)$ is non-empty, applying \eqref{cond} again implies $\cap \pi(F_1) = \{i_1\}$ for some $i_1 \in [m]$. If $p=1$ then we are done. Otherwise, define $T_2 := T_1 \setminus \{(i,S) \in T_1: i = i_1\}$, which contains $|T_2| = |T_1| - \deg(1) = \sum_{i=2}^m \deg(i)$ ordered pairs having $\bar m - 1$ distinct first indices. By \eqref{pcond} we have $|T_2| > (\bar m - 1)(\deg(2) - 1)$ and reiterating the above arguments produces a (necessarily) distinct index $i_2$. Iterating the arguments $p$ times yields the set of singletons \mbox{$J = \{\cap F_1, \ldots, \cap F_p\} \subseteq [m]$}.

%When $E$ is $r$-regular, equation \eqref{pcond} becomes $(m+1-\ell)r > (\bar m +1 - \ell)(r-1)$ which simplifies to $\ell \leq \bar m - (\bar m - m)r$; hence $p = \bar m - (\bar m - m)r$, which is positive by our upper bound on $\bar m$.
\end{proof}

\begin{proof}[Proof of Lem.~\ref{MainLemma}]
We begin by showing that: 
\begin{align}\label{eq2}
d(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}, \text{\rm Span}\{\mathbf{A}_S\} ) = d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}).
\end{align}
Since the right-hand side of \eqref{GapUpperBound} is less than one (by \eqref{delrho} and $r \leq 1$), it follows from \eqref{dimLem} that $|\pi(S)| \geq \dim(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \geq \dim(\text{\rm Span}\{\mathbf{A}_{S}\}) = |S|$, with the equality by injectivity of $\mathbf{A}$. Since $|S| = |\pi(S)|$, we in fact have $\dim(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) = \dim(\text{\rm Span}\{\mathbf{A}_{S}\})$, and \eqref{eq2} follows using \eqref{eqdim}. Note that the columns of $\mathbf{B}_{\pi(S)}$ are therefore linearly independent, for all $S \in E$.

We next show that \eqref{cond} holds.  Fix $F \subseteq E$. Since $\text{\rm Span}\{\mathbf{B}_{\cap \pi(F)}\} \subseteq \cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}$, if $\cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\} = \{\textbf{0}\}$, then we must have $|\cap_{S \in F} \pi(S)| = 0$ (as the columns of $\mathbf{B}_{\pi(S)}$ are linearly independent) and \eqref{cond} is trivially true. Suppose then that the intersection is not the zero vector. By Lem.~\ref{SpanIntersectionLemma} and Lem.~\ref{DistanceToIntersectionLemma}, and then incorporating \eqref{eq2} and \eqref{GapUpperBound}, we have:
\begin{align}\label{randoml}
d( \text{\rm Span}&\{\mathbf{B}_{\cap \pi(F)}\}, \text{\rm Span}\{\mathbf{A}_{\cap F}\}  ) \nonumber \\
&\leq d\left( \cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}, \cap_{S \in F} \text{\rm Span}\{\mathbf{A}_{S}\} \right) \nonumber \\
&\leq \sum_{T \in F} \frac{ d\left( \cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\},\text{\rm Span}\{\mathbf{A}_{T}\} \right) }{ r(  \text{\rm Span}\{\mathbf{A}_{S}\}_{S \in F}) } \nonumber \\
&\leq \sum_{T \in F} \frac{ d\left( \text{\rm Span}\{\mathbf{B}_{\pi(T)}\},\text{\rm Span}\{\mathbf{A}_{T}\} \right) }{ r( \text{\rm Span}\{\mathbf{A}_{S}\}_{S \in F}) }\nonumber \\
&\leq \frac{|F| \varepsilon}{r( \text{\rm Span}\{\mathbf{A}_{S}\}_{S \in F})} 
\leq \frac{C_2 \varepsilon}{\max_i\|\mathbf{A}_i\|_2}. 
\end{align}
%
where the third inequality follows from the definition of $d$ as a supremum. Since $\varepsilon < L_2(\mathbf{A}) / C_2$, by \eqref{delrho} the right-hand side in \eqref{randoml} is strictly less than one. Hence, $\dim(\text{\rm Span}\{\mathbf{B}_{\cap \pi(F)}\}) \leq \dim(\text{\rm Span}\{\mathbf{A}_{\cap F}\})$ by \eqref{dimLem}, and \eqref{cond} follows from the linear independence of the columns of $\mathbf{A}_{S}$ and $\mathbf{B}_{\pi(S)}$ for all $S \in F$.

Now, by Lem.~\ref{NonEmptyLemma}, the association $i \mapsto \cap \pi(F(i))$ defines an injective map $\bar \pi: J \to [\bar m]$ for some $J \in {[m] \choose p}$ with $p$ given by \eqref{pcond}, and we can be sure that $\mathbf{B}_{\bar \pi(i)} \neq \mathbf{0}$ for all $i \in J$ since the columns of $\mathbf{B}_{\pi(S)}$ are linearly independent for all $S \in E$. It then follows from \eqref{eqdim} and \eqref{randoml} that $d\left( \text{\rm Span}\{\mathbf{A}_i\}, \text{\rm Span}\{ \mathbf{B}_{\bar \pi(i)} \} \right) \leq C_2 \varepsilon / \max_i \|\mathbf{A}_i\|_2$ for all $i \in J$. Fixing $\bar \varepsilon = C_2\varepsilon$ and letting $c_i = \|\mathbf{A}_i\|_2^{-1}$, we thus have that for every basis vector $\mathbf{e}_i \in \mathbb{R}^m$ with $i \in J$ there exists some $\bar{c}_i \in \mathbb{R}$ such that $\|c_i\mathbf{A}\mathbf{e}_i - \bar{c}_i \mathbf{B}\mathbf{e}_{\bar \pi(i)}\|_2 \leq \bar \varepsilon < L_2(\mathbf{A}) \min_{i\in J} |c_i|$.  But this is exactly the supposition in \eqref{1D}, and the result follows from the case $k=1$ in Sec.~\ref{DUT} applied to the submatrix $\mathbf{A}_J$.
\end{proof}

%\section{Proofs of Thm.~\ref{robustPolythm} and Cor.~\ref{ProbabilisticCor}}

\begin{proof}[Proof (sketch) of Thm.~\ref{robustPolythm}]
Let $M$ be the matrix with columns $\mathbf{A}\mathbf{x}_i$, $i \in [N]$.  Consider the polynomial in the entries of $\mathbf{A}$ and $\mathbf{x}_i$:
\begin{align*}
g(\mathbf{A}, \{\mathbf{x}_i\}_{i=1}^N) := \prod_{S \in {[N] \choose k}} \sum_{S' \in {[n] \choose k}} (\det M_{S',S})^2,
\end{align*}
with notation as in Sec.~\ref{Results}.  
It can be checked that when $g$ is nonzero for a substitution of real numbers for the indeterminates, all of the genericity requirements on $\mathbf{A}$ and $\mathbf{x}_i$ in our proofs of stability in Thm.~\ref{DeterministicUniquenessTheorem} are satisfied (in particular, the spark condition on $\mathbf{A}$).
\end{proof}

\begin{proof}[Proof (sketch) of Cor.~\ref{ProbabilisticCor}]
First, note that if a set of measure spaces $\{(X_{\ell}, \Sigma_{\ell}, \nu_{\ell})\}_{\ell=1}^p$ has that $\nu_{\ell}$ is absolutely continuous with respect to $\mu$ for all $\ell \in [p]$, where $\mu$ is the standard Borel measure on $\mathbb{R}$, then the product measure $\prod_{\ell=1}^p \nu_{\ell}$ is absolutely continuous with respect to the standard Borel product measure on $\mathbb{R}^p$ (e.g.,  \cite{folland2013real}). By Thm.~\ref{robustPolythm}, there is a polynomial that is nonzero whenever $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$; in particular, this property (stability) holds with probability one.
\end{proof}

%\pnasbreak


\end{document}