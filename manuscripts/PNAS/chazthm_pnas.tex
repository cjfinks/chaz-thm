\documentclass[9pt,twocolumn]{pnas-new}
% Use the lineno option to display guide line numbers if required.
% Note that the use of elements such as single-column equations
% may affect the guide line number alignment. 

%%% CHAZ TODO %%%

%%% CHRIS TODO %%
% steamroll

% IMPORTED PACKAGES
\usepackage{amsmath, amssymb, amsthm} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

%\usepackage[pdftex]{graphicx}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}
%\usepackage{cite}

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} = Template for a one-column mathematics article
% {pnasinvited} = Template for a PNAS invited submission

\title{On the uniqueness and stability of dictionaries for sparse representation of noisy signals}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,1]{Charles J. Garfinkle}
\author[a,1]{Christopher J. Hillar} 

\affil[a]{Redwood Center for Theoretical Neuroscience, Berkeley, CA, USA}

% Please give the surname of the lead author for the running footer
\leadauthor{Garfinkle} 

% Please add here a significance statement to explain the relevance of your work
%\significancestatement{Authors must submit a 120-word maximum statement about the significance of their research paper written at a level understandable to an undergraduate educated scientist outside their field of speciality. The primary goal of the Significance Statement is to explain the relevance of the work in broad context to a broad readership. The Significance Statement appears in the paper itself and is required for all research papers.}

\significancestatement{[*** REDO ***] Many naturally occurring signals (e.g. patches of visual scenery, speech audio, electrocardiograms) have been usefully characterized as parsimonious combinations of building blocks drawn from a large `dictionary' of elementary waveforms. Absent analytic descriptions of these signal classes, it usually falls to machine learning algorithms to infer a suitable dictionary given a large set of examples from the class. We give conditions guaranteeing when the optimal dictionary of a given size is uniquely and consistently determined by data, regardless of the algorithm used to compute it. Our results provide theoretical grounding for the application of dictionary learning algorithms to the problem of recovering sparse signals from unknown noisy compressive measurements.}

% Please include corresponding author, author contribution and author declaration information
\authordeclaration{The authors declare no conflict of interest.}
%\equalauthors{\textsuperscript{1}C.G. proved Theorem 1 and C.H. proved Theorem 2.}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: cjg@berkeley.edu, chillar@msri.org}

% Keywords are not mandatory, but authors are strongly encouraged to provide them. If provided, please include two to five keywords, separated by the pipe symbol, e.g:
\keywords{Sparse linear coding, dictionary learning, uniqueness, compressed sensing} 

\begin{abstract}
Dictionary learning for sparse linear coding has exposed underlying structure in many natural signals.
However, there are few universal theorems guaranteeing uniqueness of model estimation independent of implementation.
Here, we prove that for all diverse enough datasets generated from the sparse coding model, latent dictionaries and codes are uniquely determined up to measurement error.  Applications are given to data analysis, neuroscience, and engineering. 
\end{abstract}

% \dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

% Optional adjustment to line up main text (after abstract) of first page with line numbers, when using both lineno and twocolumn options.
% You should only change this length when you've finalised the article contents.
\verticaladjustment{-2pt}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.
\dropcap{B}lind source identification is a classical problem in signal processing \cite{sato1975method}.  
One popular modern formulation of the idea is to find a dictionary of $m$ elementary waveforms, at most $k$ of which need be linearly combined to represent each $n$-dimensional signal in a dataset of size $N$, typically where $k < m \ll N$.  Approximating solutions to this problem have
provided insight into the structure of many signals lacking explicit analytic structure (see \cite{Zhang15} for a comprehensive review). 
In particular, it has been shown that response properties of simple-cell neurons in mammalian visual cortex emerge from learning a dictionary for sparse linear coding of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent}, a major advance in computational neuroscience. A fundamental aspect of this finding is that the latent waveforms (``Gabors'') estimated from data appear to be canonical; i.e., they are found in learned dictionaries independent of algorithmic implementation or natural image training set.

% (e.g., Fourier bases, wavelets, etc.)

Motivated by these discoveries and earlier work in the theory of neural communication \cite{Coulter10, Isely10}, we address when dictionaries and sparse representations are uniquely determined by data.  Answers to this question also have other real-world implications.  For example, a sparse coding analysis of local painting style can be used for forgery detection \cite{hughes2010, Olshausen10}, but only if sparse components of the artwork are independent of implementation idiosyncrasies. Fortunately, algorithms with proven recovery of generating dictionaries under certain 
conditions have recently been proposed (see \cite[Sec.~I-E]{Sun16} for a summary of the state-of-the-art).  However, there are few universal theorems explaining the general phenomenon of uniqueness.  

Here, we prove very generally that uniqueness in sparse linear coding is an expected property of the model.  In other words, whenever enough sparsely represented data generated from a dictionary (satisfying a spark condition) are observed, the original codes and dictionary are uniquely determined up to an error that is commensurate with the measurement noise.  Applications of this result include:  (1) a sufficient diversity of sparse codes sent through a random linear compressive channel are unique, (2) the sparse linear coding problem is well-posed in the sense of Hadamard \cite{Hadamard1902}, and (3) an explanation for universal representation of natural signals in neuroscience. 

% It has remained unknown, however, when the dictionary learning problem is well-posed (as per Hadamard \cite{Hadamard1902}) in general -- that is, independent of all methodological idiosyncracies.
%[**TODO** make sure the claim here is right...or would it be punchier to talk about the fact that everyone always gets Gabors on natural images?]
%- algos with proven recovery of generating dictionary
%- analyses of when target dictionary is local optimizer of natural recovery criteria
%- efficient algos that globally solve DR; detailed models differ but all assume each col has bounded sparsity and nonzero coefs have sub-Gaussian magnitudes



More formally, let $\mathbf{A} \in \mathbb R^{n \times m}$ be a matrix with columns $\mathbf{A}_j$ ($j = 1,\ldots,m$) and let dataset $Z$ consist of measurements:
\begin{align}\label{LinearModel}
\mathbf{z}_i = \mathbf{A}\mathbf{x}_i + \mathbf{n}_i,\indent \text{$i=1,\ldots,N$},
\end{align}
for $k$-\emph{sparse} $\mathbf{x}_i \in \mathbb{R}^m$ having at most $k$ nonzero entries and \emph{noise} $\mathbf{n}_i \in \mathbb{R}^n$, with bounded norm $\| \mathbf{n}_i \|_2 \leq  \eta$ representing our combined worst-case uncertainty in  measuring $\mathbf{A}\mathbf{x}_i$.
The precise mathematical problem addressed here is the following.
% We address the uniqueness properties for the following linear coding question in signal processing.

\begin{problem}[Sparse linear coding]\label{InverseProblem}
Find a matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\hat x}_1, \ldots, \mathbf{\hat x}_N \in \mathbb{R}^m$ such that $\|\mathbf{z}_i - \mathbf{B}\mathbf{\hat x}_i\|_2 \leq \eta$ for all $i$.
\end{problem}

Note that any particular solution $(\mathbf{B}, \mathbf{\hat x}_1 \ldots, \mathbf{\hat x}_N)$ to this problem gives rise to an orbit of other solutions $(\mathbf{BPD}, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\hat x}_1, \ldots, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\hat x}_N)$, where $\mathbf{P}$ is any permutation matrix and $\mathbf{D}$ any invertible diagonal.  
We therefore consider whether solutions to Prob.~\ref{InverseProblem} are unique only up to this ambiguity.  More specifically, 
we seek conditions for when sparsely coded datasets have a stable unique representation.


% More specifically,  we seek sufficient conditions on when sparsely coded datasets have a stable unique representation.


% We therefore consider only whether solutions to Problem~\ref{InverseProblem} are unique up to this equivalence \cite{Li15}


% Note that by formulating the problem in this way we have introduced the following ambiguity. Any particular solution represents . Alternatively, one may seek a unique solution (in the usual sense) in the quotient space induced by the action of the transformation group of matrices $PD$.


%  noiseless case $\eta = 0$ 
 
 
%  We introduce the following terminology to handle $\eta > 0$. 
\begin{definition}\label{maindef}
Fix $Y = \{ \mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$. We say $Y$ has a \textbf{$k$-sparse linear representation in $m$ dimensions} if there exists a matrix $\mathbf{A}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ such that $\mathbf{y}_i = \mathbf{A}\mathbf{x}_i$ for all $i$. %Equivalently, $\mathbf{Y} = \mathbf{A}\mathbf{X}$ for $\mathbf{Y}$ and $\mathbf{X}$ having the $\mathbf{y}_i$ and $\mathbf{x}_i$ as their columns, respectively. 
This representation is \textbf{stable} if for every $\delta_1, \delta_2 \geq 0$, there exists $\varepsilon(\delta_1, \delta_2) \geq 0$ (with $\varepsilon > 0$ when  $\delta_1, \delta_2 > 0$) such that if $\mathbf{B}$ and $k$-sparse $\mathbf{\hat x}_1, \ldots, \mathbf{\hat x}_N \in \mathbb{R}^m$ satisfy:
\begin{align*}
\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\hat x}_i\|_2 \leq \varepsilon, \ \ \text{for all $i$},
\end{align*}
%
then there is some permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ such that for all $i, j$:
\begin{align}\label{def1}
\|\mathbf{A}_j - (\mathbf{BPD})_j\|_2 \leq \delta_1 \ \ \text{and} \ \ \|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\hat x}_i\|_1 \leq \delta_2.
\end{align}
\end{definition}
% Kato mentions a distance between n-tuples that minimizes the maximum difference over all possible orderings of one of the n-tuples.
% Make just 'sparse representation in m-dimensions? (i.e. with k unspecified?)

To see how Def. \ref{maindef} directly relates to Prob. \ref{InverseProblem}, suppose that $Y$
% \mbox{$Y = \{\mathbf{A} \mathbf{x}_1, \ldots, \mathbf{A}\mathbf{x}_N\}$} 
has a stable $k$-sparse linear representation and fix $\delta_1, \delta_2$ to be the desired recovery accuracy in \eqref{def1}. Consider now any dataset $Z$ generated as in \eqref{LinearModel} with $\eta \leq \frac{1}{2} \varepsilon(\delta_1, \delta_2)$. Then from the triangle inequality, it follows that any matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\hat x}_1, \ldots, \mathbf{\hat x}_N$ solving Prob.~\ref{InverseProblem} are necessarily close to the original dictionary $\mathbf{A}$ and codes $\mathbf{x}_i$. 

%Our results, outlined in Sec.~\ref{Results}, include an explicit form for $\varepsilon(\delta_1, \delta_2)$ in Thm.~\ref{DeterministicUniquenessTheorem}. 


% We ask here: \emph{When does $Y$ have a stable $k$-sparse representation?} 

Previous theoretical work on Prob.~\ref{InverseProblem} for the noiseless case $\eta =0$ (e.g., \cite{li2004analysis, Georgiev05, Aharon06, Hillar15}) has shown that a solution (when it exists) is indeed unique provided the $\mathbf{x}_i$ are sufficiently diverse and the generating matrix $\mathbf{A}$ satisfies the \textit{spark condition} from compressed sensing (CS):
\begin{align}\label{SparkCondition}
\mathbf{A}\mathbf{x}_1 = \mathbf{A}\mathbf{x}_2 \implies \mathbf{x}_1 = \mathbf{x}_2, \ \ \ \text{for all $k$-sparse } \mathbf{x}_1, \mathbf{x}_2,
\end{align}
%
which would be, in any case, necessary for uniqueness.

Our main technical finding is that a sufficient diversity of measurements generated from any dictionary satisfying the spark condition 
has a stable sparse representation.  More specifically, such dictionaries are uniquely identifiable from as few as \mbox{$N = m + m(k-1){m \choose k}$} sparse linear combinations of their columns up to an error that is linear in the measurement noise (Thm.~\ref{DeterministicUniquenessTheorem}). In fact, provided $(n, m, k)$ satisfy CS inequality \eqref{CScondition}, in almost all cases the dictionary learning problem is well-posed given enough data (Cor.~\ref{ProbabilisticCor}). Importantly, these guarantees hold without assuming the recovered matrix satisfies the spark condition. Our results  also apply when only an upper bound on the number of dictionary elements is known. The explicit, algorithm-independent criteria we provide should be a useful tool in the theory of sparse dictionary learning.  % (Thm.~\ref{DeterministicUniquenessTheorem2})

In the next section, we give precise statements of our main results, which include an explicit form for $\varepsilon(\delta_1, \delta_2)$. We then prove our main theorem (Thm.~\ref{DeterministicUniquenessTheorem}) in Sec.~\ref{DUT} after stating some additional definitions and lemmas required for the proof, including our main tool from combinatorial matrix analysis (Lem.~\ref{MainLemma}). All other proofs are relegated to the Supplemental. 
%Our approach is a refinement of the one found in \cite{Hillar15} to handle noise and to reduce the sufficient number of samples.
%for all $k < m$ from $N=k{m \choose k}^2$ to $N = m + m(k-1){m \choose k}$. 
Finally, we present several applications in Discussion Sec.~\ref{Discussion}.

\section{Results}

To prepare for the statements of our main results, we first describe a quantitative version of the spark condition.  % We first explain how the spark condition \eqref{SparkCondition} relates to t
The \emph{lower bound} \cite{Grcar10} of a matrix $\mathbf{A} \in \mathbb R^{n \times m}$ is the largest number $\alpha$ such that \mbox{$\|\mathbf{A}\mathbf{x}\|_2 \geq \alpha\|\mathbf{x}\|_2$} for all $\mathbf{x} \in \mathbb{R}^m$. By compactness of the unit sphere, every injective linear map has a nonzero lower bound; hence, if $\mathbf{A}$ satisfies \eqref{SparkCondition}, then each submatrix formed from $2k$ of its columns or less has a nonzero lower bound. We therefore define the following domain-restricted lower bound of $\mathbf{A}$:
\begin{align*}
L_k(\mathbf{A}) := \frac{1}{\sqrt{k}}\max \{ \alpha : \|\mathbf{A}\mathbf{x}\|_2 \geq \alpha\|\mathbf{x}\|_2 \text{ for all $k$-sparse } \mathbf{x}\}.
\end{align*} 
Clearly, $L_k(\mathbf{A}) \geq L_{k'}(\mathbf{A})$ whenever $k < k'$, and for any $\mathbf{A}$ satisfying the spark condition, we have $L_{k'}(\mathbf{A}) > 0$ for all $k' \leq 2k$. Note that the quantity $1 - \sqrt{k} L_k(\mathbf{A})$ [*** TODO: check is this correct ***] is known in the CS literature as the (asymmetric) lower restricted isometry constant \cite{Blanchard2011, Foucart2009}.

% an important contribution of our work is to
Next, we identify the combinatorial criteria on support sets of generating codes allowing for stable sparse 
representations.  Let $[m]$ denote the set $\{1, \ldots, m\}$, its power set $2^{[m]}$, and ${[m] \choose k}$ the set of subsets of $[m]$ of size $k$. Let $\text{\rm Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_\ell\}$ be the $\mathbb{R}$-linear span of vectors $\mathbf{v}_1, \ldots, \mathbf{v}_\ell$. A vector $\mathbf{x}$ is said to be \emph{supported} on $S \subseteq [m]$, written supp$(\mathbf{x}) = S$, when $\mathbf{x} \in \text{\rm Span} \{\mathbf{e}_j\}_{j\in S}$, where $\mathbf{e}_j$ are the standard basis elements in $\mathbb R^m$.  Let $\mathbf{M}_j$ be the $j$th column of a matrix $\mathbf{M}$ and similarly let $\mathbf{M}_S$ be the submatrix formed by the columns of $\mathbf{M}$ indexed by $S$ (we set $\text{\rm Span}\{\mathbf{M}_\emptyset\} := \{\textbf{0}\}$).  We say a hypergraph $E \subseteq 2^{[m]}$ has \textit{rank} $k$ when every $S \in E$ has cardinality $|S| \leq k$ (and \textit{$k$-uniform} when equality holds). We also say $E$ is \emph{regular} when every element of $[m]$ is contained in exactly $\ell$ elements of $E$ for some $\ell > 0$ (for given $\ell$, we say $E$ is $\ell$-regular).

\begin{definition}
Given a hypergraph $E \subseteq 2^{[m]}$, the \textbf{star} $F(i)$ centered at $i$ is the collection of $S \in E$ containing $i$. We say $E$ satisfies the \textbf{singleton intersection property} (\textbf{SIP}) when $\cap_{S \in F(i)} S = \{i\}$ for all $i \in [m]$. 
\end{definition}

\begin{proposition}
For every $k < m$, there is a regular $k$-uniform hypergraph that satisfies the singleton intersection property.
\end{proposition}
Consider, for instance, the set of consecutive intervals of length $k$ in some cyclic order on $[m]$.

We say that a set of $k$-sparse vectors is in \emph{general linear position} when any $k$ of them are linearly independent.
The following is the technical statement of our main result. 

\begin{theorem}\label{DeterministicUniquenessTheorem}
% Fix positive integers $k, \ell, m, m', n, p$ satisfying $k < m$ and $p \leq m \leq m' \leq (\ell m - p) / (\ell - 1)$. 
Fix integers $n, \ell$, and $k < m \leq \hat m$. Suppose $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ satisfies the spark condition \eqref{SparkCondition} and that $k$-sparse \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} contain at least \mbox{$(k-1){\hat m \choose k}+1$} vectors supported in general linear position on each set in some $\ell$-regular $E \subseteq {[m] \choose k}$ satisfying the SIP.  There exists a constant $C_1 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_1$.

Every matrix $\mathbf{B} \in \mathbb{R}^{n \times \hat m}$ and $k$-sparse $\mathbf{\hat x}_1, \ldots, \mathbf{\hat x}_N \in \mathbb{R}^{\hat m}$ with  \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\hat x}_i\|_2 \leq \varepsilon$} for all $i$ necessarily satisfies:
\begin{align}\label{Cstable}
\|\mathbf{A}_{\phi(j)} - (\mathbf{B}_{J}\mathbf{PD})_j\|_2 \leq C_1 \varepsilon, \ \ \text{for $j = 1, \ldots, |J|$},
\end{align}
%
for some $J \subseteq [\hat m]$ of size \mbox{$\lfloor \hat m - \ell(\hat m - m) \rfloor$}, injective map $\phi$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.
%\footnote{We note that the condition $\varepsilon < L_2(\mathbf{A})$ above is necessary. When \mbox{$\mathbf{A}$ = $I$} and $\mathbf{x}_i = \mathbf{e}_i$, it turns out that $C_1 = 1$ and there is a $\mathbf{B}$ and $1$-sparse $\mathbf{\hat x}_i$ with $\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\hat x}_i\|_2 \leq \varepsilon$ violating \eqref{Cstable}.}

Furthermore, if $\varepsilon < L_{2k}(\mathbf{A}) / C_1$, then $\mathbf{B}_J$ also satisfies \eqref{SparkCondition} with $L_{2k}(\mathbf{B}_J\mathbf{PD}) \geq L_{2k}(\mathbf{A}) - C_1 \varepsilon$, and:
\begin{align}\label{b-PDa}
\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\hat x}_i\|_1 &\leq  \left( \frac{ 1+C_1 \|\mathbf{x}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right) \varepsilon,
 \ \ \text{for all $i \in [N]$}.
\end{align}
\end{theorem}

In other words, the smaller the difference $\hat m - m$, the more columns and coefficients of the original $n \times m$ dictionary $\mathbf{A}$ and $m$-dimensional codes $\mathbf{x}_i$ contained (up to noise) in the appropriately scaled $n \times \hat m$ dictionary $\mathbf{B}$ and $\hat m$-dimensional codes $\mathbf{\hat x}_i$. In particular, when $\hat m = m$ all columns of $\mathbf{A}$ and coefficients of each $\mathbf{x}_i$ recoverable, as per Def.~\ref{maindef}. For expositional clarity, we delay defining the (explicit) constant $C_1 = C_1(\mathbf{A}, \{\mathbf{x}_i\}_{i=1}^N, E)$ until Section \ref{DUT} (\eqref{Cdef}).  % , since it requires additional definitions not critical to the statement of our results.

An important consequence of Thm.~\ref{DeterministicUniquenessTheorem} is that \eqref{def1} is guaranteed provided $\varepsilon$ does not exceed: [*** TODO: verify this is still correct ***]
\begin{align}\label{epsdel}
\varepsilon(\delta_1, \delta_2) := \min \left\{ \frac{\delta_1}{ C_1 }, \frac{ \delta_2 (L_{2k}(\mathbf{A}) -  C_1\varepsilon)}{ 1 + C_1 \left( \max_{i \in [N]} \|\mathbf{x}_i\|_1  + \delta_2 \right) } \right\}.
\end{align}

% Note also that Thm~\ref{DeterminsiticUniquenessTheorem} gives conditions for when a single solution to Problem~\ref{InverseProblem} in fact represents \emph{all} possible solutions, related by transformations of the form $PD$.

\begin{corollary}\label{DeterministicUniquenessCorollary}
Given $n, m$, $k < m$, and a regular hypergraph $E \subseteq {[m] \choose k}$ of rank $k$ with the SIP, there are $N =  |E| \left[ (k-1){m \choose k}+1 \right]$ vectors \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} such that every matrix $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} generates a dataset $Y = \{\mathbf{A}\mathbf{x}_1, \ldots, \mathbf{A}\mathbf{x}_N\}$ with a stable $k$-sparse representation in $m$ dimensions.
\end{corollary}
\begin{proof}
Something quick here.
\end{proof}

%\begin{proposition}
%If $E$ is an $\ell$-regular hypergraph of rank $k$ satisfying the singleton intersection property then $|E| \geq \ell m/k$. 
%\end{proposition}
Note that if $E$ is an $\ell$-regular hypergraph of rank $k$ satisfying the singleton intersection property then $|E| \geq \ell m/k$.
This is easy to see by noting that $|E|k \geq \sum_{S \in E}|S| = \sum_{i \in [m]} \deg(i) = \ell m$. In certain cases, there exist simple constructions achieving this minimum; for example, when $k = \sqrt{m}$ one can take $E$ to be the rows and columns formed by arranging the elements of $[m]$ in a square grid.

It is straightforward to provide a probabilistic extension of Thm.~\ref{DeterministicUniquenessTheorem} using the following fact in random matrix theory.  A random matrix $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfies \eqref{SparkCondition} with probability one (or with ``high probability'' for discrete variables) 
provided:
\begin{align}\label{CScondition}
n \geq \gamma k\log\left(\frac{m}{k}\right),
\end{align}
%
in which $\gamma$ is a positive constant that depends on the particular distribution from which the entries of $\mathbf{A}$ are sampled i.i.d. (many ensembles suffice, e.g. \cite[Sec.~4]{Baraniuk08}). 

We motivate our next theorem by the following observation:  the spark condition can be made explicit.  Let $\mathbf{A}$  be the $n \times m$ matrix of $nm$ indeterminates $A_{ij}$. When real numbers are substituted for $A_{ij}$, the resulting matrix satisfies \eqref{SparkCondition} if and only if the following polynomial is nonzero:
\begin{align*}
f(\mathbf{A}) := \prod_{S \in {[m] \choose k}} \sum_{S' \in {[n] \choose k}} (\det \mathbf{A}_{S',S})^2,
\end{align*}
%
where for any $S' \in {[n] \choose k}$ and $S \in {[m] \choose k}$, the symbol $\mathbf{A}_{S',S}$ denotes the submatrix of entries $A_{ij}$ with $(i,j) \in S' \times S$.   We note that the large number of terms in this product is likely necessary due to the NP-hardness of deciding whether a given matrix $A$ satisfies the spark condition \cite{tillmann2014computational}.

Since $f$ is a real analytic function, one substitution of real numbers with $f(\mathbf{A}) \neq 0$ implies that its zeroes form a set of (Borel) measure zero. Hence, almost every $n \times m$ real matrix $\mathbf{A}$ satisfies \eqref{SparkCondition} provided \eqref{CScondition} holds for a value of $\gamma$ for some distribution. We remark that the precise relationship between $m$, $n$, and $k$ guaranteeing that $f$ is not identically zero is a challenging problem in real algebraic geometry. In any case, we set $\gamma_0$ to be the smallest known such $\gamma$.

% It so happens that a similar statement applies to sets of vectors with a stable sparse representation. As in \cite[Sec.~IV]{Hillar15}, consider the ``symbolic'' dataset $Y = \{\mathbf{A}\mathbf{x}_1,\ldots,\mathbf{A} \mathbf{x}_N\}$ generated by indeterminate $\mathbf{A}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N$. 

\begin{theorem}\label{robustPolythm} %label is fucking up formatting..???
Fix $n, m$, and $k < m$. There is a polynomial in the entries of $\mathbf{A}$ and the $\mathbf{x}_i$ with the following property:  if the polynomial evaluates to a nonzero number and at least \mbox{$(k-1){m \choose k}+1$} of the resulting vectors $\mathbf{x}_i$ are supported on each $S \in E$ for some regular $k$-uniform $E \subseteq 2^{[m]}$ satisfying the singleton intersection property, then $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ (Def.~\ref{Uniqueness}). In particular, either no substitutions impart to $Y$ this property or all but a Borel set of measure zero do. 
\end{theorem}

% this is cool, but let's just save it for later...i can ask around.  -cjh
%An interesting open question in real algebraic geometry is which collections of $(m,n,k)$ determine this last ``or".

\begin{corollary}\label{ProbabilisticCor}
Fix $n, m$, and $k$ satisfying \eqref{CScondition} for $\gamma = \gamma_0$, and let the entries of the matrix $\mathbf{A} \in \mathbb{R}^{n \times m}$ and $k$-sparse vectors $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ be drawn independently from probability measures absolutely continuous with respect to the standard Borel measure $\mu$. If at least $(k-1){m \choose k} + 1$ of the vectors $\mathbf{x}_i$ are supported on each $S \in E$ for some regular $k$-uniform $E \subseteq 2^{[m]}$ satisfying the singleton intersection property, then $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ with probability one.
\end{corollary}

[ ** TODO ** elaborate a little on these? ]

%Next, we address the case when only an upper bound $m'$ on the latent dimension $m$ is known (assuming that $\mathbf{B}$ satisfies \eqref{SparkCondition}).

%\begin{theorem}\label{DeterministicUniquenessTheorem2}
%Fix integers $n$ and $k < m \leq m'$ and matrices $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ and $\mathbf{B} \in \mathbb{R}^{n \times m'}$ both satisfying \eqref{SparkCondition}. Suppose \mbox{$\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$} include at least \mbox{$(k-1){m' \choose k}+1$} $k$-sparse vectors in general linear position supported on each set in some $k$-uniform $E \subseteq 2^{[m]}$ satisfying the singleton intersection property. Then there exists a constant $C_3 > 0$ for which the following holds.

%If for some $k$-sparse $\mathbf{\hat x}_1, \ldots, \mathbf{\hat x}_N \in \mathbb{R}^{m'}$ and $\varepsilon < L_2(\mathbf{A}) / C_3$ we have \mbox{$\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\hat x}_i\|_2 \leq \varepsilon$} for all $i$, then:
%\begin{align}\label{Cstablem'}
%\|\mathbf{A}_j-(\mathbf{B}_J\mathbf{PD})_j\|_2 \leq C_3\varepsilon \ \ \text{for all $j \in [m]$}
%\end{align}
%
%for some $J \in {[m'] \choose m}$, permutation matrix $\mathbf{P}$, and invertible diagonal matrix $\mathbf{D}$.
%\end{theorem}

%In other words, the columns of $B$ contain (up to noise, after appropriate scaling) the columns of the original dictionary $\mathbf{A}$. Similarly, one can show by the same arguments at the beginning of Sec.~\ref{DUT} that the $\mathbf{\hat x}_i$ contain the original codes $\mathbf{x}_i$. The constant $C_3$ here is expression (\ref{Cdef2}) from the proof of Thm.~\ref{DeterministicUniquenessTheorem2}. Note that, in contrast to Thm.~\ref{DeterministicUniquenessTheorem}, this constant is dependent on $\mathbf{B}$; hence, \eqref{Cstablem'} holds for \emph{all} matrices $\mathbf{B} \in \mathbb{R}^{n \times m'}$ satisfying the spark condition only in the case $\varepsilon = 0$. 

\section{Proof of Theorem~\ref{DeterministicUniquenessTheorem}}\label{DUT}

% ======== b - PDa =========
First note that $\|\mathbf{x}\|_1 \leq \sqrt{k} \|\mathbf{x}\|_2$ for $k$-sparse $\mathbf{x} \in \mathbb{R}^m$, which by definition of $L_k(A)$ implies the following often used inequality:
\begin{align}\label{delrho}
L_k(\mathbf{A}) \leq \frac{\|\mathbf{A}\mathbf{x}\|_2}{\sqrt{k} \|\mathbf{x}\|_2} %\leq \frac{\|\mathbf{x}\|_1}{\sqrt{k} \|\mathbf{x}\|_2} \max_{i \in [m]}\|\mathbf{A}_i\|_2 
\leq  \max_{i \in [m]}\|\mathbf{A}_i\|_2.
\end{align}

Our first step is to show how dictionary recovery \eqref{Cstable} already implies sparse code recovery \eqref{b-PDa} when $\varepsilon < L_{2k}(\mathbf{A}) / C_1$. For all $2k$-sparse $\mathbf{x} \in \mathbb{R}^m$, the triangle inequality gives \mbox{$\|(\mathbf{A}-\mathbf{BPD})\mathbf{x}\|_2  \leq C_1\varepsilon \|\mathbf{x}\|_1 \leq C_1 \varepsilon \sqrt{2k}  \|\mathbf{x}\|_2$}. Thus, 
\begin{align*}
\|\mathbf{BPD}\mathbf{x}\|_2 
&\geq | \|\mathbf{A}\mathbf{x}\|_2 - \|(\mathbf{A}-\mathbf{BPD})\mathbf{x}\|_2 | \\
&\geq \sqrt{2k} (L_{2k}(\mathbf{A}) -  C_1\varepsilon) \|\mathbf{x}\|_2,
\end{align*}
%
where we drop the absolute value due to the upper bound on $\varepsilon$. Hence, $L_{2k}(\mathbf{BPD}) \geq L_{2k}(\mathbf{A}) - C_1\varepsilon  > 0$ and \eqref{b-PDa} then follows from:
\begin{align*}
\|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\hat x}_i \|_1
%&\leq \sqrt{2k} |\mathbf{x}_i - D^{-1}P^{\top}\mathbf{\hat x}_i\|_2 \\
&\leq \frac{\|\mathbf{BPD}(\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\hat x}_i)\|_2}{L_{2k}(\mathbf{BPD})} \\
&\leq \frac{\|\mathbf{B}\mathbf{\hat x}_i - \mathbf{A}\mathbf{x}_i\|_2 + \|(\mathbf{A} - \mathbf{BPD})\mathbf{x}_i\|_2}{L_{2k}(\mathbf{BPD})} \\
&\leq \frac{\varepsilon (1+C_1 \|\mathbf{x}_i\|_1)}{L_{2k}(\mathbf{BPD})}.
%&\leq \frac{\varepsilon}{\sqrt{2k}} \left( \frac{ 1+C_1 \|\mathbf{x}_i\|_1 }{ L_{2k}(\mathbf{A}) -  C_1\varepsilon } \right).
%\leq \frac{\varepsilon }{\varepsilon_0 - \varepsilon} \left( C_1^{-1}+|\mathbf{x}_i\|_1 \right).
\end{align*}

The heart of the matter is therefore \eqref{Cstable}, which we now establish using the following approach. Pigeonholing the data into its set of possible supports with respect to the matrix $\mathbf{B}$, one sees that the column space of every $\mathbf{A}_S$ with $S \in E$ must be nearby (in a sense yet to be defined; see Def.~\ref{dDef}) the column space of some $n \times k$ submatrix of $\mathbf{B}$. \eqref{Cstable} then follows from our result in combinatorial matrix analysis (Lem.~\ref{MainLemma}).

In fact, in the case $k=1$ we can relax our assumptions even further and assume that the matrix $\mathbf{B}$ has only at least as many columns as $\mathbf{A}$; as we will see, the conclusions of Thm.~\ref{DeterministicUniquenessTheorem} then hold for some $n \times m$ submatrix of $\mathbf{B}$.  [*** ??? ***]

\begin{proof}[Proof ($k=1$)]
Note that the only 1-uniform hypergraph satisfying the singleton intersection property is $[m]$; hence, we have $\mathbf{x}_i = c_i \mathbf{e}_i$ for $c_i \in \mathbb{R} \setminus \{\mathbf{0}\}$, $i \in [m]$. In this case we may take $C_1$ to be any value satisfying $C_1^{-1} \leq \min_{j \in [m]} |c_{j}|$. 

Fix $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfying \eqref{SparkCondition} and suppose that for some $\mathbf{B} \in \mathbb{R}^{n \times m'}$ ($m' \geq m$) and $1$-sparse $\mathbf{\hat x}_i \in \mathbb{R}^{m'}$ we have  $\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\hat x}_i\|_2 \leq \varepsilon < L_2(\mathbf{A}) / C_1$ for all $i$. Then there are $c'_1, \ldots, c'_m \in \mathbb{R}$ and $\pi: [m] \to [m']$ with:
\begin{align}\label{1D}
\|c_j\mathbf{A}_j - c'_j\mathbf{B}_{\pi(j)}\|_2 \leq \varepsilon, \ \ \text{for all $j$}.
\end{align} 
Note that $c'_j \neq 0$ for all $j$ since otherwise by \eqref{delrho} and definition of $C_1$ we would have $\|c_j\mathbf{A}_j\|_2 < \min_{\ell \in [m]}\|c_{\ell}\mathbf{A}_{\ell}\|_2$. 

We  now show that $\pi$ is injective (and thus is a permutation if $m' = m$). Suppose that $\pi(i) = \pi(j) = \ell$ for some $i \neq j$ and $\ell$. Then, $\|c_{j}\mathbf{A}_{j} - c'_{j}\mathbf{B}_{\ell}\|_2 \leq \varepsilon$ and $\|c_{i}\mathbf{A}_{i} - c'_{i} \mathbf{B}_{\ell}\|_2  \leq \varepsilon$. Scaling and summing these inequalities by $|c'_{i}|$ and $|c'_{j}|$, respectively, and applying the triangle inequality, we obtain:
\begin{align*}%\label{contra}
(|c'_{i}| + |c'_{j}|) \varepsilon
&\geq\|\mathbf{A}(c'_{i}c_{j} \mathbf{e}_{j} - c'_{j}c_{i}\mathbf{e}_{i})\|_2 \nonumber \\ 
&\geq  \left( |c'_{i}| + |c'_{j}| \right) L_2(\mathbf{A}) \min_{\ell \in [m]} |c_\ell |,
\end{align*}
%
which contradicts the upper bound $\varepsilon < L_2(\mathbf{A})/C_1$. Hence, the map $\pi$ is injective. Setting $J = \pi([m])$ and letting $P = \left( \mathbf{e}_{\pi(1)} \cdots \mathbf{e}_{\pi(m)}\right)$ and $D = \text{diag}(\frac{c'_1}{c_1},\ldots,\frac{c'_m}{c_m})$, we see that \eqref{1D} becomes, for all $j \in [m]$:
\begin{align*}%\label{k=1result}
\|\mathbf{A}_j - (\mathbf{B}_J\mathbf{PD})_j\|_2 
= \|\mathbf{A}_j - \frac{c'_j}{c_j}\mathbf{B}_{\pi(j)}\|_2 
\leq \frac{\varepsilon}{|c_j|} 
\leq C_1\varepsilon.
\end{align*}
\end{proof}

%\begin{remark}
%The above arguments can be easily modified to show that the conclusions of Thm.~\ref{DeterministicUniquenessTheorem} in this case ($k=1$) hold for some $n \times m$ submatrix of $\mathbf{B}$ in the event where only an upper bound on the number of columns in $\mathbf{A}$ is known; i.e., the recovered matrix $\mathbf{B}$ is set to have as many or more columns than $\mathbf{A}$. 
%\end{remark}

%It is easy to see that when $m < m'$, the above result holds for the submatrix of $B$ composed of columns indexed by the image of $\pi$.

We require a few additional tools to extend the proof to the general case $k < m$. These include a general notion of distance (Def.~\ref{dDef}) and angle (Def.~\ref{FriedrichsDefinition}) between subspaces and a stability result in matrix analysis (Lem.~\ref{MainLemma}).

\begin{definition}\label{dDef}
For any point $\mathbf{u}$, let $\text{\rm dist}(\mathbf{u}, V) := \inf \{\| \mathbf{u}-\mathbf{v} \|_2: \mathbf{v} \in V\}$ be the distance of $\mathbf{u}$ from the set $V$. For subspaces $U,V \subseteq \mathbb{R}^m$, we define: %[Kato p.197]
\begin{align}\label{d}
d(U,V) := \sup_{\mathbf{u} \in U, \|\mathbf{u}\|_2 = 1} \text{\rm dist}(\mathbf{u},V).
\end{align}
Note that $d(U,\{\textbf{0}\}) = 1$ for $U \neq \{\mathbf{0}\}$, whereas if $U = \{\textbf{0}\}$ then $d$ has no meaning; in this case, set $d(\{\textbf{0}\},V) = 0$ for any $V$.
\end{definition}
%Note that the supremum is always achievable by a point in $U$ when $U, V \subseteq \mathbb{R}^m$.

We note the following facts about $d$. For subspaces $U,V \subseteq \mathbb{R}^m$, we have \cite[Cor.~2.6]{Kato2013}:
\begin{align}\label{dimLem}
d(U,V) < 1 \implies \dim(U) \leq \dim(V).
\end{align}
%
and \cite[Lem.~3.2]{Morris10}:
\begin{align}\label{eqdim}
\dim(U) = \dim(V) \implies d(U,V) = d(V,U).
\end{align}

%\begin{lemma}\label{dimLem} %Kato p.200, 
%\cite[Cor.~2.6]{Kato2013} For subspaces $U,V \subseteq \mathbb{R}^m$, $d(U,V) < 1$ implies $\dim(U) \leq \dim(V)$. 
%\end{lemma}

%\begin{lemma}\label{eqdim}
%\cite[Lem.~3.2]{Morris10} If $\dim(U) = \dim(V)$ then $d(U,V) = d(V,U)$. 
%\end{lemma}

Our result in combinatorial matrix analysis is the following, which we derive by the following arguments (see the Supplemental for the full proof). First, we show that the aforementioned proximity between $k$-dimensional subspaces implies a proximity between smaller subspaces spanned by columns of $\mathbf{A}$ indexed by the intersections of sets in $E$ and those spanned by as many or fewer columns of $\mathbf{B}$. Another pigeonholing argument here combined with our assumptions on $E$ (e.g., the singleton intersection property) reveals that, in fact, each column of $\mathbf{A}$ spans a subspace proximal to that spanned by some column of $\mathbf{B}$. \eqref{Cstable} is a simple consequence of this last fact, which actually constitutes the proof of the theorem for the case $k=1$. We present this special case now before restating the above arguments in greater detail.

\begin{lemma}\label{MainLemma}
Fix integers $n$ and $k < m$, and suppose $\mathbf{A} \in \mathbb{R}^{n \times m}$ satisfies the spark condition \eqref{SparkCondition}. There exists a constant $C_2 > 0$ for which the following holds for all $\varepsilon < L_2(\mathbf{A}) / C_2$. If for some  $\mathbf{B} \in \mathbb{R}^{n \times m}$ and regular $E \subseteq2^{[m]}$ satisfying the singleton intersection property there exists a size-preserving map $\pi: E \mapsto 2^{[m]}$ satisfying:
\begin{align}\label{GapUpperBound}
d(\text{\rm Span}\{\mathbf{A}_{S}\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \leq \varepsilon, \ \ \text{for all $S \in E$},
\end{align}
%
then there exists some permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ such that:
\begin{align}\label{MainLemmaBPD}
\|\mathbf{A}_j - (\mathbf{BPD})_j\|_2 \leq C_2 \varepsilon, \indent \text{for } j \in [m].
\end{align}
\end{lemma}

The constant $C_1$ in Thm.~\ref{DeterministicUniquenessTheorem} is then given by:
\begin{align}
C_1(\mathbf{A}, \{\mathbf{x}_i\}_{i=1}^N, E) := \frac{ C_2(\mathbf{A}, E) } { \min_{S \in E} L_k(\mathbf{AX}_{I(S)}) },
\end{align}
%
where $\mathbf{X}$ is the $m \times N$ matrix with columns $\mathbf{x}_i$ and $I(S) := \{i : \text{supp}(\mathbf{x}_i) = S\}$.\footnote{We can be sure $C_1 > 0$ is well-defined (provided $C_2$ is well-defined) since $L_k(\mathbf{A}), L_k(\mathbf{X}_{I(S)}) > 0$ by the spark condition and general linear position of the $\mathbf{x}_i$; hence $\|\mathbf{A}_j\|_2 > 0$ for all $j$ and for all $S \in E$ we have $\|\mathbf{AX}_{I(S)}\mathbf{c}\|_2 \geq L_k(\mathbf{A})\|\mathbf{X}_{I(S)}\mathbf{c}\|_2 \geq L_k(\mathbf{A}) L_k(\mathbf{X}_{I(S)})\|\mathbf{c}\|_2$ for all $k$-sparse $\mathbf{c}$ and therefore $L_k(\mathbf{AX}_{I(S)}) \geq L_k(\mathbf{A}) L_k(\mathbf{X}_{I(S)}) > 0$.} The constant $C_2 = C_2(\mathbf{A}, E)$ is defined explicitly below in terms of the following technical quantity based on one used in \cite{Deutsch12} to analyze the convergence of the alternating projections algorithm for projecting a point onto the intersection of a set of subspaces. We use it to bound the distance between a point and the intersection of a set of subspaces given an upper bound on its distance from each subspace individually.

\begin{definition}\label{SpecialSupportSet}\label{FriedrichsDefinition}
For subspaces $V_1, \ldots, V_\ell \subseteq \mathbb{R}^m$, set $r := 1$ when $\ell = 1$ and define for $\ell \geq 2$:
\begin{align*}
r(\{V_i\}_{i=1}^\ell) := 1 - \left(1 -  \max \prod_{i=1}^{\ell-1} \sin^2  \theta \left(V_i, \cap_{j>i} V_j \right)  \right)^{1/2},
\end{align*} 
%
where the maximum is taken over all possible orderings\footnote{We modify the quantity in \cite{Deutsch12} in this way since the subspace ordering is irrelevant to our purpose.} of the $V_i$ and where the angle $\theta \in (0,\frac{\pi}{2}]$ is defined in terms of its cosine as \cite[Def.~9.4]{Deutsch12}:
\begin{align*}
\cos{\theta(U,W)} := \max\left\{ |\langle \mathbf{u}, \mathbf{w} \rangle|: \substack{ \mathbf{u} \in U \cap (U \cap W)^\perp, \ \|\mathbf{u}\|_2 \leq 1 \\ \mathbf{w} \in W \cap (U \cap W)^\perp, \  \|\mathbf{w}\|_2 \leq 1 } \right\}.
\end{align*}
\end{definition}
Note that $\theta \in (0,\frac{\pi}{2}]$ implies $0 < r \leq 1$. (We acknowledge the somewhat counter-intuitive property that $\theta =  \pi/2$ when $U = V$ and when $U \perp V$.)  %\textbf{Note:} we can also define $\theta$ by \cite[Lem.~9.5]{Deutsch12}:
%\begin{align*}
%\cos{\theta(U,W)} := \max\left\{ |\langle \mathbf{u}, \mathbf{w} \rangle|: \mathbf{u} \in U \cap (U \cap W)^\perp, \|\mathbf{u}\|_2 \leq 1, \mathbf{w} \in W \|\mathbf{w}\|_2 \leq 1  \right\}.
%\end{align*}

The constant $C_2$ in Lem.~\ref{MainLemma} is then given by:  % \footnote{Note that $C_2 > 0$ is well-defined since $r > 0$ by definition.}
\begin{align}
C_2(\mathbf{A}, E) := \frac{ 2^{|E|} \max_{i \in [m]} \|\mathbf{A}_i\|_2}{ \min_{F \subseteq E} r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F}) }.
\end{align}

Note that this definition leads to a value for $C_1$ consistent with that used in our proof of the case $k=1$ above.

\begin{proof}[Proof ($k < m$)] 
%We begin by showing that for every $S \in E$ there is some $S' \in {[m] \choose k}$ for which the distance $d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_S'\})$ is controlled by $\varepsilon$. Fix $S \in E$. Since there are $(k-1){m \choose k}+1$ vectors $\mathbf{x}_i$ supported on $S$, the pigeon-hole principle implies that there exists some $S' \in {[m] \choose k}$ and some set of $k$ indices $J(S)$ such that the supports of all $\mathbf{x}_i$ and $\mathbf{\hat x}_i$ with $i \in J(S)$ are contained in $S$ and $S'$, respectively.

%%% No |S| = k assumption %%%
We begin by showing that for every $S \in E$ there is some $S' \subseteq [m]$, $|S'| \leq k$ for which the distance $d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_S'\})$ is controlled by $\varepsilon$. Fix $S \in E$. Since there are $(k-1){m \choose k}+1$ vectors $\mathbf{x}_i$ supported on $S$, the pigeon-hole principle implies that there exists some $S' \in {[m] \choose k}$ and some set of $k$ indices $J(S)$ such that the supports of all $\mathbf{x}_i$ and $\mathbf{\hat x}_i$ with $i \in J(S)$ are contained in $S$ and $S'$, respectively.

Let $\mathbf{X}$ and $\mathbf{X}'$ be the $m \times N$ matrices with columns $\mathbf{x}_i$ and $\mathbf{\hat x}_i$, respectively. It follows from the general linear position of the $\mathbf{x}_i$ and the linear independence of every $k$ columns of $\mathbf{A}$ that $L_k(\mathbf{AX}_{J(S)}) > 0$; that is, the columns of the $n \times k$ matrix $\mathbf{AX}_{J(S)}$ are linearly independent and thus form a basis for $\text{\rm Span}\{\mathbf{A}_{S}\}$. Fixing $\mathbf{y} \in \text{\rm Span}\{\mathbf{A}_{S}\}$, there then exists a unique $\mathbf{c} = (c_1, \ldots, c_k) \in \mathbb{R}^k$ such that $\mathbf{y} = \mathbf{AX}_{J(S)}\mathbf{c}$. Letting \mbox{$\mathbf{y'} = \mathbf{BX}'_{J(S)}\mathbf{c}$}, which is in $\text{\rm Span}\{\mathbf{B}_{S'}\}$, we have:
\begin{align*}
\|\mathbf{y} - \mathbf{y'}\|_2 
&= \|\sum_{i=1}^k c_i(\mathbf{AX}_{J(S)} - \mathbf{BX}'_{J(S)})_i\|_2
\leq \varepsilon \sum_{i=1}^k |c_i| \\
&\leq \varepsilon \sqrt{k}  \|\mathbf{c}\|_2 
%\leq \frac{\varepsilon \sqrt{k}}{L(\mathbf{A}X_{J(S)})} \|\mathbf{A}X_{J(S)}\mathbf{c}\|_2 
= \frac{\varepsilon}{L_k(\mathbf{AX}_{J(S)})} \|\mathbf{y}\|_2,
\end{align*}
where the last equality is by definition of the domain-restricted matrix lower bound $L_k$. It follows that for all $S \in E$,
\begin{align}\label{rhs222}
d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{S'}\}) 
\leq \frac{\varepsilon}{ \min_{S \in E} L_k(\mathbf{AX}_{I(S)}) }.
%\leq \frac{\varepsilon}{L(\mathbf{AX}_{J(S)})}.
%&< \left( \frac{ L_2(\mathbf{A}) }{ 2^{|E|} \max_{j \in [m]} \|\mathbf{A}_j\|_2 } \right) \frac{ \min_{S \in E} L_k(\mathbf{AX}_{I(S)}) } { L(\mathbf{AX}_{J(S)}) } \min_{\substack{F \subseteq E}} r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F}) \\
\end{align}
The result then follows by  Lem.~\ref{MainLemma}, with the map $\pi: E \to {[m] \choose k}$ defined by the association $S \mapsto S'$ above.
%\begin{align}
%\delta= \frac{\varepsilon}{ \min_{S \in E} L_k(\mathbf{AX}_{I(S)}) } = \frac{ C \varepsilon }{ \tilde C } < \frac{L_2(\mathbf{A})}{\tilde C}
%\end{align}
\end{proof}

%We then show that the map over subsets of column indices defined by this association is such that the number of common elements shared by sets in its domain bounds the number of elements common to their images under this map. 
 
\section{Discussion}\label{Discussion}



Phrased 
in the language of Hadamard \cite{Hadamard1902}), 
 


In this note, we generalize the approach of \cite{Hillar15} to the uniqueness of solutions to Prob.~\ref{InverseProblem} for noisy measurements while also reducing the number of required samples.
% from $N=k{m \choose k}^2$ to $N = m(k-1){m \choose k}+m$. 
% CUT OK? Surprisingly, almost all $n \times m$ dictionaries satisfying the standard assumption \eqref{CScondition} from compressed sensing (CS) are identifiable from enough generic noisy $k$-sparse linear combinations of their elements, up to an error linear in the noise. Moreover, if solutions are constrained to satisfy \eqref{SparkCondition}, then only an upper bound on the number of dictionary elements need be taken as given. 
We remark that our result in the deterministic case (Thm.~\ref{DeterministicUniquenessTheorem}) accounts for \emph{worst-case} noise, whereas the ``effective'' noise might be much smaller when it is sampled from a given distribution; in such cases, the constants $C_1, C_3$ in Thms.~\ref{DeterministicUniquenessTheorem},~\ref{DeterministicUniquenessTheorem2} will be much smaller with high probability.
We note also that these results extend trivially to cases where point-wise injective nonlinearities are applied to the data. We close by outlining several diverse application areas.

% FRITZ: Inverse problems instead of Data Analysis. Don't assume that everyone is assuming they are recovering ground truth. Cite results again and state implications! Focus on them (probabilistic ones too).
\textbf{Inverse Problems}.  
Our results provide theoretical grounding for the use of sparse linear coding in blind source separation, wherein the goal is to infer the generating dictionary and sparse codes from noisy measurements generated as in \eqref{LinearModel} (e.g., recovering a rat's position on a linear track from local field potentials in Hippocampus \cite{Agarwal14}). It would be of practical utility therefore to determine the best possible dependence of $\varepsilon$ on $\delta_1, \delta_2$ (see \eqref{epsdel}) as well as the minimal requirements on the number and diversity of generating codes. %We encourage researchers to extend our results and tighten these parameters.

Mention sparse matrix factorization.

[Effective computation of stable datasets, at least in principle.]

\textbf{Theoretical Neuroscience}.
Sparse dictionary learning and related methods have recovered characteristic components of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent} and sounds \cite{bellsejnowski1996, smithlewicki2006, Carlson12}, reproducing response properties of cortical neurons. Our theorems suggest that this correspondence could be due to the uniqueness of sparse representations. Furthermore, our guarantees justify the hypothesis of \cite{Coulter10} and \cite{Isely10} that sparse codes passed through a communication bottleneck in the brain can be recovered from random projections via (unsupervised) biologically plausible sparse dictionary learning (e.g., \cite{rehnsommer2007, rozell2007neurally, hu2014hebbian}).   

Invariance in neuroscience: Pitts \cite{pitts1947}.
%TODO: add reproduction of response propeties in olfactory cortex?

% Reiterate probabilistic result in the smooth analysis part?
%\textbf{Smoothed Analysis}.
%The main concept in smoothed analysis \cite{Spielman04} is that certain algorithms having exponential worst-case behavior are, nonetheless, efficient if certain (typically, measure zero in the continuous case and with ``low probability" in the discrete case) pathological input sets are avoided. Our results imply that if there is an efficient ``smoothed" algorithm for solving Prob.~\ref{InverseProblem} given enough samples, then for generic inputs this algorithm determines the unique original solution. We note that avoiding certain pathological sets of inputs is often a necessary technicality for dictionary learning \cite{Razaviyayn15, Tillmann15}.

\textbf{Engineering}.
Several groups utilize compressed sensing for signal processing tasks: MRI analysis \cite{lustig2008compressed},  image compression \cite{Duarte08}, and, more recently, the design of an ultrafast camera \cite{Gao14}. Given such effective uses of compressed sensing, it is only a matter of time before these systems incorporate sparse dictionary learning to encode and process data. Guarantees such as those offered by our theorems allow any such device to be equivalent to any other (having different initial parameters and data samples) as long as enough data originate from a statistically identical system.


\acknow{We thank Friedrich Sommer for turning our attention to the dictionary learning problem, Darren Rhea for sharing early explorations, and Ian Morris for posting identity \eqref{SubspaceMetricSameDim} online.}

\showacknow % Display the acknowledgments section

% \pnasbreak % splits and balances the columns before the references.
% If you see unexpected formatting errors, try commenting out this line
% as it can run into problems with floats and footnotes on the final page.
%\pnasbreak %was causing only first two pages to print..??

% Bibliography
\bibliography{chazthm_pnas}

%% SUPPLEMENTAL INFO? %%%
%\begin{remark}
%Actually, $\mathbf{A}$ need not be injective on all $k$-sparse vectors for Theorem \ref{DeterministicUniquenessTheorem} to hold; rather, it need only be injective the set of all vectors with supports in $E$. [** TODO ** we will probably have to redefine $L_2$ since it is over all $k$-sparse vectors -- may have to instead define $L_E$.] Wait, $E$ need not be regular for $B$ sat. spark cond. Also it need not be $k$-regular!
%\end{remark}

\clearpage

\section{Proof of Main Lemma}

We require some auxiliary lemmas.

\begin{lemma}\label{SpanIntersectionLemma}
Let $\mathbf{M} \in \mathbb{R}^{n \times m}$. If every $2k$ columns of $\mathbf{M}$ are linearly independent, then for any $F \subseteq 2^{[m]}$ of rank $k$:
\begin{align*}
\text{\rm Span}\{\mathbf{M}_{\cap F}\}  = \cap_{S \in F} \text{\rm Span}\{\mathbf{M}_S\}.
\end{align*}
\end{lemma}
\begin{proof}
By induction, it is enough to prove the lemma when $|F| = 2$. The proof now follows directly from the assumption.
\end{proof}

% This can be made tighter by using Pythagoras' Thm for first projection instead of triangle inequality. 
\begin{lemma}\label{DistanceToIntersectionLemma}
Fix $k \geq 2$. Let $V_1, \ldots, V_k$ be subspaces of $\mathbb{R}^m$ and set $V = \cap_{i = 1}^k V_i$. For every $\mathbf{x} \in \mathbb{R}^m$, we have:
\begin{align}\label{DTILeq}
\text{\rm dist}(\mathbf{x}, V) \leq \frac{1}{r(\{V_i\}_{i = 1}^k)} \sum_{i=1}^k \text{\rm dist}(\mathbf{x}, V_i),
\end{align}
where $r$ is given in Def.~\ref{SpecialSupportSet}.
\end{lemma}
\begin{proof} 
Fix $\mathbf{x} \in \mathbb{R}^m$ and $k \geq 2$. Recall that the orthogonal projection onto a subspace $V \subseteq \mathbb{R}^m$ is the mapping $\Pi_V$ from $\mathbb{R}^m$ to $V$ that associates with each $\mathbf{x}$ its unique nearest point in $V$; i.e., $\|\mathbf{x} - \Pi_V\mathbf{x}\|_2 = \text{\rm dist}(\mathbf{x}, V)$.
% Since subspaces of real vector spaces are closed, we can replace inf with min in def. of orthogonal projection

We begin the proof by observing that:
%Use Pythagoras' Theorem first?
\begin{align}\label{f}
\|\mathbf{x} - \Pi_V\mathbf{x}\|_2 &\leq \|\mathbf{x} - \Pi_{V_k} \mathbf{x}\|_2 + \|\Pi_{V_k}  \mathbf{x} - \Pi_{V_k}\Pi_{V_{k-1}}\mathbf{x}\|_2 \nonumber \\
&\ \ \ + \cdots + \|\Pi_{V_k} \Pi_{V_{k-1}}\cdots \Pi_{V_1} \mathbf{x} - \Pi_V \mathbf{x}\|_2 \nonumber \\
&\leq \|\Pi_{V_k}\cdots\Pi_{V_{1}} \mathbf{x} - \Pi_V \mathbf{x}\|_2 + \sum_{\ell=1}^k \|\mathbf{x} - \Pi_{V_{\ell}} \mathbf{x}\|_2,
\end{align}
%
by the triangle inequality and the fact that the spectral norm $\|\Pi_{V_{\ell}}\|_2 \leq 1$ for all $\ell$ (since $\Pi_{V_{\ell}}$ are orthogonal projections).

The desired result \eqref{DTILeq} now follows by bounding the second term on the right-hand side using the following fact \cite[Thm.~9.33]{Deutsch12}:
\begin{align}
\|\Pi_{V_k}\Pi_{V_{k-1}}\cdots\Pi_{V_1} \mathbf{x} - \Pi_V\mathbf{x}\|_2 \leq z \|\mathbf{x}\|_2, %  \indent \text{for } \mathbf{x} \in \mathbb{R}^m,
\end{align}
for \mbox{$z^2= 1 - \prod_{\ell =1}^{k-1}(1-z_{\ell}^2)$} and \mbox{$z_{\ell} = \cos\theta\left(V_{\ell}, \cap_{s=\ell+1}^k V_s\right)$}. Together with $\Pi_{V_\ell} \Pi_V = \Pi_V$ for all $\ell = 1, \ldots, k$ and $\Pi_V^2 = \Pi_V$, this yields:
\begin{align*}
\|\Pi_{V_k} \cdots \Pi_{V_1}\mathbf{x}  - \Pi_V \mathbf{x} \|_2 
&= \|\left( \Pi_{V_k} \cdots\Pi_{V_1} - \Pi_V \right) (\mathbf{x} - \Pi_V\mathbf{x})\|_2 \\
&\leq z\|\mathbf{x} - \Pi_V\mathbf{x}\|_2.
\end{align*}

Plugging this into \eqref{f} and rearranging yields \eqref{DTILeq} after substituting $r(\{V_i\}_{i=1}^k)$ for $1 - z$ (as $z$ may depend on the arbitrary ordering of the $V_i$).
\end{proof}

\begin{lemma}\label{NonEmptyLemma} 
Fix integers $\ell$ and $m \leq \hat m$. Suppose $E \subseteq 2^{[m]}$ is $\ell$-regular and satisfies the singleton intersection property. If there exists a map $\pi: E \to 2^{[\hat m]}$ such that $\sum_{S \in E} |S| = \sum_{S \in E} |\pi(S)|$ and:
\begin{align}\label{cond}
|\cap_{S \in F} \pi(S)| \leq |\cap_{S \in F} S | \ \ \text{for all } F \subseteq E,
\end{align}
%
then the association $i \mapsto \cap \pi(F(i))$ defines an injective map from $[J]$ to $[\hat m]$ for some $J \subseteq [m]$ of size $\hat m - \ell(\hat m - m)$. In particular, if $\hat m = m$ then the map $\pi$ is induced by a permutation.
%$\pi(E)$ is $\ell$-regular and satisfies the singleton intersection property. In particular, $|\cap_{S \in F} S| = 1$ if and only if $|\cap_{S \in F} \pi(S)| = 1$. 
\end{lemma}
\begin{proof}
[* TODO * Chris rewrite for $\hat m$] Fix $T = \{(j, S): j \in \pi(S), S \in E\}$. We have $|T| = \sum_{S \in E} |\pi(S)| = \sum_{S \in E} |S| = \sum_{i \in [m]} \deg(i) = \ell m$ by regularity of $E$. Note that by \eqref{cond} there can be no more than $\ell$ elements of $T$ with a given first index, since $E$ is $\ell$-regular. Pigeonholing the elements of $T$ with respect to the set of possible first indices, we see then that for each $j$ there must be exactly $\ell$ elements of $T$ with $j$ as a first index; hence, $\pi(E)$ is $\ell$-regular. 

Fix $j$ and let $F(j) = \{S \in E: j \in \pi(S)\}$ be the preimage of the star in $\pi(E)$ centered at $j$, which is of size $\ell$ by the above arguments. It follows from \eqref{cond} that $\cap_{S \in F(j)} S$ is nonempty. In fact, we must have $|\cap _{S \in F(j)} S| = 1$, since $E$ is $\ell$-regular and satisfies the singleton intersection property. It follows by \eqref{cond} that $\cap_{S \in F(j)} \pi(S) = \{j\}$. Thus $\pi(E)$ satisfies the singleton intersection property as well, and the preimage of every one of the $m$ stars in $\pi(E)$ is a star in $E$.

It remains to show that every star in $E$ maps through $\pi$ to some star in $\pi(E)$. This follows by pigeonholing the $m$ stars of $\pi(E)$ with respect to their $m$ possible preimages if no two stars in $\pi(E)$ share the same preimage. This indeed must be the case, since $F(i) = F(j)$ implies $\{i\} = \cap_{S \in F(i)} \pi(S) = \cap_{S \in F(j)} \pi(S) = \{j\}$.
\end{proof}

\begin{proof}[Proof of Main Lemma]
We first claim $\dim(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) = \dim(\text{\rm Span}\{\mathbf{A}_{S}\})$ for all $S \in E$. To see why, note that the right-hand side of \eqref{GapUpperBound} is then strictly less than one, since $r \leq 1$ and $L_2(\mathbf{A}) \leq 1$ by \eqref{delrho}. By \eqref{dimLem}, it then follows that $|\pi(S)| \geq \dim(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \geq \dim(\text{\rm Span}\{\mathbf{A}_{S}\}) = |S|$, with the equality due to $\mathbf{A}$ satisfying \eqref{SparkCondition}; hence, $\dim(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) = \dim(\text{\rm Span}\{\mathbf{A}_{S}\})$ (since $|S| = |\pi(S)|$). It follows that the columns of $\mathbf{B}_{\pi(S)}$ are linearly independent and, by \eqref{eqdim}:% and $\mathbf{B}_i \neq \textbf{0}$ for all $i$.
\begin{align}\label{eq2}
d(\text{\rm Span}\{\mathbf{B}_{\pi(S)}\}, \text{\rm Span}\{\mathbf{A}_S\} ) = d(\text{\rm Span}\{\mathbf{A}_S\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}).
\end{align}
We will now show that:
\begin{align}\label{fact2}
|\bigcap_{S \in F} \pi(S)| \leq |\bigcap_{S \in F} S |, \ \ \text{for all } \ F \subseteq E.
\end{align}

Fix $F \subseteq E$. Since $\text{\rm Span}\{\mathbf{B}_{\cap_{S \in F}\pi(S)}\} \subseteq \cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}$, if $\cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\} = \textbf{0}$ then we must have $|\cap_{S \in F} \pi(S)| = 0$ (as $\mathbf{B}_i \neq \textbf{0}$ for all $i$) and \eqref{fact2} trivially holds. Suppose then that the intersection is not the zero vector. By Lem.~\ref{SpanIntersectionLemma} and Lem.~\ref{DistanceToIntersectionLemma}, and incorporating \eqref{eq2} and \eqref{GapUpperBound}, we have:
\begin{align}\label{randoml}
d( \text{\rm Span}&\{\mathbf{B}_{\cap_{S \in F}\pi(S)}\}, \text{\rm Span}\{\mathbf{A}_{\cap_{S \in F} S}\}  ) \nonumber \\
&\leq d\left( \cap_{S \in F} \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}, \cap_{S \in F} \text{\rm Span}\{\mathbf{A}_{S}\} \right) \nonumber \\
&\leq \sum_{S \in F} \frac{ d\left( \cap_{T \in F} \text{\rm Span}\{\mathbf{B}_{\pi(T)}\},\text{\rm Span}\{\mathbf{A}_{S}\} \right) }{ r( \{ \text{\rm Span}\{\mathbf{A}_{T}\}_{T \in F}) } \nonumber \\
&\leq \sum_{S \in F} \frac{ d\left( \text{\rm Span}\{\mathbf{B}_{\pi(S)}\},\text{\rm Span}\{\mathbf{A}_{S}\} \right) }{ r( \{ \text{\rm Span}\{\mathbf{A}_{T}\}_{T \in F}) }\nonumber \\
%&\leq \frac{1}{r( \{ \text{\rm Span}\{\mathbf{A}_{S}\}_{S \in F})} \sum_{S \in F} \frac{ \varepsilon }{ L(\mathbf{AX}_{J(S)}) } \nonumber \\
&\leq \frac{|F| \delta}{r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F})} 
%&< L_2(\mathbf{A}) \frac{|F| }{ 2^{|E|}} \left( \frac{ \min_{\substack{F \subseteq E}} r( \{ \text{\rm Span}\{\mathbf{A}_{S}\}_{S \in F}) }{ r( \{ \text{\rm Span}\{\mathbf{A}_{S}\}_{S \in F}) } \right) \left( \frac{\min_{S \in E} L_k(\mathbf{AX}_{I(S)}) }{ \min_{T \in F} L(\mathbf{AX}_{J(T)}) } \right) \nonumber \\
\leq \tilde C_2 \delta, 
\end{align}
%
Note that since $\delta < L_2(\mathbf{A}) / \tilde C_2$, by \eqref{delrho} the right-hand side in \eqref{rando} is strictly less than one, so \eqref{dimLem} implies that $\dim(\text{\rm Span}\{\mathbf{B}_{\cap_{S \in F}\pi(S)}\}) \leq \dim(\text{\rm Span}\{\mathbf{A}_{\cap_{S \in F} S}\})$ and \eqref{fact2} follows from the linear independence of the columns of $\mathbf{A}_{S}$ and $\mathbf{B}_{\pi(S)}$ for all $S \in F$.

Now, fix $\ell \in [m]$. Since $E$ satisfies the singleton intersection property, $\{\ell\} = \cap F(\ell)$ for the star $F(\ell) \subseteq E$. By \eqref{fact2}, we have that $\cap_{S \in F(\ell)} \pi(S)$ is either empty or it contains a single element. Lem.~\ref{NonEmptyLemma} ensures the latter case is the only possibility. Thus, the association $\ell \mapsto \cap_{S \in F(\ell)} \pi(S)$ defines a map $\hat \pi: [m] \to [m]$ and $\dim(\text{\rm Span}\{\mathbf{B}_{\hat \pi(\ell)}\}) = 1$, since $\mathbf{B}_i \neq \textbf{0}$ for all $i$. By \eqref{eqdim}, it follows from \eqref{rando} that $d\left( \text{\rm Span}\{\mathbf{A}_\ell\}, \text{\rm Span}\{ \mathbf{B}_{\hat \pi(\ell)} \} \right) \leq C_2\delta$. Since $\ell$ is arbitrary, fixing $\hat \varepsilon = C_2\delta$ it follows from \eqref{rando} that for every basis vector $\mathbf{e}_\ell \in \mathbb{R}^m$ there exists some $c'_\ell \in \mathbb{R}$ such that $\|\mathbf{A}\mathbf{e}_\ell - c'_\ell \mathbf{B}\mathbf{e}_{\hat \pi(\ell)}\|_2 \leq \hat \varepsilon < L_2(\mathbf{A})$. This is exactly the supposition in \eqref{1D} (letting $c_i = \|\mathbf{A}_i\|_2^{-1}$ for all $i$) and the result follows from the subsequent arguments for the case $k=1$ in Sec.~\ref{DUT}.
\end{proof}

The above arguments can be easily modified to prove the following variation of Lem.~\ref{MainLemma}, key to proving Thm.~\ref{DeterministicUniquenessTheorem2}. 

\begin{lemma}\label{MainLemma2}
Fix integers $n$ and $k < m \leq m'$ and suppose $\mathbf{A}~\in~\mathbb{R}^{n \times m}$ and $\mathbf{B} \in \mathbb{R}^{n \times m'}$ both satisfiy \eqref{SparkCondition}. There exists a constant $C_4 > 0$ for which for all $\varepsilon < L_2(\mathbf{A}) / C_4$ the following holds. If for some $E \subseteq2^{[m]}$ satisfying the singleton intersection property there exists a size-preserving map $\pi: E \mapsto 2^{[m']}$ satisfying:
\begin{align}\label{GapUpperBound}
d(\text{\rm Span}\{\mathbf{A}_{S}\}, \text{\rm Span}\{\mathbf{B}_{\pi(S)}\}) \leq \varepsilon \ \ \text{for all $S \in E$},
\end{align}
%
then for some $J \in {[m'] \choose m}$, permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$, we have:
\begin{align}\label{MainLemmaBPD}
\|\mathbf{A}_j - (\mathbf{BPD})_j\|_2 \leq C_4 \varepsilon, \indent \text{for } j \in [m].
\end{align}
The constant $C_4$ is given by:
\begin{align}\label{Cdefm'}
C_4 := \frac{2^{|E|}}{R} \max_{j \in [m]} \|\mathbf{A}_j\|_2
\end{align}
%
with $R$ being the lesser of $\min_{\substack{F \subseteq E}} r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F})$ and $\min_{\substack{F \subseteq {[m'] \choose k}}} r( \{ \text{\rm Span}\{\mathbf{B}_{S}\} \}_{S \in F})$.
\end{lemma}

\begin{proof}
The proof is very similar to the case $m' = m$, the difference being that now we may not invoke Lem.~\ref{NonEmptyLemma} (which requires $m' = m$) to infer from \eqref{fact2} that $| \cap_{S \in F(i)} \pi(S) | = 1$ for all stars $F(i)$. We circumvent this issue by instead assuming the spark condition on $\mathbf{B}$, which allows us to swap the roles of $\mathbf{A}$ and $\mathbf{B}$ in the arguments leading to the derivation of \eqref{fact2} to prove the opposite inequality for every star $F(i)$. Of course, this requires that the constant $C_4$  depend on $\mathbf{B}$. By these arguments we establish that $| \cap_{S \in F(i)} \pi(S) | = 1$ for all stars $F(i)$, yielding an injective map $\hat \pi: [m] \to [m']$. The result then follows from the proof of the case $k=1$. 
\end{proof}

\section{Proofs of Thms.~\ref{robustPolythm} \& \ref{DeterministicUniquenessTheorem2} and Cors.~\ref{DeterministicUniquenessCorollary} \& \ref{ProbabilisticCor}}\label{AppendixB}

\begin{proof}[Proof of Cor.~\ref{DeterministicUniquenessCorollary}]
We need only demonstrate how to produce $N$ vectors $\mathbf{a}_i$ such that for some $E \subseteq {[m] \choose k}$ satisfying the singleton intersection property there are \mbox{$(k-1){m \choose k}+1$} vectors supported on each $S \in E$ in general linear position. Let $\gamma_1, \ldots, \gamma_N$ be any distinct numbers. Then the columns of the $k \times N$ matrix $V = (\gamma^i_j)^{k,N}_{i,j=1}$ are in general linear position (since the $\gamma_j$ are distinct, any $k \times k$ ``Vandermonde" sub-determinant is nonzero). Next, fix some $E \subseteq {[m] \choose k}$ satisfying the singleton intersection property. Finally, form the $k$-sparse vectors $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ with supports $S \in E$ (partitioning the $\mathbf{x}_i$ evenly among these supports so that each contains $(k-1){m \choose k}+1$ vectors $\mathbf{x}_i$) by setting the nonzero values $\mathbf{x}_i$ to be those contained in the $i$th column of $V$.
\end{proof}
% Actually this is even overkill since 

We now determine classes of datasets $Y$ having a stable sparse representation that are cut out by a single polynomial equation.

\begin{proof}[Proof of Thm.~\ref{robustPolythm}]
We sketch the argument, leaving the details to the reader.
Let $M$ be the $n \times m$ matrix with columns $\mathbf{A}\mathbf{x}_i$, $i \in [N]$.  Consider the following polynomial \cite[Sec.~IV]{Hillar15} in the entries of $\mathbf{A}$ and the $\mathbf{x}_i$:
\begin{align*}
g(\mathbf{A}, \{\mathbf{x}_i\}_{i=1}^N) = \prod_{S \in {[n] \choose k}} \sum_{S' \in {[N] \choose k}} (\det M_{S',S})^2,
\end{align*}
with notation as in Sec.~\ref{Results}.  

It can be checked that when $g$ is nonzero for a substitution of real numbers for the indeterminates, all of the genericity requirements on $\mathbf{A}$ and $\mathbf{x}_i$ in our proofs of stability in Thm.~\ref{DeterministicUniquenessTheorem} are satisfied (in particular, the spark condition on $\mathbf{A}$). The statement of the theorem now follows directly.
\end{proof}

\begin{proof}[Proof of Cor.~\ref{ProbabilisticCor}]
First, note that if a set of measure spaces $\{(X_{\ell}, \Sigma_{\ell}, \nu_{\ell})\}_{\ell=1}^p$ is such that $\nu_{\ell}$ is absolutely continuous with respect to $\mu$ for all $\ell = 1, \ldots, p$, where $\mu$ is the standard Borel measure on $\mathbb{R}$, then the product measure $\prod_{\ell=1}^p \nu_{\ell}$ is absolutely continuous with respect to the standard Borel product measure on $\mathbb{R}^p$ (e.g.,  \cite{folland2013real}). By Thm.~\ref{robustPolythm}, there is a polynomial that is nonzero whenever $Y$ has a stable $k$-sparse representation in $\mathbb R^m$; in particular, this property (stability) holds with probability one.
\end{proof}

\begin{proof}[Proof of Thm.~\ref{DeterministicUniquenessTheorem2}]
The proof is the same as that of the case $m' = m$, only now we establish a map $\pi: E \to {[m'] \choose k}$ by pigeonholing $(k-1){m' \choose k} + 1$ vectors with respect to holes $[m']$ and eventually applying Lem.~\ref{MainLemma2} instead of Lem.~\ref{MainLemma}. 

In this case the constant $C_3$ is given by:
\begin{align}\label{Cdefm'}
C_3 := \frac{2^{|E|} \max_{j \in [m]} \|\mathbf{A}_j\|_2}{ R \min_{S \in E} L_k(\mathbf{AX}_{I(S)}) },
\end{align}
%
with $R$ being the lesser of $\min_{\substack{F \subseteq E}} r( \{ \text{\rm Span}\{\mathbf{A}_{S}\} \}_{S \in F})$ and $\min_{\substack{F \subseteq {[m'] \choose k}}} r( \{ \text{\rm Span}\{\mathbf{B}_{S}\} \}_{S \in F})$.
\end{proof}

\end{document}