Dear editors at PNAS,

We greatly appreciate the feedback from the editor and referees concerning our manuscript, and we have modified it to reflect all their valuable suggestions and concerns.  As the major issue is not of technical correctness but rather importance, in this cover letter we argue for the significance of our mathematical work and its deep interactions with many fields, including universality in neuroscience and signal processing, as well as its unification of hundreds (if not thousands) of publications on the topic of unsupervised sparse linear coding, i.e. dictionary learning.  We also append to the end of this letter detailed point-by-point responses to the two reviews.

First, we address a major concern that our submission represents a minor mathematical generalization of [Hillar-Sommer-2015] and that it uses the same techniques.  In fact, our submission contains many significant new results, including a solution to Problem 2, which is the optimization formulation of uniqueness in dictionary learning and of fundamental interest to theorists, numerical analysts, and practitioners of their methods in data science.  
Of course, another such major result is a rigorous demonstration of uniqueness in sparse linear coding up to a discrepancy (error) that scales linearly in the non-zero measurement noise or modeling inaccuracy (with minimal assumptions).  This vast generalization of [Hillar-Sommer-2015] represents a significant endeavor in its own right, which necessitated the development of powerful practical tools that will allow researchers to understand better the theoretical implications of (the many different) dictionary learning platforms.  For example, one new innovation in our work is a theory of combinatorial designs for support sets that provides a handle for understanding some of the complexities of intersecting subspaces in dictionary learning problems.  Another is a fundamental lemma on inexact sparse matrix factorization that will be useful in many other contexts where uniqueness up to feature relabelling and scaling is desirable.  To give further evidence for how our novel combinatorial and analytical proof allows for more powerful conclusions in this generalization of [Hillar-Sommer-2015], consider also the following advances: 1) Reduction in number of samples to determine model, 2) latent dictionary dimension can be unknown (m'>m), 3) weaker spark condition assumption on the dictionary, and 4) a practical amount of data determines the model in certain settings.

Another major concern about our manuscript is that computational and practical relevance is lacking.  To address this concern, we have added new material on complexity considerations, including statements that are fully-polynomial in certain cases.  all our constants are explicit.  we are dealing with NP-hard situation, we should expect difficult -- in fact, the NP-hardness of dictionary learning with noise.  Tillman: the expert on this says that Problem 1's computational complexity: https://arxiv.org/pdf/1405.6664.pdf.  Maybe we can solve Tillman's problem The remaining very difficult problem is determining the smallest number of samples that determine the model.  but we think we have made progress on this and brought non-trivial, but elegant, tools to bear on the problem.

Finally, we address how our results impact model consistency in data analysis as well as their implications for science, more generally.  Occam's razor -- Problem 1, basic move in lots of data science "dictionary learning" (278,000), the central (NP-Hard) problem of dictionary learning approximated using L1-optimization.  "all interesting problems are NP-hard". in fact, there has never been an experiment with zero noise, so the results of [Hillar Sommer] affect zero scientists. new theorem on optimization formulation of the result -- that's how scientists approach their data completes the theory of NIPS ACS making viable actual result in theory of mind -- theories have to be resilient to noise or their nothing applies to thousands of scientific algorithmic theory of sparse linear coding -- removes one step for them to write algorithm papers universals in perception -- with noise (as all datasets have noise) lends further understanding to the basic problem of universal coding from sensors to cortex.  applied optimization theory: Hadamard well-posed, sparse linear coding with learning has a natural consistency, versus Deep Learning non-uniqueness of models. big win for sparse coding as a model: theoretical grounding for what you are doing.  NSF Grant () just recently proved with this result as the project's theoretical underpinnings.
