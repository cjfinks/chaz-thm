[Draft start]

Dear editors at PNAS,

We greatly appreciate the feedback from the editor and referees on our manuscript, and we have modified it to reflect all their valuable suggestions and concerns.  As the major issue is not of technical correctness but rather importance, in this cover letter, we argue for the significance of our mathematical work and its deep interactions with many fields, including universality in neuroscience and signal processing, as well as its unification of hundreds (if not thousands) of publications on the topic of unsupervised sparse linear coding (i.e. dictionary learning).  We also append to the end of this cover letter point-by-point responses to the two detailed reviews.

First, we address the major concern that our submission is only a modest contribution mathematically.  This is actually a significant applied mathematical result.  and is is not simply a generalization of the noise-free results.  we have significant brand new results, including Problem 2, the optimization formulation of the sparse coding problem, which is the one of most interest to optimization theorist and practitioners of their methods. in the sense that we do generalize the noise-free result, it is a significantly more difficult endeavor with noise, and our results in this regard provide more tools for scientists to understand the theoretical implications of their sparse liner coding setups.  Moreover, it is a very clean and usable statement of theorems and results, with minimal possible assumptions.  Theorem 1 is Generalization, but our novel combinatorial proof techniques that give more powerful conclusions: 
reduction in number samples to determine model.  full spark condition not needed.  m'>m.  in certain settings, polynomial amount of data to determine the model mathematical domain for sparsity and combinatorics we did the hard work of finding right tools for noisy dictionary learning theory, complete self-contained treatment.

Another concern about our manuscript is that computational relevance is lacking.  we add new material on computational complexity considerations, including fully polynomial result in certain cases.  all our constants are explicit.  we are dealing with NP-hard situation, we should expect difficult -- in fact, the NP-hardness of dictionary learning with noise.  Tillman: the expert on this says that Problem 1's computational complexity: https://arxiv.org/pdf/1405.6664.pdf.  Maybe we can solve Tillman's problem The remaining very difficult problem is determining the smallest number of samples that determine the model.  but we think we have made progress on this and brought non-trivial, but elegant, tools to bear on the problem.

Finally, we address how our results impact model consistency in data analysis as well as their implications for science, more generally.  Occam's razor -- Problem 1, basic move in lots of data science "dictionary learning" (278,000), dictionary learning (154,000,000) the central (NP-Hard) problem of dictionary learning approximated using L1-optimization.  "all interesting problems are NP-hard". in fact, there has never been an experiment with zero noise, so the results of [Hillar Sommer] affect zero scientists. new theorem on optimization formulation of the result -- that's how scientists approach their data completes the theory of NIPS ACS making viable actual result in theory of mind -- theories have to be resilient to noise or their nothing applies to thousands of scientific algorithmic theory of sparse linear coding -- removes one step for them to write algorithm papers universals in perception -- with noise (as all datasets have noise) lends further understanding to the basic problem of universal coding from sensors to cortex.  applied optimization theory: Hadamard well-posed, sparse linear coding with learning has a natural consistency, versus Deep Learning non-uniqueness of models. big win for sparse coding as a model: theoretical grounding for what you are doing.  NSF Grant () just recently proved with this result as the project's theoretical underpinnings.