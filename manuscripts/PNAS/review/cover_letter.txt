-------------------------------------------
Charles J. Garfinkle
Christopher J. Hillar
Redwood Center for Theoretical Neuroscience
University of California, Berkeley
-------------------------------------------


Dear editors at PNAS,

We greatly appreciate the feedback from the handling editor and two referees concerning our manuscript, and we have modified the paper to reflect all of their valuable suggestions and concerns.  As the major issue is not one of technical correctness but rather importance, in this cover letter we argue for the mathematical and practical significance of our work and its deep ramifications for many fields, including universality in neuroscience and signal processing as well as its unification of hundreds (if not thousands) of publications on the topic of unsupervised sparse linear coding (SLC), i.e. dictionary learning.  We also append to the end of this letter detailed point-by-point responses to the two reviews.

First, we address a major concern that our submission may represent an incremental advance over [Hillar-Sommer-2015] by way of similar techniques. In fact, among the several new results we provide is the first-ever (to our knowledge) solution to Problem 2 — the optimization problem of fundamental interest to theorists, numerical analysts, and practitioners of their methods in data science — which seeks a dictionary providing the smallest average support size over the training data. We demonstrate that, given enough data, this problem in fact reduces to an instance of Problem 1 to which one may then apply our major result: every dictionary and sequence of sparse codes solving Problem 1 are equivalent up natural symmetries and a discrepancy (error) that scales linearly in the non-zero measurement noise or modeling inaccuracy. This vast -- and essentially optimal up to constants -- generalization of [Hillar-Sommer-2015] to the noisy case represents a significant endeavor in its own right, which necessitated the development of powerful, practical tools that will allow researchers to organize and understand better the theoretical implications of their different dictionary learning platforms. For example, one new innovation in our work is a theory of combinatorial designs for support sets (the "singleton intersection property") that provides a handle on the complexities of intersecting subspaces in dictionary learning problems. Another is a fundamental lemma in matrix theory (Lem. 1) that will be useful in many other contexts where uniqueness up to feature relabelling and scaling in noisy settings is desirable. To give further evidence for how our novel combined combinatorial and analytical proof allows for more powerful conclusions in this generalization of [Hillar-Sommer-2015], consider the following additional theoretical advances: 1) latent dictionary dimension can be unknown (m'>m), 2) an exponential reduction in sample complexity N, 3) weaker spark condition assumption on the generating dictionary, and 4) practical amounts of data determine the model in certain settings.

Another major technical issue with our manuscript appearing in the reviews was that computational and practical relevance is lacking.  To address this, we have added new material on complexity considerations, including statements that are fully-polynomial in certain cases.  In addition, we should mention that, unlike a number of mathematical treatments involving dictionary learning, the constants in our theorems are explicit and effectively computable.  However, as we are dealing with problems in a (believed to be) intractable NP-hard situation, we should expect difficulties regarding the computational complexity of determining constants and the minimal numbers of samples that determine a model.  Indeed, the computational complexity of Problem 1 is still a fundamental open problem in the field [Tillman, https://arxiv.org/pdf/1405.6664.pdf].  Nevertheless, in this very challenging setting, we have made significant progress.  For instance, consider the practical problem of designing algorithms that provably recover model parameters in sparse linear coding.  In each such research endeavor of the many that we have followed, correctness of algorithm and consistency of model are proved together, usually in ad-hoc ways.  What we have done here is to effectively half the theoretical work of these efforts by determining clean conditions under which sparse linear coding "up to the noise" automatically guarantees uniqueness up to (unavoidable) relabelling and scaling ambiguities.  This theoretical savings will greatly simplify mathematical proofs and, moreover, detaches the problem of model consistency (sparse linear coding up to noise is "unique") with that of finding an underlying set of parameters using an algorithm (e.g., using Expectation-Maximization, ICA, etc.) that eventually converges on a solution.

Finally, we address how our results significantly impact data analysis and science, more broadly.  In the last several decades, from JPEG's dominance in image compression to the discovery of intrinsic "Gabors" in natural images to detecting painting forgeries to artificial "cat detecting neurons" in machine intelligence, solutions to the problem of dictionary learning have become standard tools in signal analysis, allowing for the unsupervised discovery of myriad insights about natural data and its interactions with science research (e.g., neuroscience).  A compelling feature of the SLC problem is its elegant formulation using the principle of Occam's razor.  For example, a natural video or sound can be modeled, quite accurately, as a linear combination of a small number of spatiotemporal "basis functions", each representing an archetypical feature latent in the data.  Our work conclusively demonstrates that this parsimonious model of (noisy) natural data is well-posed in the sense of Hadamard, a classical yet still fundamental property of models.  In particular, all data scientists, using very different methods, will find the same latent parameters for modeling datasets arising from the same noisy statistical system.  Fundamental to this general fact is that small errors in measurement of the system do not alter the unique latent sparse representations inherent in the data.  To contrast with the current hot topic of "Deep Learning", we note that there are no such uniqueness guarantees concerning these multi-layer feedforward models of data.  In particular, it appears that a main reason for the sustained interest in unsupervised dictionary learning is this well-posedness of the model, which represents the heart of our theoretical findings. 

We look forward to your response and thank you for consideration of our manuscript.


Sincerely,


Charles J. Garfinkle
Christopher J. Hillar
