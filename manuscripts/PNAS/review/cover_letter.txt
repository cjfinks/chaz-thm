Charles J. Garfinkle
Christopher J. Hillar
Redwood Center for Theoretical Neuroscience
University of California, Berkeley
-------------------------------------------

Dear editors at PNAS,

We greatly appreciate the feedback from the handling editor and two referees concerning our manuscript, and we have modified it to reflect all their valuable suggestions and concerns.  As the major issue is not one of technical correctness but rather importance, in this cover letter we argue for the mathematical and practical significance of our work and its deep ramifications for many fields, including universality in neuroscience and signal processing as well as its unification of hundreds (if not thousands) of publications on the topic of unsupervised sparse linear coding (SLC), i.e. dictionary learning.  We also append to the end of this letter detailed point-by-point responses to the two reviews.

First, we address a major concern that our submission may represent an incremental advance over [Hillar-Sommer-2015] by way of similar techniques. In fact, among the several new results we provide is the first-ever (to our knowledge) solution to Problem 2 — the optimization problem of fundamental interest to theorists, numerical analysts, and practitioners of their methods in data science — which seeks a dictionary providing the smallest average support size over the training data. We demonstrate that, given enough data, this problem in fact reduces to an instance of Problem 1 to which one may then apply our major result: every dictionary and sequence of sparse codes solving Problem 1 are equivalent up natural symmetries and a discrepancy (error) that scales linearly in the non-zero measurement noise or modeling inaccuracy. This vast -- and essentially optimal up to constants -- generalization of [Hillar-Sommer-2015] to the noisy case represents a significant endeavor in its own right, which necessitated the development of powerful, practical tools that will allow researchers to organize and understand better the theoretical implications of their different dictionary learning platforms. For example, one new innovation in our work is a theory of combinatorial designs for support sets (the “singleton intersection property”) that provides a handle on the complexities of intersecting subspaces in dictionary learning problems. Another is a fundamental lemma in matrix theory (Lem. 1) that will be useful in many other contexts where uniqueness up to feature relabelling and scaling in noisy settings is desirable. To give further evidence for how our novel combined combinatorial and analytical proof allows for more powerful conclusions in this generalization of [Hillar-Sommer-2015], consider the following additional theoretical advances: 1) latent dictionary dimension can be unknown (m'>m), 2) an exponential reduction in sample complexity N, 3) weaker spark condition assumption on the generating dictionary A, and 4) practical amounts of data determine the model in certain settings.

We next address what we believe is the most significant referee objection regarding mathematical theory -- that our results are "not conclusive" vis-a-vis the dictionary learning problem. To be clear, the main unresolved question in our work is whether a polynomial amount of sparse samples are sufficient to determine model parameters (dictionary and sparse codes) in sparse linear coding. Our response is that this question is intimately related to P=NP, the central problem of computer science, even in the noiseless (exact) case. Indeed, the hard work has already been done by A. Tillman, an expert on the computational complexity of dictionary learning. Take the natural problem of deciding the spark condition of a given matrix A, an NP-hard problem [Tillman-2014]. 

CHRIS’ ARGUMENT: Consider now conjectures, C1: There is a polynomial number of (polynomial-time constructible, e.g. by a Vandermonde matrix) sparse samples determining the dictionary whenever the dictionary satisfies the spark condition, and C2: There is a polynomial-time algorithm that returns a spark condition satisfying dictionary that encodes a given dataset when such a dictionary exists, and otherwise quitting.  We claim the somewhat surprising (but trivial, given [Tillman-2014]) fact: C1 and C2 together imply P=NP. To see this, take any matrix A and multiply it with the sparse codes from C1 so that, in polynomial time, we can either find a B that satisfies the spark condition and codes this dataset or determine if not. In the latter case, A does not satisfy the spark condition. In the former, we consider the two possibilities: a) A satisfies the spark condition and b) A does not. In case a), B is a permutation/scaling of A (checkable in polynomial time), and in case b), B is not a permutation/scaling of A because otherwise A would satisfy the spark condition. It follows that C1 and C2 imply P=NP; in particular, it is unlikely that both C1 and C2 are true.

CHAZ’ ARGUMENT: Take the natural problem of deciding the spark condition of a given matrix A, an NP-hard problem [Tillman-2014]. Consider now conjectures, C1: There is a polynomial number of (polynomial-time constructible, e.g. using a Vandermonde matrix) sparse samples determining (up to permutation/scaling) the dictionary whenever the dictionary satisfies the spark condition, and C2: There is a polynomial-time algorithm that decides if a dataset admits a dictionary satisfying the spark condition. We claim the somewhat surprising (but trivial, given [Tillman-2014]) fact: C1 and C2 together imply P=NP. To see this, take any matrix A and generate data by mapping the sparse codes from C1 through A. In polynomial time, the algorithm decides if a dictionary that satisfies the spark condition exists for this dataset. If yes, then A satisfies the spark condition since by C1 it is some permutation/scaling of a dictionary that satisfies the spark condition. Therefore, C1 and C2 imply P=NP; in particular, it is unlikely that both C1 and C2 are true.

Another major technical issue with our manuscript from the referees was that computational and practical relevance is lacking. To address this, we have added new material on complexity considerations, including statements that are fully-polynomial in certain cases.  In addition, we should mention that, unlike a number of mathematical treatments involving dictionary learning, the constants in our theorems are explicit and effectively computable.  However, as we are dealing with problems in a (believed to be) intractable NP-hard situation, we should expect difficulties regarding the computational complexity of determining constants and the minimal numbers of samples that determine a model.  Indeed, , as we have formulated it here, to be a fundamental open problem in the field (https://arxiv.org/pdf/1405.6664.pdf).  Nevertheless, in this very challenging setting, we have made significant progress.  For instance, consider the practical problem of designing algorithms that provably recover model parameters in sparse linear coding.  In each such research endeavor of the many that we have followed, correctness of algorithm and consistency of model are proved together, usually in ad-hoc ways.  What we have done here is to effectively half the theoretical work of these efforts by determining clean conditions under which sparse linear coding "up to the noise" automatically guarantees uniqueness up to (unavoidable) relabelling and scaling ambiguities.  This theoretical savings will greatly simplify mathematical proofs and, moreover, detaches the problem of model consistency (sparse linear coding up to noise is "unique") with that of finding an underlying set of parameters (Expectation-Maximization, ICA, etc.) that converges.

Finally, we address how our results impact data analysis and science, more broadly.  In the last several decades, from JPEG's dominance in image compression to decoding fMRI using subsampled signals to detecting painting forgeries to artificial "cat detecting neurons" in machine learning, dictionary learning has become a standard tool in signal analysis for the unsupervised understanding of natural data.  Its formulation naturally arises as an optimization problem, elegantly from the principle of Occam's razor; e.g., a natural sound is thought of as a linear combination of a small number of spatiotemporal "basis functions", each representing an archetypical feature latent in the data.  


-- Problem 1, basic move in lots of data science "dictionary learning" (278,000), the central (NP-Hard) problem of dictionary learning approximated using L1-optimization.  "all interesting problems are NP-hard". in fact, there has never been an experiment with zero noise, so the results of [Hillar Sommer] affect zero scientists. new theorem on optimization formulation of the result -- that's how scientists approach their data completes the theory of NIPS ACS making viable actual result in theory of mind -- theories have to be resilient to noise or their nothing applies to thousands of scientific algorithmic theory of sparse linear coding -- removes one step for them to write algorithm papers universals in perception -- with noise (as all datasets have noise) lends further understanding to the basic problem of universal coding from sensors to cortex.  applied optimization theory: Hadamard well-posed, sparse linear coding with learning has a natural consistency, versus Deep Learning non-uniqueness of models. big win for sparse coding as a model: theoretical grounding for what you are doing.  NSF Grant () just recently proved with this result as the project's theoretical underpinnings.
