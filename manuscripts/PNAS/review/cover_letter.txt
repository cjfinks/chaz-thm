Dear editors at PNAS,

We greatly appreciate the feedback from the editor and referees on our manuscript, and we have modified it to reflect all their valuable suggestions and concerns.  As the major issue is not of technical correctness but rather importance, in this cover letter we argue for the significance of our mathematical work and its deep interactions with many fields, including universality in neuroscience and signal processing, as well as its unification of hundreds (if not thousands) of publications on the topic of unsupervised sparse linear coding, i.e. dictionary learning.  We also append to the end of this letter detailed point-by-point responses to the two reviews.

First, we address a major concern that our submission represents a minor mathematical generalization of [Hillar-Sommer-2015], and that it uses the same techniques.  In fact, our submission contains many significant new results, including a solution to Problem 2, which is the optimization formulation of uniqueness in sparse coding and of fundamental interest to optimization theorists and practitioners of their methods.  In the sense that we do generalize the noise-free result, it is a significantly more difficult endeavor with noise, and our results in this regard provide more tools for scientists to understand the theoretical implications of their sparse liner coding setups.  Our Theorem 1, uniqueness in sparse linear coding up to the measurement noise, many new concepts and proof methods were required.  Moreover, it is a very clean and usable statement, with minimal possible assumptions and great generality.  To give evidence for these statements, consider how our novel combinatorial and analytical proof gives more powerful conclusions: 1) reduction in number samples to determine model, 2) full spark condition not needed, 3) unknown dictionary dimension (m'>m), 4) in certain settings, polynomial amount of data to determine the model mathematical domain for sparsity.

Another concern about our manuscript is that computational relevance is lacking.  we add new material on computational complexity considerations, including fully polynomial result in certain cases.  all our constants are explicit.  we are dealing with NP-hard situation, we should expect difficult -- in fact, the NP-hardness of dictionary learning with noise.  Tillman: the expert on this says that Problem 1's computational complexity: https://arxiv.org/pdf/1405.6664.pdf.  Maybe we can solve Tillman's problem The remaining very difficult problem is determining the smallest number of samples that determine the model.  but we think we have made progress on this and brought non-trivial, but elegant, tools to bear on the problem.

Finally, we address how our results impact model consistency in data analysis as well as their implications for science, more generally.  Occam's razor -- Problem 1, basic move in lots of data science "dictionary learning" (278,000), the central (NP-Hard) problem of dictionary learning approximated using L1-optimization.  "all interesting problems are NP-hard". in fact, there has never been an experiment with zero noise, so the results of [Hillar Sommer] affect zero scientists. new theorem on optimization formulation of the result -- that's how scientists approach their data completes the theory of NIPS ACS making viable actual result in theory of mind -- theories have to be resilient to noise or their nothing applies to thousands of scientific algorithmic theory of sparse linear coding -- removes one step for them to write algorithm papers universals in perception -- with noise (as all datasets have noise) lends further understanding to the basic problem of universal coding from sensors to cortex.  applied optimization theory: Hadamard well-posed, sparse linear coding with learning has a natural consistency, versus Deep Learning non-uniqueness of models. big win for sparse coding as a model: theoretical grounding for what you are doing.  NSF Grant () just recently proved with this result as the project's theoretical underpinnings.
