-------------------------------------------
Charles J. Garfinkle
Christopher J. Hillar
Redwood Center for Theoretical Neuroscience
University of California, Berkeley
-------------------------------------------


Dear editors at PNAS,

We greatly appreciate the feedback from the handling editor and two referees concerning our manuscript, and we have modified the paper to reflect all of their valuable suggestions and concerns.  As the major issue is not one of technical correctness but rather importance, in this cover letter we argue for the mathematical and practical significance of our work which unifies hundreds (if not thousands) of publications on the topic of inverse problems in sparse linear coding (i.e. dictionary learning), a central model in modern neuroscience and signal processing; and we discuss its ramifications for these fields. We also append to the end of this letter detailed point-by-point responses to the two reviews.

<<<<<<< HEAD
First, we address a major concern that our submission may represent an incremental advance by way of similar techniques over the noiseless results of [HS15]. In fact, among the several new results in our manuscript are the first-ever (even in the noiseless case, to our knowledge) uniqueness conditions for solutions of Problem 2 — the optimization formulation for dictionary learning of fundamental interest to theorists, numerical analysts, and practitioners of their methods in data science — which seeks a dictionary with respect to which training data can be encoded with the smallest average support size. We demonstrate rigorously that, given enough data, this problem reduces to an instance of Problem 1 to which one may then apply our main result: every dictionary and sparse codes solving Problem 1 are equivalent up to the inherent relabeling/scaling ambiguities and a discrepancy (error) that scales linearly with the measurement noise or modeling inaccuracy. This vast -- and essentially optimal up to constants -- generalization of [HS15] to the noisy case represents a significant endeavor in its own right, which necessitated the development of powerful tools that will allow researchers to organize and understand better the theoretical properties of various dictionary learning schemes. For example, one of the major innovations in our work is a theory of combinatorial designs for support sets (the "singleton intersection property") that provides a handle on the complexities of intersecting subspaces in dictionary learning problems. We incorporate this discovery into a new fundamental lemma in matrix theory (Lem. 1) that will be useful in any context where uniqueness up to feature relabelling and scaling in noisy settings is desirable; this discovery also inspires the definition of a new restricted isometry property (matrix lower bound) restricted only to those supports in a given hypergraph. To give further evidence for how our novel combined combinatorial and analytical proof allows for more powerful conclusions in this generalization of [HS15], consider the following new theoretical advances: 1) the number of dictionary elements can be unknown, 2) data require a polynomial, not exponential, number of distinct sparse supports and in certain setting a practical (polynomial) number of samples in total is sufficient, 3) the spark condition is not necessary for recovery of the dictionary. 

Another major issue stated in the reviews was that discussion of the computational and practical relevance of our results was found lacking. To address these concerns, we have added new material on complexity considerations. Note also that, unlike a number of mathematical treatments involving dictionary learning, the constants in our theorems are explicit, effectively computable, and deterministic; i.e., they are not particular to any assumed distribution from which the coefficients of sparse vectors are drawn. Consider now the practical problem of designing algorithms that provably recover model parameters in sparse linear coding. In each such research endeavor of the many that we have followed, correctness of algorithm and consistency of model are proved together, usually in ad-hoc ways. Typically, theorists go through an optimization procedure (usually with the L_1-norm in place of the L_0-norm) with convergence guarantees and then zero all but the top k most significant components in the returned vectors x_i. What we have done here is effectively halve the theoretical work of these efforts by determining clean, minimally restrictive conditions under which the original sparse linear coding automatically guarantees uniqueness up to (unavoidable) relabelling/scaling ambiguities and an error commensurate with the noise. This theoretical savings will greatly simplify mathematical proofs by detaching the problem of model consistency (sparse linear coding is "unique" up to noise) with that of finding an underlying set of parameters using an algorithm (e.g., using Expectation-Maximization, ICA, etc.) that eventually converges on a solution. Secondly, we outline an effective procedure which can confirm if one’s proposed solution to Prob. 1 or Prob. 2 is indeed unique and have updated the manuscript with an explicit statement of this fact.

Finally, we address how our results significantly impact data analysis and science, more broadly. In the last several decades, from JPEG's dominance in image compression to the discovery of intrinsic "Gabors" in natural images to detecting painting forgeries to artificial "cat detecting neurons" in machine intelligence, sparse linear coding has become a standard tool in signal analysis, allowing for the unsupervised discovery of myriad insights about natural data and its interactions with science research (e.g., neuroscience).  A compelling feature of the dictionary learning problem is its elegant derivation from the principle of Occam's razor. For example, a natural video or sound can be modeled, quite accurately, as a linear combination of a small number of spatiotemporal "basis functions", each representing an archetypical feature latent in the data. Our work conclusively demonstrates that this parsimonious model of (noisy) natural data is well-posed in the sense of Hadamard, a long-standing necessary property of reductionist models in science. In particular, all data scientists, using very different methods, will find the same latent parameters for modeling datasets arising from the same noisy statistical system. Fundamental to this general fact is that small errors in measurement of the system do not alter the unique latent sparse representations inherent in the data. To contrast with the current hot topic of "Deep Learning", we note that there are no such guarantees concerning these multi-layer feedforward models of data. In particular, it appears that a main reason for the sustained interest in unsupervised dictionary learning for data analysis has been the assumed well-posedness of the model, confirmation of which forms the core of our theoretical findings.  As a final application to neuroscience, we mention that our main results certify the validity of the only published theory of neurally-plausible bottleneck communication between brain regions -- unsupervised feature (dictionary) learning in a significantly smaller space of nervous activity can recover noisy sparse codes sent through a randomly-constructed (but unknown) wiring bottleneck.  See [Ganguli-Sompolinsky-2012; Annual review of neuroscience] for a discussion of this application's significance (in the noiseless case) and more on the interactions of sparse linear coding with neuroscience theory.
=======
First, we address a major concern that our submission may represent an incremental advance by way of similar techniques over the noiseless results of (HS15). In fact, among the several new results in our manuscript are the first-ever (even in the noiseless case, to our knowledge) uniqueness conditions for solutions of Problem 2 — the optimization formulation of dictionary learning of fundamental interest to theorists, numerical analysts, and practitioners of their methods in data science — which seeks a dictionary with respect to which training data can be encoded with the smallest average support size. We demonstrate rigorously that, given sufficient data and suitable dictionary, Problem 2 reduces to an instance of Problem 1 to which one may then apply our main result (Theorem 1): every dictionary and sparse codes solving Problem 1 are equivalent up to the inherent relabeling/scaling ambiguities and a discrepancy (error) that scales linearly with the measurement noise or modeling inaccuracy. This vast -- and essentially optimal up to constants -- generalization of (HS15) to the noisy case represents a significant endeavor in its own right, which necessitated the development of powerful tools that will allow researchers to organize and understand better the theoretical properties of various dictionary learning schemes. For example, one of the major innovations in our work is a theory of combinatorial designs for support sets (the "singleton intersection property") that provides a handle on the complexities of intersecting subspaces in dictionary learning problems. We incorporate these ideas in a new fundamental lemma in matrix theory (Lem. 1) that will be useful in any noisy setting where uniqueness up to feature relabelling/scaling is desirable.  Also inspired by these methods is the definition of a new restricted isometry property (matrix lower bound) restricted only to those supports in a given hypergraph. To give further evidence for how our novel combined combinatorial and analytical proof allows for more powerful conclusions in this generalization of (HS15), consider the following new theoretical advances: 1) the number of dictionary elements can be unknown; 2) data require a polynomial, not exponential, number of distinct sparse supports and in certain settings a practical (polynomial) number of samples in total is sufficient; and 3) the spark condition is not necessary for recovery of the dictionary. 

Another major issue stated in the reviews was that discussion of the computational and practical relevance of our results was found lacking. To address these concerns, we have added new material on complexity considerations. Note also that, unlike a number of mathematical treatments involving dictionary learning, the constants in our theorems are explicit, effectively computable, they are not particular to any assumed distribution from which the coefficients of sparse vectors are drawn. Consider now the practical problem of designing algorithms that provably recover model parameters in sparse linear coding. In each such research endeavor of the many that we have followed, correctness of algorithm and consistency of model are proved together, usually in ad-hoc ways. Typically, theorists go through an optimization procedure (usually with the L_1-norm as a regularizer in place of the L_0-norm) with convergence guarantees and then zero all but the top k most significant components in the returned vectors x_i. What we have done here is to halve effectively the theoretical work of these efforts by determining clean, minimally restrictive conditions for detaching the problem of model consistency (sparse linear coding is "unique" up to noise) with that of finding an underlying set of parameters using a numerical algorithm (e.g., using Expectation-Maximization, ICA, etc.) that eventually converges on a solution.  In addition to theoretical savings, we also outline an effective procedure that can confirm if one’s proposed solution to Prob. 1 or Prob. 2 is indeed unique. We have updated the manuscript with an explicit statement of this fact.

Finally, we address how our results significantly impact data analysis and science, more broadly. In the last several decades, from JPEG's dominance in image compression to the discovery of intrinsic "Gabors" in natural images to detecting painting forgeries to artificial "cat detecting neurons" in machine intelligence, sparse linear coding has become a standard tool in signal analysis, allowing for the unsupervised discovery of myriad insights about natural data and its interactions with science research (e.g., neuroscience).  A compelling feature of the dictionary learning problem is its elegant derivation from the principle of Occam's razor. For example, a natural video or sound can be modeled, quite accurately, as a linear combination of a small number of spatiotemporal "basis functions", each representing an archetypical feature latent in the data. Our work conclusively demonstrates that this parsimonious model of (noisy) natural data is well-posed in the sense of Hadamard, a long-standing necessary property of reductionist models in science. In particular, all data scientists, using very different methods, will find the same latent parameters for modeling datasets arising from the same noisy statistical system. Fundamental to this general fact is that small errors in measurement of the system do not alter the unique latent sparse representations inherent in the data. To contrast with the current hot topic of "Deep Learning", we note that there are few such uniqueness guarantees for these multi-layer feedforward models of data. In particular, it appears that a main reason for the sustained interest in unsupervised (sparse) dictionary learning for data analysis has been the assumed well-posedness of the model, confirmation of which forms the core of our theoretical findings.  As a final application to neuroscience, we mention that our main results certify the validity of the only published theory of neurally-plausible bottleneck communication between brain regions -- unsupervised feature (dictionary) learning in a significantly smaller space of nervous activity can recover noisy sparse codes sent through a randomly-constructed (but unknown) wiring bottleneck.  See (GS12), published in the Annual Review of Neuroscience, for a discussion of this application's significance (in the noiseless case) and more on the interactions of sparse linear coding with neuroscience theory.
>>>>>>> 0f43e20ed6aedbadaa5ace053bbe4755f5b7284b

We look forward to your response and thank you for consideration of our manuscript.


Sincerely,

Charles J. Garfinkle
Christopher J. Hillar
