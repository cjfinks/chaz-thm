Charles J. Garfinkle
Christopher J. Hillar
Redwood Center for Theoretical Neuroscience
University of California, Berkeley
-------------------------------------------

Dear editors at PNAS,

We greatly appreciate the feedback from the handling editor and two referees concerning our manuscript, and we have modified it to reflect all their valuable suggestions and concerns.  As the major issue is not one of technical correctness but rather importance, in this cover letter we argue for the mathematical and practical significance of our work and its deep ramifications for many fields, including universality in neuroscience and signal processing as well as its unification of hundreds (if not thousands) of publications on the topic of unsupervised sparse linear coding (SLC), i.e. dictionary learning.  We also append to the end of this letter detailed point-by-point responses to the two reviews.

First, we address a major concern that our submission represents a minor mathematical generalization of [Hillar-Sommer-2015] and that it uses the same techniques.  In fact, our submission contains many significant new results, including a solution to Problem 2, which is the optimization formulation of uniqueness in noisy dictionary learning and of fundamental interest to theorists, numerical analysts, and practitioners of their methods in data science.  Of course, another such major result is a rigorous demonstration of uniqueness in sparse linear coding up to a discrepancy (error) that scales linearly in the non-zero measurement noise or modeling inaccuracy (with minimal assumptions).  This vast -- and essentially optimal up to constants -- generalization of [Hillar-Sommer-2015] represents a significant endeavor in its own right, which necessitated the development of powerful, practical tools that will allow researchers to organize and understand better the theoretical implications of their different dictionary learning platforms.  For example, one new innovation in our work is a theory of combinatorial designs for support sets that provides a handle on the complexities of intersecting subspaces in dictionary learning problems.  Another is a fundamental lemma on noisy sparse matrix factorization that will be useful in many other contexts where uniqueness up to feature relabelling and scaling is desirable.  To give further evidence for how our novel combined combinatorial and analytical proof allows for more powerful conclusions in this generalization of [Hillar-Sommer-2015], consider the following additional theoretical advances: 1) Latent dictionary dimension can be unknown (m'>m), 2) reduction in number of samples to determine the model, 3) weaker spark condition assumption on the dictionary, and 4) practical amounts of data determine the model in certain settings.

We next address what we believe is the most significant referee objection regarding mathematical theory -- that our results are "not conclusive" vis-a-vis the dictionary learning problem.  To be clear, the main unresolved question in our work is whether a polynomial amount of (polynomial-time constructible) sparse samples are sufficient to determine model parameters (dictionary and sparse codes) in sparse linear coding.  Our response is that this question is intimately related to P=NP, the central problem of computer science, even in the noiseless (exact) case.  Indeed, the hard work has already been done by A. Tillman, an expert on the computational complexity of dictionary learning.  Take the natural problem of deciding the spark condition of a given matrix A, an NP-hard problem [Tillman-2014].  Consider now conjectures, C1: There is a polynomial number of sparse samples determining the SLC model (whenever a generating matrix satisfies the spark condition), and C2: There is a polynomial-time algorithm determining a spark condition satisfying dictionary from data when such a dictionary exists and otherwise quitting.  We claim the somewhat surprising (but trivial, given [Tillman-2014]) fact: C1 and C2 together imply P=NP.  To see this, take any matrix A and multiply it with the sparse codes from C1 so that in polynomial time, we can find a spark condition satisfying B coding this dataset or determine if not.  In the latter case, A does not satisfy the spark condition.  In the former, we consider the two possibilities: a) A satisfies the spark condition and b) A does not.  In case a), B is a permutation/scaling of A (checkable in polynomial time), and in case b), B is not because otherwise A would satisfy the spark condition.    It follows that C1 and C2 imply P=NP; in particular, it is unlikely that both C1 and C2 are true.

Another major technical issue with our manuscript from the referees was that computational and practical relevance is lacking.  To address this, we have added new material on complexity considerations, including statements that are fully-polynomial in certain cases.  In addition, we should mention that, unlike a number of mathematical treatments involving dictionary learning, the constants in our theorems are explicit and effectively computable.  However, as we are dealing with problems in a (believed to be) intractable NP-hard situation, we should expect difficulties regarding the computational complexity of determining constants and the minimal numbers of samples that determine a model.  Indeed, , as we have formulated it here, to be a fundamental open problem in the field (https://arxiv.org/pdf/1405.6664.pdf).  Nevertheless, in this very challenging setting, we have made significant progress.  For instance, consider the practical problem of designing algorithms that provably recover model parameters in sparse linear coding.  In each such research endeavor of the many that we have followed, correctness of algorithm and consistency of model are proved together, usually in ad-hoc ways.  What we have done here is to effectively half the theoretical work of these efforts by determining clean conditions under which sparse linear coding "up to the noise" automatically guarantees uniqueness up to (unavoidable) relabelling and scaling ambiguities.  This theoretical savings will greatly simplify mathematical proofs and, moreover, detaches the problem of model consistency (sparse linear coding up to noise is "unique") with that of finding an underlying set of parameters (Expectation-Maximization, ICA, etc.) that converges.

Finally, we address how our results impact data analysis and science, more broadly.  In the last several decades, from JPEG's dominance in image compression to decoding fMRI using subsampled signals to detecting painting forgeries to artificial "cat detecting neurons" in machine learning, dictionary learning has become a standard tool in signal analysis for the unsupervised understanding of natural data.  Its formulation naturally arises as an optimization problem, elegantly from the principle of Occam's razor; e.g., a natural sound is thought of as a linear combination of a small number of spatiotemporal "basis functions", each representing an archetypical feature latent in the data.  


-- Problem 1, basic move in lots of data science "dictionary learning" (278,000), the central (NP-Hard) problem of dictionary learning approximated using L1-optimization.  "all interesting problems are NP-hard". in fact, there has never been an experiment with zero noise, so the results of [Hillar Sommer] affect zero scientists. new theorem on optimization formulation of the result -- that's how scientists approach their data completes the theory of NIPS ACS making viable actual result in theory of mind -- theories have to be resilient to noise or their nothing applies to thousands of scientific algorithmic theory of sparse linear coding -- removes one step for them to write algorithm papers universals in perception -- with noise (as all datasets have noise) lends further understanding to the basic problem of universal coding from sensors to cortex.  applied optimization theory: Hadamard well-posed, sparse linear coding with learning has a natural consistency, versus Deep Learning non-uniqueness of models. big win for sparse coding as a model: theoretical grounding for what you are doing.  NSF Grant () just recently proved with this result as the project's theoretical underpinnings.
