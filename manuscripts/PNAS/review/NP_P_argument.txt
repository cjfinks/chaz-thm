We next address what we believe is the most significant referee objection regarding mathematical theory -- that our results are "not conclusive" vis-a-vis the dictionary learning problem.  To be clear, the main unresolved question in our work is whether a polynomial amount of (polynomial-time constructible) sparse samples are sufficient to determine model parameters (dictionary and sparse codes) in sparse linear coding.  Our response is that this question is likely intimately related to P=NP, the central problem of computer science, even in the noiseless (exact) case, as we now sketch.  Indeed, the hard work has already been done by A. Tillman, an expert on the computational complexity of dictionary learning.  Take the natural problem of deciding the spark condition of a given matrix A, an NP-hard problem [Tillman-2014]. Consider now conjectures, C1: There is a polynomial number of (polynomial-time constructible, e.g. using a Vandermonde matrix) sparse samples determining (up to permutation/scaling) the dictionary whenever the dictionary satisfies the spark condition, and C2: There is a polynomial-time algorithm that decides if a dataset admits a dictionary satisfying the spark condition. We claim the somewhat surprising (but trivial, given [Tillman-2014]) fact: C1 and C2 together imply P=NP. To see this, take any matrix A and generate data by mapping the sparse codes from C1 through A. In polynomial time, the algorithm decides if a dictionary that satisfies the spark condition exists for this dataset. If yes, then A satisfies the spark condition since by C1, it is some permutation/scaling of a dictionary that satisfies the spark condition. Therefore, C1 and C2 imply P=NP; in particular, it is unlikely that both C1 and C2 are true.