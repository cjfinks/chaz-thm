TODO:

References for Cover Letter and Point-by-point Response:

(IHS10) Isely, Hillar, Sommer, Advances in neural information processing systems, 2010.
(GS12) Ganguli, Sompolinksi, Annual review of neuroscience, 2012.
(TP14) Tillman, Pfetsch, IEEE Transactions on information processing, 2014.
(HS15) Hillar, Sommer, IEEE Transactions on information processing, 2015.
(M10) Morris I (2010), Advances in Mathematics 225(6):3425–3445.

-----------------
Reviewer Comments:

Reviewer #1:

Comments :
SUMMARY OF THE PAPER

The paper finds conditions under which solutions for the sparse linear coding problem are unique. In sparse linear coding, one is given a matrix Y and the problem is to find a dictionary A and sparse coefficient matrix X such that Y=AX, up to a bounded uncertainty. This paper finds conditions under which A and X are unique up to scalings and permutation. Previous works looked at the noise-free case; this work considers the case of "noisy" measurements and proves a stability condition.

GENERAL COMMENTS


Overall, while I think that the manuscript makes a solid contribution, I believe is better suited to a more specialized journal. Unlike many journals, PNAS gives a criterion for significance: that the paper be within the top 10% of publications and should be of interest to a general scientific audience. Without compelling evidence to the contrary from the authors, it seems to me that the submission does not rise to that level.
-----------------

RESPONSE: We have included in our revised manuscript a more compelling exposition of the significance of our results and their relevance to the general scientific audience, and we thank the editor for the opportunity to address this concern shared by both reviewers.

-----------------
Since the result is an extension of the noise-free result of (HS15) to a noisy case, the authors should provide betzer justification for the significance of contributions. The main result seems to be based on an approach similar to (HS15), and there is little to no discussion of the computability and tightness of the sufficient condition for stability in the main results. Thus, the submission does not seem to be greatly more valuable than (HS15) and also not conclusive regarding the central problems of sparse linear coding and dictionary learning. That being said, the paper (with the modifications below) would certainly merit publication in very solid journal like IEEE Transactions on Information Theory, where it would be a median paper, in my opinion.
-----------------

RESPONSE:

We agree with the reviewer that a straightforward extension of the approach and noise-free result of (HS15) to the noisy case — while still a significant advancement in applied mathematics with actual practical implications — would not necessarily merit publication in PNAS on its own. The reviewer, however, seems to have overlooked the fact that our main result (Thm. 1) generalizes the noise-free result of (HS15) far beyond a simple extension to the noisy case in ways which provide valuable insight into the dictionary learning model and that a significant deviation from the approach taken in (HS15) was necessary to prove these new facts. Moreover, our solution (Cor. 2) to the optimization formulation of the dictionary learning problem (Prob. 2) — that of most interest to optimization theorists and practitioners of their methods — is (to our knowledge) the first of its kind in both the noise-free and noisy domains. We discuss all of these points in more detail in our response to the reviewer’s later "detailed comments". 

We have included in our updated manuscript a discussion of the computability of the sufficient conditions (they are, indeed, computable), as per the reviewer’s request. The main constants in our work -- C_1, C_2 -- are computable, elementarily, in terms of two positive quantities that are natural in our context: (a) the "Lower Bound" L_H(A) of a matrix A given a hypergraph H (the support sets of sparse x's generating the data — see *NOTE* below this paragraph), which is a measure of how distinguishable two x’s with supports in H are by their images under A, and (b) the value in Def. 4 of r(V_1,...,V_l) given subspaces V_i spanned by columns of A, which bounds the distance between intersections of such subspaces and each subspace individually. The first quantity (a) is, in some form or another, related to standard objects in the field and may be computed as the smallest singular value of a set of specific sub-matrices; in particular, for fixed sparsity constraint k on H, the lower bound L_H(A) is efficiently computable (polynomial time). Because deciding L_H(A) > 0 is already NP-hard (TP14), it seems likely that without fixing a sparsity level there is no efficient computation in general for (a). The calculation of (b) involves determining "canonical angles" between subspaces, which is usually reduced again to a numerical (efficient) singular value decomposition given the fixed subspaces.  However, upon close inspection, C_2 nonetheless requires an exponential number of instances of computing r. Thus, while our constants are effective (an algorithm exists to compute them), they are not necessarily polynomially-time computable.

*NOTE* We have introduced for reasons discussed below the notion of L_H(A) in our re-submission where previously (in our original submission) we had defined only L_k(A). 

Another natural question that arises is the mathematical "tightness" of result statements for the three types of quantities: (a) required coding error eps(delta_1, delta_2) sufficient to guarantee uniqueness in terms of desired dictionary delta_1 and sparse representation error delta_2, (b) the constants C_1 and C_2 involved in our sufficient linear bound on eps(delta_1, delta_2), and (c) the number of sufficient samples N guaranteeing uniqueness.  With respect to (a), even with fixed dictionary, the recovery of sparse representations up to an error that is linear in the noise is classically optimal (up to constants). Thus, the fact that we have a sufficient error in coding for uniqueness being linear in terms of both the desired dictionary and sparse vector recovery error is optimal.  Although optimal up to constants, our solution to Problem 1 involves constants C_1 and C_2, it would be interesting and relevant for practical applications to determine the minimal such constants. In the case of sparsity k=1, we have given an example showing optimality of constants in this case. Finally, with respect to (c), we regard this as a fundamental open problem in the field: how many sparsely generated samples determine the linear coding model?  (One reason to be skeptical that a polynomial suffice is again (TP14).)  However, there is some good news -- if k is fixed and we constrain sparse codes \hat{x}_i to come from a polynomial number of supports, then a polynomial number of samples N is enough to guarantee uniqueness. In particular, a sample complexity polynomial in (m,k) can be achieved by limiting the size of the support set of the \tilde x_i, and we have revised the manuscript to discuss this fact (which, incidentally, does not follow from the results of (HS15) in the noiseless case).

It is true, however, that we have not proven the tightness of these conditions and the manuscript is therefore not conclusive regarding the central problems of sparse linear coding. We believe that these sufficient conditions yield strong enough conclusions to stand on their own; for instance, in synchronizing device parameters for a particular sparsely representable sensory ensemble. Nonetheless, we have included in our revised manuscript some discussion of where the results can be tightened (e.g. Lem. 4).

-----------------
DETAILED COMMENTS

-- The paper does a good job of motivating the general problem of sparse coding to a general audience. In particular, there is a good review of the areas in which dictionary learning arises.

-- I think the biggest potential issue for publication in PNAS is that the significance of the result needs more discussion. Specifically, the authors could address why going from the noise-free case to the noisy case is not incremental. PNAS specifically targets submissions that go beyond what could be published in a more specialized journal. Could the authors, for example, contrast the proof technique between the noisy and noise-free cases? Or, perhaps, the practical implications?
-----------------

RESPONSE:

With all due respect to the reviewer, we believe the most compelling practical implication of a noisy result over a noiseless result is plainly evident: never has there been an experiment without noise and an unstable model is therefore useless in practice. We assume the reviewer must instead be concerned with what additional practical “take-away” message our manuscript contains.  One significant one is the outline of a procedure which can affirm if one’s proposed solution to Prob. 1 or Prob. 2 is indeed unique. We have updated the manuscript with an explicit statement of this fact, and outline one such procedure here:

Given a dictionary A and codes x_i that solve Problem 1 (e.g. learned by any algorithm), check that that they satisfy the assumptions of Thm. 1:
   - List the support sets of the x_i 
      - Discard those for which there are (k-1){m \choose k} or fewer x_i with that support.
      - Discard those for which the supported x_i are not in GLP. (This should also exclude any supports less than k in size.)
   - From the support sets that remain, list all SIP-satisfying hypergraphs that are subsets of this set
   - Discard those for which L_{2H}(A) = 0.
   - For each of the remaining hypergraphs, determine the constant C_1 from A and the x_i (or for every subset of the x_i of size (k-1){m \choose k})
   - Check that the derived upper bound on \varepsilon exceeds that of the original problem. If yes, the solution is “unique”.

More practical implications arise from our discovery of sufficient combinatorial designs for support sets of generating codes x_i, which have implications for polynomial time. Moreover, we have shown that a subset of dictionary elements are recoverable even if the number of dictionary elements in total is unknown; these observations are discussed in more detail below. 

The reviewer also raises the concern that, regardless of practical implications, our results may amount to an incremental advance over those of (HS15) by way of similar techniques. We disagree with this assessment on both counts: our main result (Thm. 1) goes far beyond a straightforward extension of that in (HS15) to the noisy case and this required a significant deviation from their approach.

It is understandable that the reviewer may have thought otherwise, considering how the proof of Thm. 1 is presented in the manuscript. As observed by our second reviewer, however, the extension to the noisy case did indeed require an novel combination of several results in the literature. Specifically, the main difficulty was to generalize Lemma 1 to the case where the k-dimensional subspaces spanned by corresponding (through the map \pi) submatrices of A and B are assumed only to be proximal (small ‘d’), and not identical as in (HS15). In contrast to the noiseless case, here it must be explicitly demonstrated that this proximity relation is propagated through the repeated intersections of these submatrix spans all the way down to the spans of dictionary elements themselves. We designed and proved Lemma 4 to address this issue, which draws its bound from the convergence guarantees of an alternating projections algorithm first proposed by von Neumann. This result, combined with a little known fact about the distance ‘d’ requiring proof in (M10), constitute the more sophisticated and obscure components of the deduction in eq.(26). To reiterate, this step is completely trivial in the noiseless case and required no mention for the inductive steps taken in (HS15).  

Our proof of Lemma 1 diverges perhaps even more significantly from the approach taken in (HS15) by way of Lemma 5. Key to our reduction of the sample complexity given in (HS15) by an exponential factor is the introduction of a combinatorial design (the "singleton intersection property") for support sets. Since in this case the map \pi from supports in the hypergraph to {[m] \choose k} is not surjective, one cannot apply the same inductive method as in (HS15), which freely chooses supports in the codomain to intersect at (k-1) nodes and map back to some corresponding (k-1)-sized intersection of supports in the domain. Instead, we demonstrate the surprising fact that by pigeonholing the images of supports in the (SIP-satisfying) hypergraph H one can still guarantee a bijection between the nodes and therefore the subspaces spanned by individual dictionary elements. We note that it was necessary to forgo inductive methods altogether in order to prove this fact for all hypergraphs satisfying the SIP; otherwise, we would require that the supports in the hypergraph have intersections of size k’ for every k’ < k (e.g. this is not the case for the small SIP example we give consisting of the rows and columns of nodes arranged in a square grid). Again, to our surprise, it so happens that this new induction-less argument easily generalizes to the case where B has an arbitrary number of columns, in which case we find that a one-to-one correspondence exists between a subset of columns of A and B of a size that has a nice closed-form expression for regular SIP hypergraphs.

To be clear, the new perspective we take on the problem yields the following powerful conclusions beyond those of a straightforward extension of (HS15) to the noisy case:

1) An extension to the case where the number of dictionary elements is unknown: The results of (HS15) only apply to the case where the matrix B has the same number of columns as A. We forgo this assumption and show that B must have at least as many columns as A and contains (up to noise) a subset of the columns of A. The size of this subset depends on a simple relation between the number of columns of B and the regularity of the support-set hypergraph.

2) A significant reduction in sample complexity: To identify the n x m generating dictionary A, (HS15) require that data be sampled from every k-dimensional subspace spanned by the m columns of A (that is, {m \choose k} subspaces in total). We show that the data need only be sampled from m subspaces in total (e.g. those supported by consecutive intervals of length k in some cyclic order on [m]) and in some cases as few as 2\sqrt{m} (e.g. when m = k^2, take the supports to be the rows and columns of [m] arranged in a square grid). Moreover, if the size of the support set of reconstructing codes is known to be polynomial in m and k, then the pigeonholing argument in the proof of Thm. 1 requires only a polynomial number of samples distributed over a polynomial number of supports; thus, N is polynomial in m, k. This point was only hinted at in the Discussion section of our original submission, but we have included it in the updated manuscript to make clear the power of our approach over that taken in (HS15). We are aware of only one work treating combinatorial hypergraph designs in the setting of dictionary learning, "An incidence geometry approach to dictionary learning" (arxiv, non-published paper)

3) No spark condition requirement for the generating matrix A: One of the mathematically significant contributions made by (HS15) was to forgo the constraint that the recovery matrix B also satisfy the spark condition, in contrast to all previously known uniqueness results which (either explicitly or implicitly) made this assumption.  Our proof is powerful enough to show that, in fact, even the matrix A need not satisfy the spark condition to be identifiable! Rather, it need only be injective on the union of subspaces with supports that form sets in a hypergraph satisfying the singleton intersection property (SIP). (For example, consider the matrix A = [e_1,..., e_5, v] where v = e_1 + e_3 + e_5. and take H to be the set of all consecutive pairs of [m] arranged in cyclic order. Then A satisfies the assumptions of Thm. 1 without satisfying the spark condition.) We had omitted this point in our original submission of the manuscript to keep things simple, but have decided to include this fact in our resubmission. This also required us to redefine the restricted matrix lower bound L to be in terms of a hypergraph, which is an interesting object for further study in its own right. 

We must also reiterate here that our solution to Prob. 2, that of most interest to practitioners of dictionary learning methods, is to our knowledge the first of its kind in both the noise-free and noisy domains. We recognize that this was not clearly communicated in the Discussion section of our submitted manuscript, and we have adjusted the manuscript accordingly.

Finally, we should mention that our main mathematical results justify the neurally plausible model of bottleneck communication between brain regions, first explained in depth in this NIPS paper (IHS10) and then in a review of sparse linear coding applications to neuroscience (GS12).

-----------------
-- A second general problem was that it was difficult for me to understand the theorem conditions and sample complexity. For example, in Theorem 1, I think you meant to say:

Let A satisfy eqn [2], x_1,...,x_N be a set of k-sparse vectors, and E \subseteq \choose{[m],k}, and \bar{m} be given and satisfy the following property:

For every S \in E, there exists a subset of x_1,...,x_N of at least size (k-1)\choose{\bar{m},k} with support in S. Moreover, any k vectors in this subset are linearly independent.

Is this what you meant?
-----------------

RESPONSE: Yes, though actually for every S \in E there are "more than" (k-1)\choose{\bar{m},k} vectors with support in S (not “at least”). Thank you for helping us to clarify our statements. We have updated the manuscript to disambiguate these conditions.

-----------------
-- Related point: Is there a condition on \ell in Theorem 1?
-----------------

RESPONSE: Yes, thanks for this question. The condition on \ell is perhaps best understood as a condition on \bar m not being too large (given \ell) in order to guarantee the recovery of at least one dictionary element. We have updated the manuscript to reflect this, and the condition now reads “provided \bar m < \ell m / (\ell - 1)” where previously it read “provided p = \bar m − \ell( \bar m − m ) is positive”.

-----------------
-- The paper could also compare the sample complexity results on N with those given in the noise-free case. For example, Table I in reference (18) gives a comparison of sample complexity requirements for different conditions in the noise-free case. Where would the results of this paper stand in that table?

-- Otherwise, all the math seemed correct.
-----------------

RESPONSE: We appreciate this question but, since in all comparable cases the sample complexities improve in this work, we feel that an explicit comparison would not be the best use of our limited space; although we are willing to do so if necessary. Many thanks for your careful review of the mathematics. We have tried and continue to try to be as clear, concise, and correct as we possibly can to elevate this work into the top echelon of applied mathematical theory papers.

-----------------
Reviewer #2:

Comments :
The paper presents an extension of existing conditions for uniqueness of the existence of a sparsity-inducing dictionary for a given set of data vectors that depends on the sampling of the resulting data subspaces. The extension considers the stability of dictionary learning in terms of the degree of variability present in distinct, feasible sparsity dictionary representations for the data under a constraint on the representation accuracy. A similar condition on the sampling of the underlying subspaces is obtained.

The paper's writing and composition is clear (with an exception detailed below), concise, and relevant to the topic. The analytical work appears to be correct to the best of my understanding, and the paper's combined use of several results in the literature to extend an existing dictionary learning uniqueness condition to a new dictionary learning stability condition appears to be original.

Some questions that remain and may be worth discussing deal with the practical impact of these results.

When is it possible to determine (numerically) if enough data has been gathered to obtain an accurate estimate of the underlying sparsity dictionary?
-----------------

RESPONSE: A statement of this very general nature requires that we calculate the probability that all of the conditions of Thm. 1 are satisfied by random data. We have provided an "almost certain" guarantee of this flavor in Cor 3, wherein the sparse vector supports are constrained to form a hypergraph satisfying the SIP. It would be useful and interesting to calculate furthermore the probability that random supports form a hypergraph satisfying the SIP. We have opted not to include these calculations due in part to space constraints, but also because we believe that what sets our work apart from the vast majority of results in the field is their deterministic nature, e.g. they do not depend on any kind of assumption about the random distribution from which the sparse coefficients or dictionary entries are drawn.

-----------------
Are the conditions and constants/metrics provided in this paper feasible to compute?
-----------------

RESPONSE: We have determined explicit forms for the effectively computable constants in Theorem 1.

-----------------
Is there any new intuition to be had regarding the necessary sampling of the union of subspaces from this analysis (with respect to the intuition obtained from existing uniqueness results)?
-----------------

RESPONSE: Yes, there is indeed. While all existing uniqueness results require a sufficient number of samples be drawn from each of {m \choose k} possible supports, we have shown that it is enough that a sufficient number of samples be drawn from every support in a subset of drastically smaller size, provided this subset forms a regular hypergraph satisfying the SIP. As was pointed out in our manuscript, a support set of size m satisfying these criteria can always be constructed for any k < m (e.g. take the consecutive intervals of length k in some cyclic order on [m]), and in certain cases such a set can be constructed from as few as 2\sqrt(m) supports (e.g. when m = k^2, take the supports to be the rows and columns of [m] arranged in a square grid).

-----------------
In these terms, it is also unclear to me if a theoretical extension of the uniqueness results to the stability results merit publication in PNAS given the "top 10%" and "sufficient general interest" requirements for publication. It definitely is deserving of publication in a specialty journal.

The work presented is analytical and does not rely on data validation.
-----------------

RESPONSE: Both reviewers have raised this concern and we again thank the editor for the opportunity to address it. It seems both reviewers have not noticed the several ways in which our results generalize the modern results of (HS15) beyond a straightforward extension to the noisy case, not to mention our solution to the optimization Prob. 2, which as far as we know is the first of its kind and of most interest to practitioners. We have already discussed these points in detail in our response to the first reviewer’s identical comment.

-----------------
In the paragraph "Since f is an analytic function..." (page 3) it is argued that a specific construction of a suitable matrix implies that "almost every real nxm matrix... satisfies Eq. (2)". This appears to overstate the result, in my opinion - can the authors elaborate on why "almost every real matrix" can be obtained from this construction based on a Vandermonde matrix with additional zero rows?
-----------------

RESPONSE: The point we make here is not that almost every real matrix satisfying Eq. (2) can be obtained from the construction based on a Vandermonde matrix which we provide; rather, the point is that almost every matrix satisfies Eq. (2) because at least one matrix does (e.g. our construction) and therefore, since f is analytic, the set of all those that don’t satisfy the condition has measure zero. We have updated the manuscript to clarify this argument for our readers.

-----------------
Some typos appear in Eq. (24), closing } missing from the denominators of the second and third inequalities. Additionally, it was not clear to me how B_{pi(S)} change to B_{pi(T)}.
-----------------

RESPONSE: Thank you for pointing out this typo in the second and third inequalities of Eq. (24). We have also clarified in our resubmission how it is that B_{pi(S)} can be replaced with B_{pi(T)} (it is because d(U, V) \leq d(U’,V) whenever U \subseteq U’).

