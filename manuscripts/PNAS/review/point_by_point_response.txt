References for Cover Letter and Point-by-point Response:

(IHS10) Isely, Hillar, Sommer, Advances in neural information processing systems (NIPS), 2010.
(M10) Morris I (2010), Advances in Mathematics 225(6):3425–3445.
(GS12) Ganguli, Sompolinksi, Annual review of neuroscience, 2012.
(ACD13) Arias-Castro, Candes, Davenport, IEEE Trans. Inf. Theory, 2013
(TP14) Tillman, Pfetsch, IEEE Transactions on information processing, 2014.
(T15) Tillman, IEEE Signal Processing Letters, 2015.
(HS15) Hillar, Sommer, IEEE Transactions on information processing, 2015.


-----------------
Reviewer Comments:

Reviewer #1:

Comments :
SUMMARY OF THE PAPER

The paper finds conditions under which solutions for the sparse linear coding problem are unique. In sparse linear coding, one is given a matrix Y and the problem is to find a dictionary A and sparse coefficient matrix X such that Y=AX, up to a bounded uncertainty. This paper finds conditions under which A and X are unique up to scalings and permutation. Previous works looked at the noise-free case; this work considers the case of "noisy" measurements and proves a stability condition.

GENERAL COMMENTS


Overall, while I think that the manuscript makes a solid contribution, I believe is better suited to a more specialized journal. Unlike many journals, PNAS gives a criterion for significance: that the paper be within the top 10% of publications and should be of interest to a general scientific audience. Without compelling evidence to the contrary from the authors, it seems to me that the submission does not rise to that level.
-----------------

RESPONSE: We have included in our revised manuscript a more compelling exposition of the significance of our results and their relevance to a general scientific audience, and we thank the editor for the opportunity to address this concern shared by both reviewers.

-----------------
Since the result is an extension of the noise-free result of (HS15) to a noisy case, the authors should provide better justification for the significance of contributions. The main result seems to be based on an approach similar to (HS15), and there is little to no discussion of the computability and tightness of the sufficient condition for stability in the main results. Thus, the submission does not seem to be greatly more valuable than (HS15) and also not conclusive regarding the central problems of sparse linear coding and dictionary learning. That being said, the paper (with the modifications below) would certainly merit publication in very solid journal like IEEE Transactions on Information Theory, where it would be a median paper, in my opinion.
-----------------

RESPONSE:

We agree with the reviewer that a straightforward extension of the approach and noise-free result of (HS15) to the noisy case — while still a significant advancement in applied mathematics with actual practical implications — would not necessarily merit publication in PNAS on its own. However, our main result (Thm. 1) generalizes the noise-free result of (HS15) far beyond a simple extension to the noisy case in ways which provide valuable insight into the dictionary learning model and, moreover, a significant deviation from the approach taken in (HS15) was necessary to prove these new facts. Additionally, our solution (Cor. 2) to the optimization formulation of the dictionary learning problem (Prob. 2) — of most interest to optimization theorists and practitioners of their methods — is (to our knowledge) the first of its kind in both the noise-free and noisy domains. We discuss all of these points in more detail in our response to the reviewer’s later "detailed comments". 

As per the reviewer’s request, we have included in our updated manuscript a discussion of the computability of the sufficient conditions (they are, indeed, computable). The main constants in our work -- C_1, C_2 -- are computable, elementarily, in terms of two positive quantities that are natural in our context: (a) the restricted "Lower Bound" L_k(M) of a matrix M, which is a measure of how distinguishable two (k/2)-sparse x’s are by their images under M, and (b) the value in Def. 4 of \xi(V_1,..,V_l) given subspaces V_i spanned by columns of A, which bounds the distance between intersections of such subspaces and each subspace individually. The first quantity (a) is, in some form or another, related to standard objects in the field and may be computed as the smallest singular value of a set of specific sub-matrices; in particular, for fixed sparsity constraint k, the lower bound L_k(A) is efficiently computable (polynomial time). Because deciding L_k(A) > 0 is already NP-hard (TP14), it seems likely that without fixing a sparsity level there is no efficient computation in general for (a). The calculation of (b) involves determining "canonical angles" between subspaces, which is usually reduced again to a numerical (efficient) singular value decomposition given the fixed subspaces. However, upon close inspection, C_2 nonetheless requires an exponential number of instances of computing \xi. Thus, while our constants are effective (an algorithm exists to compute them), they are not necessarily polynomial-time computable.

We next respond to the reviewer's question about the mathematical "tightness" for the three types of quantities in our results : (a) required coding error eps(delta_1, delta_2) sufficient to guarantee uniqueness in terms of desired dictionary delta_1 and sparse representation error delta_2, (b) the constants C_1 and C_2 involved in our sufficient linear bound on eps(delta_1, delta_2) in terms of the noise, and (c) the number of sufficient samples N guaranteeing uniqueness.  With respect to (a), even with fixed dictionary, the recovery of sparse representations up to an error that is linear in the noise is essentially optimal (ACD13) (up to constants). Thus, the fact that we have a sufficient error in coding for uniqueness being linear for both the desired dictionary and sparse vector recovery error is optimal.  Although optimal, our solution to Problem 1 involves constants C_1 and C_2, and it would be interesting and relevant for practical applications to determine the minimal such constants. In the case of sparsity k=1, we have given an example showing optimality of constants in this case. Finally, with respect to (c), we regard this as a fundamental open problem in the field: how many sparsely generated samples determine the linear coding model?  (One reason to be skeptical that a polynomial suffice is again (TP14, T15).)  However, there is some good news -- if k is fixed or if the sparse codes \bar x_i are known to come from a polynomial number of supports, then a polynomial number of samples N is enough to guarantee uniqueness. We have revised the manuscript to discuss this last fact (which, incidentally, does not follow from the results of (HS15) in the noiseless case).

It is true, however, that we have not proven the tightness of these conditions and the manuscript is therefore not conclusive regarding the central problems of sparse linear coding. We believe that these sufficient conditions yield strong enough conclusions to stand on their own; for instance, in synchronizing device parameters for a particular sparsely representable sensory ensemble.

-----------------
DETAILED COMMENTS

-- The paper does a good job of motivating the general problem of sparse coding to a general audience. In particular, there is a good review of the areas in which dictionary learning arises.

-- I think the biggest potential issue for publication in PNAS is that the significance of the result needs more discussion. Specifically, the authors could address why going from the noise-free case to the noisy case is not incremental. PNAS specifically targets submissions that go beyond what could be published in a more specialized journal. Could the authors, for example, contrast the proof technique between the noisy and noise-free cases? Or, perhaps, the practical implications?
-----------------

RESPONSE:

With all due respect to the reviewer, we believe the most compelling practical implication of a noisy result over a noiseless result is plainly evident: never has there been an experiment without noise and an unstable model is therefore useless in practice. We assume the reviewer must instead be concerned with what additional practical "take-away" message our manuscript contains.  One significant one is the outline of a procedure that can (sufficiently) affirm if one’s proposed solution to Prob. 1 or Prob. 2 is indeed unique. We have updated the manuscript with an explicit statement of this fact, and outline one such procedure here:

[ ** go through the algorithm below again *** ]

Given a dictionary A and codes x_i that solve Problem 1 (e.g. learned by any algorithm), check that that they satisfy the assumptions of Thm. 1:
   - List the support sets of the x_i 
      - Discard those for which there are (k-1){m \choose k} or fewer x_i with that support.
      - Discard those for which the supported x_i are not in general linear position. (This should also exclude any supports less than k in size.)
   - From the support sets that remain, list all SIP-satisfying hypergraphs that are subsets of this set
   - Discard those for which L_{2H}(A) = 0.
   - For each of the remaining hypergraphs, determine the constant C_1 from A and the x_i (or for every subset of the x_i of size (k-1){m \choose k})
   - Check that the derived upper bound on \varepsilon exceeds that of the original problem. If yes, the solution is "unique".

There are other practical (polynomial time) implications of our discovery of sufficient combinatorial designs for support sets of generating codes x_i. Moreover, we have shown that a subset of dictionary elements are recoverable even if the number of dictionary elements in total is unknown; these observations are discussed in more detail below. 

The reviewer also raises the concern that, regardless of practical implications, our results may amount to an incremental advance over those of (HS15) by way of similar techniques. We disagree with this assessment on both counts: our main result (Thm. 1) goes far beyond a straightforward extension of that in (HS15) to the noisy case and this required a significant deviation from their approach.

It is understandable that the reviewer may have thought otherwise, considering how the proof of Thm. 1 is presented in the manuscript. As observed by our second reviewer, however, the extension to the noisy case did indeed require a novel combination of several results in the literature. Specifically, the main difficulty was to generalize Lemma 1 to the case where the k-dimensional subspaces spanned by corresponding (through the map \pi) submatrices of A and B are assumed only to be proximal (small ‘d’), and not identical as in (HS15). In contrast to the noiseless case, here it must be explicitly demonstrated that this proximity relation is propagated through the repeated intersections of these submatrix spans all the way down to the spans of dictionary elements themselves. We designed and proved Lemma 4 to address this issue, which draws its bound from the convergence guarantees of an alternating projections algorithm first proposed by von Neumann. This result, combined with a little known fact about the distance ‘d’ requiring proof in (M10), constitute the more obscure components of the deduction in Eq. (26). To reiterate, this step is completely trivial in the noiseless case and required no mention for the inductive steps taken in (HS15).  

Our proof of Lemma 1 diverges perhaps even more significantly from the approach taken in (HS15) by way of Lemma 5. Key to our reduction of the sample complexity given in (HS15) by an exponential factor is the introduction of a combinatorial design (the "singleton intersection property") for support sets. Since in this case the map \pi from supports in the hypergraph to {[m] \choose k} is not surjective, one cannot apply the same inductive method as in (HS15), which freely chooses supports in the codomain to intersect at (k-1) nodes and map back to some corresponding (k-1)-sized intersection of supports in the domain. Instead, we demonstrate the surprising fact that by pigeonholing the images of supports in the (SIP-satisfying) hypergraph H, one can still guarantee a bijection between the nodes and therefore the subspaces spanned by individual dictionary elements. We note that it was necessary to forgo inductive methods altogether in order to prove this fact for all hypergraphs satisfying the SIP; otherwise, we would require that the supports in the hypergraph have intersections of size k’ for every k’ < k (e.g. this is not the case for the small SIP example we give consisting of the rows and columns of nodes arranged in a square grid). Again, to our surprise, it so happens that this new induction-less argument easily generalizes to the case where B has an arbitrary number of columns, in which case we find that a one-to-one correspondence exists between a subset of columns of A and B of a size that has a nice closed-form expression for regular SIP hypergraphs.

To be clear, the new perspective we take on the problem yields the following powerful conclusions beyond those of a straightforward extension of (HS15) to the noisy case:

1) An extension to the case where the number of dictionary elements is unknown: The results of (HS15) only apply to the case where the matrix B has the same number of columns as A. We forgo this assumption and show that B must have at least as many columns as A and contains (up to noise) a subset of the columns of A. The size of this subset depends on a simple relation between the number of columns of B and the regularity of the support-set hypergraph.

2) A significant reduction in sample complexity: To identify the n x m generating dictionary A, (HS15) require that data be sampled from every k-dimensional subspace spanned by the m columns of A (that is, {m \choose k} subspaces in total). We show that the data need only be sampled from m subspaces in total (e.g. those supported by consecutive intervals of length k in some cyclic order on [m]) and in some cases as few as 2\sqrt{m} (e.g. when m = k^2, take the supports to be the rows and columns of [m] arranged in a square grid). Moreover, if the size of the support set of reconstructing codes is known to be polynomial in m and k, then the pigeonholing argument in the proof of Thm. 1 requires only a polynomial number of samples distributed over a polynomial number of supports; thus, N is then polynomial in \bar m, k. This point was only hinted at in the Discussion section of our original submission, but we have included it in the updated manuscript to make clear the power of our approach over that taken in (HS15). 

3) No spark condition requirement for the generating matrix A: One of the mathematically significant contributions made by (HS15) was to forgo the constraint that the recovery matrix B also satisfy the spark condition, in contrast to all previously known uniqueness results which (either explicitly or implicitly) made this assumption.  Our proof is powerful enough to show that, in fact, even the matrix A need not satisfy the spark condition to be identifiable! Rather, it need only be injective on the union of subspaces with supports that form sets in a hypergraph satisfying the singleton intersection property (SIP). (For example, consider the matrix A = [e_1,.., e_5, v] where v = e_1 + e_3 + e_5, and take H to be the set of all consecutive pairs of [m] arranged in cyclic order. Then A satisfies the assumptions of Thm. 1 without satisfying the spark condition.) We had omitted this point in our original submission of the manuscript to keep things simple, but have decided to include this fact in our resubmission. This also required us to redefine the restricted matrix lower bound L_k to be in terms of a hypergraph H (L_H in the revised manuscript), which is an interesting object for further study in its own right. 

We must also reiterate here that our solution to Prob. 2, that of most interest to practitioners of dictionary learning methods, is to our knowledge the first of its kind in both the noise-free and noisy domains. We recognize that this was not clearly communicated in the Discussion section of our submitted manuscript, and we have adjusted the manuscript accordingly.

Finally, we should mention that our main mathematical results justify the neurally plausible model of bottleneck communication between brain regions, first explained in depth in this NIPS paper (IHS10) and then in a review of sparse linear coding applications to neuroscience (GS12).

-----------------
-- A second general problem was that it was difficult for me to understand the theorem conditions and sample complexity. For example, in Theorem 1, I think you meant to say:

Let A satisfy eqn [2], x_1,..,x_N be a set of k-sparse vectors, and E \subseteq \choose{[m],k}, and \bar{m} be given and satisfy the following property:

For every S \in E, there exists a subset of x_1,..,x_N of at least size (k-1)\choose{\bar{m},k} with support in S. Moreover, any k vectors in this subset are linearly independent.

Is this what you meant?
-----------------

RESPONSE: Yes, though actually for every S \in E there are "more than" (k-1)\choose{\bar{m},k} vectors with support in S (not "at least"). Thank you for helping us to clarify our statements. We have updated the manuscript to disambiguate these conditions.

-----------------
-- Related point: Is there a condition on \ell in Theorem 1?
-----------------

RESPONSE: Yes, thanks for this question. The condition on \ell is perhaps best understood as a condition on \bar m not being too large (given \ell) in order to guarantee the recovery of at least one dictionary element. We have updated the manuscript to reflect this, and the condition now reads "provided \bar m < \ell m / (\ell - 1)" where previously it read "provided p = \bar m − \ell( \bar m − m ) is positive". (Note: in the resubmitted manuscript, we have actually opted to use r instead of \ell for the regularity of the hypergraph.)

-----------------
-- The paper could also compare the sample complexity results on N with those given in the noise-free case. For example, Table I in reference (18) gives a comparison of sample complexity requirements for different conditions in the noise-free case. Where would the results of this paper stand in that table?

-- Otherwise, all the math seemed correct.
-----------------

RESPONSE: In brief, since all of the theorems and corollaries in (HS15) are corollaries of our theorems, all sample complexities improve by our work. The main point of difference between our statements about sample complexity and those of (HS15) is the assumed constraint on the underlying support set of sparse codes x_i. Our theory of combinatorial designs (the “singleton intersection property”) requires only that there be a sufficient number of x_i drawn from each support in a hypergraph satisfying the SIP, whereas the theory in (HS15) requires that a sufficient number of x_i be drawn from every support of size k in [m]. Below, we make an explicit comparison with Table I in (HS15) row by row, assuming for our results that sufficient data have been sampled from all supports in a hypergraph H satisfying the SIP:

ROW I. Our result here is |H|(k-1){m \choose k} + 1. We improve on the result in (HS15) by an exponential factor since our theory does not require that data be sampled with every possible support of size k in [m]; as noted in our manuscript, for every k < m there exists a hypergraph H with |H| = m that satisfies the SIP. 

ROW 2. Our result here is again |H|(k-1){m \choose k} + 1 with certainty, not with almost certainty (i.e. probability 1) as in (HS15). Here, the authors in (HS15) have applied probabilistic arguments to achieve an almost certain result with a sample complexity on the same order as that for which we have achieved a certain result by means of our theory of combinatorial designs.    

ROW 3. We cannot make a direct comparison here because we have not calculated the probability that a random set of supports satisfy the SIP (see our response to a similar question by the second reviewer). We think this is an interesting problem for the community to solve; regardless, the result of (HS15) here is still implied by our more general theory.

ROW 4. Our result here is |H|(k-1){m \choose k} + 1 with probability 1. Here is the only case where the sample complexity stated in (HS15) is technically better than ours, but it differs in flavor: their x_i are assumed to be distributed as (k+1) samples per support for every support of size k in [m], whereas ours supposes that (k-1){m \choose k} codes x_i are distributed over each support in some hypergraph H satisfying the SIP. Regardless, the (technically) better result of (HS15) is still implied by our more general theory in the case \varepsilon = 0 with the addition of their probabilistic argument.

ROW 5: Again, we cannot make a direct comparison here for the reason stated in ROW 3.

To sum up, while we appreciate this question, we feel that since our results improve in every case except for one noise-free technicality (ROW 4), and since the results of (HS15) are all entailed anyway by our more general theory, an explicit comparison such as that which we provided above would not be the best use of our limited space in the manuscript; although we are willing to add a discussion about this if necessary. Many thanks for your careful review of the mathematics. We have tried and continue to try to be as clear, concise, and correct as we possibly can to elevate this work into the top echelon of applied mathematical theory papers.

-----------------
Reviewer #2:

Comments :
The paper presents an extension of existing conditions for uniqueness of the existence of a sparsity-inducing dictionary for a given set of data vectors that depends on the sampling of the resulting data subspaces. The extension considers the stability of dictionary learning in terms of the degree of variability present in distinct, feasible sparsity dictionary representations for the data under a constraint on the representation accuracy. A similar condition on the sampling of the underlying subspaces is obtained.

The paper's writing and composition is clear (with an exception detailed below), concise, and relevant to the topic. The analytical work appears to be correct to the best of my understanding, and the paper's combined use of several results in the literature to extend an existing dictionary learning uniqueness condition to a new dictionary learning stability condition appears to be original.

Some questions that remain and may be worth discussing deal with the practical impact of these results.

When is it possible to determine (numerically) if enough data has been gathered to obtain an accurate estimate of the underlying sparsity dictionary?
-----------------

RESPONSE: A statement of this very general nature requires that we calculate the probability that all of the conditions of Thm. 1 are satisfied by random data. We have provided an "almost certain" guarantee of this flavor in Cor 3, wherein the sparse vector supports are constrained to form a hypergraph satisfying the SIP. It would be useful and interesting to calculate furthermore the probability that random supports form a hypergraph satisfying the SIP. We have opted not to include these calculations due in part to space constraints, but also because we believe that what sets our work apart from the vast majority of results in the field is their deterministic nature, e.g. they do not depend on any kind of assumption about the particular random distribution from which the sparse coefficients or dictionary entries are drawn.

-----------------
Are the conditions and constants/metrics provided in this paper feasible to compute?
-----------------

RESPONSE: We have determined explicit forms for the effectively computable constants in Theorem 1, but they are not necessarily feasible to compute. We have included discussion of this in the revised manuscript.

-----------------
Is there any new intuition to be had regarding the necessary sampling of the union of subspaces from this analysis (with respect to the intuition obtained from existing uniqueness results)?
-----------------

RESPONSE: Yes, there is indeed. While all existing uniqueness results require a sufficient number of samples be drawn from each of {m \choose k} possible supports, we have shown that it is enough that a sufficient number of samples be drawn from every support in a subset of drastically smaller size, provided this subset forms a regular hypergraph satisfying the SIP. As was pointed out in our manuscript, a support set of size m satisfying these criteria can always be constructed for any k < m (e.g. take the consecutive intervals of length k in some cyclic order on [m]), and in certain cases such a set can be constructed from as few as 2\sqrt(m) supports (e.g. when m = k^2, take the supports to be the rows and columns of [m] arranged in a square grid).

-----------------
In these terms, it is also unclear to me if a theoretical extension of the uniqueness results to the stability results merit publication in PNAS given the "top 10%" and "sufficient general interest" requirements for publication. It definitely is deserving of publication in a specialty journal.

The work presented is analytical and does not rely on data validation.
-----------------

RESPONSE: Both reviewers have raised this concern and we again thank the editor for the opportunity to address it. It seems both reviewers have not noticed the several ways in which our results generalize the modern results of (HS15) beyond a straightforward extension to the noisy case, not to mention our solution to the optimization Prob. 2, which as far as we know is the first of its kind and of most interest to practitioners. We have already discussed these points in detail in our response to the first reviewer’s identical comment.

-----------------
In the paragraph "Since f is an analytic function..." (page 3) it is argued that a specific construction of a suitable matrix implies that "almost every real nxm matrix... satisfies Eq. (2)". This appears to overstate the result, in my opinion - can the authors elaborate on why "almost every real matrix" can be obtained from this construction based on a Vandermonde matrix with additional zero rows?
-----------------

RESPONSE: The point we make here is not that almost every real matrix satisfying Eq. (2) can be obtained from the construction based on a Vandermonde matrix which we provide; rather, the point is that almost every matrix satisfies Eq. (2) because at least one matrix does (e.g. our construction) and therefore, since f is analytic, the set of all those that do not satisfy the condition has measure zero. We have updated the manuscript to clarify this argument for our readers.

-----------------
Some typos appear in Eq. (24), closing } missing from the denominators of the second and third inequalities. Additionally, it was not clear to me how B_{pi(S)} change to B_{pi(T)}.
-----------------

RESPONSE: Thank you for pointing out this typo in the second and third inequalities of Eq. (24). We have also clarified in our resubmission how it is that B_{pi(S)} can be replaced with B_{pi(T)} (it is because d(U, V) \leq d(U’,V) whenever U \subseteq U’).
