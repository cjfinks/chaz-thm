-----------------
Reviewer Comments:

Reviewer #1:

Comments :
SUMMARY OF THE PAPER

The paper finds conditions under which solutions for the sparse linear coding problem are unique. In sparse linear coding, one is given a matrix Y and the problem is to find a dictionary A and sparse coefficient matrix X such that Y=AX, up to a bounded uncertainty. This paper finds conditions under which A and X are unique up to scalings and permutation. Previous works looked at the noise-free case; this work considers the case of "noisy" measurements and proves a stability condition.

GENERAL COMMENTS


Overall, while I think that the manuscript makes a solid contribution, I believe is better suited to a more specialized journal. Unlike many journals, PNAS gives a criterion for significance: that the paper be within the top 10% of publications and should be of interest to a general scientific audience. Without compelling evidence to the contrary from the authors, it seems to me that the submission does not rise to that level.
-----------------

RESPONSE: We have included in our revised manuscript a more compelling exposition of the significance of our results and their relevance to the general scientific audience, and we thank the editor for the opportunity to address this concern shared by both reviewers.

-----------------
Since the result is an extension of the noise-free result of [Hillar & Sommer (2015)] to a noisy case, the authors should provide better justification for the significance of contributions. The main result seems to be based on an approach similar to [Hillar & Sommer (2015)], and there is little to no discussion of the computability and tightness of the sufficient condition for stability in the main results. Thus, the submission does not seem to be greatly more valuable than [Hillar & Sommer (2015)] and also not conclusive regarding the central problems of sparse linear coding and dictionary learning. That being said, the paper (with the modifications below) would certainly merit publication in very solid journal like IEEE Transactions on Information Theory, where it would be a median paper, in my opinion.
-----------------

RESPONSE:

We agree with the reviewer that a straightforward extension of the approach and noise-free result of [Hillar & Sommer (2015)] to the noisy case — while still a significant advancement in applied mathematics with actual practical implications — would not necessarily merit publication in PNAS on its own. The reviewer, however, seems to have overlooked the fact that our main result (Thm. 1) generalizes the noise-free result of [Hillar & Sommer (2015)] far beyond a simple extension to the noisy case in ways which provide valuable insight into the dictionary learning model. A significant deviation from the approach taken in [Hillar & Sommer (2015)] was necessary to prove these new facts. Moreover, our solution (Cor. 2) to the optimization formulation of the dictionary learning problem (Prob. 2) — that of most interest to optimization theorists and practitioners of their methods — is the first of its kind (to our knowledge) in both the noise-free and noisy domains. We discuss all of these points in more detail in our response to the reviewer’s later “detailed comments”.

We have included in our updated manuscript a discussion of the computability of the sufficient conditions (they are, indeed, computable), as per the reviewer’s request. It is true, however, that we have not proven the tightness of these conditions and the manuscript is therefore not *conclusive* regarding the central problems of sparse linear coding. We believe that these sufficient conditions yield strong enough conclusions to stand on their own. Nonetheless, we have included in our revised manuscript some discussion of where the results can be tightened (e.g. Lem. 4). A sample complexity polynomial in (m,k) can be achieved, however, by limiting the size of the support set of the \tilde x_i, and we have revised the manuscript to discuss this fact (which, incidentally, does *not* follow from the results of [Hillar & Sommer (2015)] in the noiseless case). [And for fixed k […]]

-----------------
DETAILED COMMENTS

-- The paper does a good job of motivating the general problem of sparse coding to a general audience. In particular, there is a good review of the areas in which dictionary learning arises.

-- I think the biggest potential issue for publication in PNAS is that the significance of the result needs more discussion. Specifically, the authors could address why going from the noise-free case to the noisy case is not incremental. PNAS specifically targets submissions that go beyond what could be published in a more specialized journal. Could the authors, for example, contrast the proof technique between the noisy and noise-free cases? Or, perhaps, the practical implications?
-----------------

RESPONSE:

With all due respect to the reviewer, we believe the most compelling practical implication of a noisy result over a noiseless result is plainly evident: never has there been an experiment without noise and an unstable model is therefore useless in practice. We assume the reviewer must instead be concerned with what additional practical “take-away” message our manuscript contains. In brief, it is this: we have outlined a procedure by which one can determine if one’s proposed solution to Prob. 1 or Prob. 2 is indeed unique. We have updated the manuscript with an explicit statement of this fact.

** IS THAT ENOUGH FOR PRACTICAL IMPLICATIONS..? **

The reviewer also raises the concern that, regardless of practical implications, our results may amount to an incremental advance over those of [Hillar & Sommer (2015)] by way of similar techniques. We disagree with this assessment on both counts: our main result (Thm. 1) goes far beyond a straightforward extension of that in [Hillar & Sommer (2015)] to the noisy case and this required a significant deviation from their approach.

It is understandable that the reviewer may have thought otherwise, considering how the proof of Thm. 1 is presented in the manuscript. As observed by our second reviewer, however, the extension to the noisy case did indeed require an original combination of several results in the literature. Specifically, the main difficulty was to generalize Lemma 1 to the case where the k-dimensional subspaces spanned by corresponding (through the map \pi) sub-matrices of A and B are assumed only to be proximal (small ‘d’), and not identical as in [Hillar & Sommer (2015)]. In contrast to the noiseless case, here it must be explicitly demonstrated that this proximity relation is propagated through the repeated intersections of these sub-matrix spans all the way down to the spans of dictionary elements themselves. We designed and proved Lemma 4 to address this issue, which draws its bound from the convergence guarantees of an alternating projections algorithm first proposed by von Neumann. This result, combined with a little known fact about the distance ‘d’ requiring proof in [Morris; Advances in Mathematics], constitute the more sophisticated and obscure components of the deduction in eq.(26). To reiterate, this step is completely trivial in the noiseless case and required no mention for the inductive steps taken in [HS15].  

Our proof of Lemma 1 diverges perhaps even more significantly from the approach taken in [HS15] by way of Lemma 5. Key to our reduction of the sample complexity given in [HS15] by an exponential factor is the introduction of a combinatorial design (the ‘singleton intersection property’) for the support sets. Since in this case the map \pi from supports in the hypergraph to {[m] \choose k} is not surjective, one cannot apply the same inductive method as in [HS15], which freely chooses supports in the codomain to intersect at (k-1) nodes and map back to some corresponding (k-1)-sized intersection of supports in the domain. Instead, we demonstrate the surprising fact that by pigeonholing the images of supports in the (SIP-satisfying) hypergraph one can still guarantee a bijection between the nodes and therefore the subspaces spanned by individual dictionary elements. We note that it was necessary to forgo inductive methods altogether in order to prove this fact for all hypergraphs satisfying the SIP; otherwise, we would require that the supports in the hypergraph have intersections of size k’ for every k’ < k (e.g. this is not the case for the small SIP example we give consisting of the rows and columns of nodes arranged in a square grid). Again, to our surprise, it so happens that this new induction-less argument easily generalizes to the case where B has an arbitrary number of columns, in which case we find that a one-to-one correspondence exists between a subset of columns of A and B of a size that has a nice closed-form expression for regular SIP hypergraphs.

To be clear, the new perspective we take on the problem yields the following powerful conclusions beyond those of a straightforward extension of the noiseless result of [Hillar & Sommer (2015)] to the noisy case:

1) An extension to the case where the number of dictionary elements is unknown: The results of [Hillar & Sommer (2015)] only apply to the case where the matrix B has the same number of columns as A. We forgo this assumption and show that B must have at least as many columns as A and contains (up to noise) a subset of the columns of A. The size of this subset depends on a simple relation between the number of columns of B and the regularity of the support-set hypergraph.

2) A significant reduction in sample complexity: To identify the n x m generating dictionary A, [Hillar & Sommer (2015)] require that data be sampled from *every* k-dimensional subspace spanned by the m columns of A (that is, {m \choose k} subspaces in total). We show that the data need only be sampled from m subspaces in total (e.g. those supported by consecutive intervals of length k in some cyclic order on [m]) and in some cases as few as 2\sqrt{m} (e.g. when m = k^2, take the supports to be the rows and columns of [m] arranged in a square grid). Moreover, if the size of the support set of reconstructing codes is polynomial in m,k then the pigeonholing argument in the proof of Thm. 1 requires only a polynomial number of samples distributed over a polynomial number of supports; thus N is polynomial in m, k. This point was only hinted at in the Discussion section of our original submission, but we have included it in the updated manuscript to make clear the power of our approach over that taken in [Hillar & Sommer (2015)]. [For fixed k…]

3) No spark condition requirement for the generating matrix A: One of the mathematically significant contributions made by [Hillar & Sommer (2015)] was to forgo the constraint that the recovery matrix B also satisfy the spark condition, in contrast to all previously known uniqueness results which (either explicitly or implicitly) made this assumption. Our proof is powerful enough to show that, in fact, even the matrix A need not satisfy the spark condition to be identifiable! Rather, it need only be injective on the union of subspaces with supports that form sets in a hypergraph satisfying the singleton intersection property (SIP). We had omitted this point in our original submission of the manuscript to keep things simple, but have decided to include this interesting fact in our resubmission. This also required us to redefine the restricted matrix lower bound L to be in terms of a hypergraph, which is an interesting object for further study in its own right.

We must also mention here that our solution to Prob. 2, that of most interest to practitioners of dictionary learning methods, is to our knowledge the first of its kind in both the noise-free and noisy domains. We recognize that this was not clearly communicated in the Discussion section of our submitted manuscript, which begins: “In this note, we generalized the approach of (18) to prove the stability of unique solutions to Probs. 1 and 2 […]” 

-----------------
-- A second general problem was that it was difficult for me to understand the theorem conditions and sample complexity. For example, in Theorem 1, I think you meant to say:

Let A satisfy eqn [2], x_1,...,x_N be a set of k-sparse vectors, and E \subseteq \choose{[m],k}, and \bar{m} be given and satisfy the following property:

For every S \in E, there exists a subset of x_1,...,x_N of at least size (k-1)\choose{\bar{m},k} with support in S. Moreover, any k vectors in this subset are linearly independent.

Is this what you meant?
-----------------

RESPONSE: Yes, though actually for every S \in E there are *more than* (k-1)\choose{\bar{m},k} vectors with support in S (not “at least”). Thank you for helping us to clarify our statements. We have updated the manuscript to disambiguate these conditions.

-----------------
-- Related point: Is there a condition on \ell in Theorem 1?
-----------------

RESPONSE: Yes, thanks for this question. The condition on \ell is perhaps best understood as a condition on \bar m not being too large (given \ell) in order to guarantee the recovery of at least one dictionary element. We have updated the manuscript to reflect this, and the condition now reads “provided \bar m < \ell m / (\ell - 1)” where previously it read “provided p = \bar m − \ell( \bar m − m ) is positive”.

-----------------
-- The paper could also compare the sample complexity results on N with those given in the noise-free case. For example, Table I in reference (18) gives a comparison of sample complexity requirements for different conditions in the noise-free case. Where would the results of this paper stand in that table?

-- Otherwise, all the math seemed correct.
-----------------

RESPONSE: We appreciate this question and have added a discussion of this comparison to the manuscript (all sample complexities improve in this work) [DO THIS]. Many thanks for your careful review of the mathematics. We have tried and continue to try to be as clear, concise, and correct as we possibly can to elevate this work into the top echelon of applied mathematical theory papers.

[Do comparison here, state that noiseless is a corollary of our noisy and therefore we don’t discuss the comparison in the paper. They have the table only because they needed randomness to get what we get with SIP. We’re willing to add it if you insist, but we think it detracts from the paper..]

-----------------
Reviewer #2:

Comments :
The paper presents an extension of existing conditions for uniqueness of the existence of a sparsity-inducing dictionary for a given set of data vectors that depends on the sampling of the resulting data subspaces. The extension considers the stability of dictionary learning in terms of the degree of variability present in distinct, feasible sparsity dictionary representations for the data under a constraint on the representation accuracy. A similar condition on the sampling of the underlying subspaces is obtained.

The paper's writing and composition is clear (with an exception detailed below), concise, and relevant to the topic. The analytical work appears to be correct to the best of my understanding, and the paper's combined use of several results in the literature to extend an existing dictionary learning uniqueness condition to a new dictionary learning stability condition appears to be original.

Some questions that remain and may be worth discussing deal with the practical impact of these results.

When is it possible to determine (numerically) if enough data has been gathered to obtain an accurate estimate of the underlying sparsity dictionary?
-----------------

RESPONSE: A statement of this very general nature requires that we calculate the probability that all of the conditions of Thm. 1 are satisfied by random data. We have provided an ‘almost certain’ guarantee of this flavor in Cor 3, wherein the sparse vector supports are constrained to form a hypergraph satisfying the SIP. It would be interesting to calculate furthermore the probability that random supports form a hypergraph satisfying the SIP. We think we have cracked open a nice set of interesting combinatorial problems (with practical implications!) to be solved by the community.

[We could have done this but we believe what sets our results apart from the vast majority of results in this field so far is its deterministic nature from which any probabilistic result can be derived, e.g. the ones we give] 

-----------------
Are the conditions and constants/metrics provided in this paper feasible to compute?
-----------------

RESPONSE: We have determined explicit forms for the effectively computable constants in Theorem 1. [But they are not feasible to compute, since they require calculating the RIP of all k-wise subsets of the data. Revive NP-hard argument?]

-----------------
Is there any new intuition to be had regarding the necessary sampling of the union of subspaces from this analysis (with respect to the intuition obtained from existing uniqueness results)?
-----------------

RESPONSE: Yes, there is indeed. While all existing uniqueness results require a sufficient number of samples be drawn from *each* of {m \choose k} possible supports, we have shown that it is enough that a sufficient number of samples be drawn from every support in a subset of drastically smaller size, provided this subset forms a regular hypergraph satisfying the SIP. As was pointed out in our manuscript, a support set of size m satisfying these criteria can always be constructed for any k < m (e.g. take the consecutive intervals of length k in some cyclic order on [m]), and in certain cases such a set can be constructed from as few as 2\sqrt(m) supports (e.g. when m = k^2, take the supports to be the rows and columns of [m] arranged in a square grid).

-----------------
In these terms, it is also unclear to me if a theoretical extension of the uniqueness results to the stability results merit publication in PNAS given the "top 10%" and "sufficient general interest" requirements for publication. It definitely is deserving of publication in a specialty journal.

The work presented is analytical and does not rely on data validation.
-----------------

RESPONSE: Both reviewers have raised this concern and we again thank the editor for the opportunity to address it. It seems both reviewers have not noticed the several ways in which our results generalize the modern results of [Hillar & Sommer (2015)] beyond a straightforward extension to the noisy case, not to mention our solution to the optimization Prob. 2, which as far as we know is the first of its kind and of most interest to practitioners. We have already discussed these points in detail in our response to the first reviewer’s identical comment.

-----------------
In the paragraph "Since f is an analytic function..." (page 3) it is argued that a specific construction of a suitable matrix implies that "almost every real nxm matrix... satisfies Eq. (2)". This appears to overstate the result, in my opinion - can the authors elaborate on why "almost every real matrix" can be obtained from this construction based on a Vandermonde matrix with additional zero rows?
-----------------

RESPONSE: The point we make here is not that almost every real matrix satisfying Eq. (2) can be obtained from the construction based on a Vandermonde matrix which we provide; rather, the point is that almost every matrix satisfies Eq. (2) because at least one matrix does (e.g. our construction) and therefore, since f is analytic, the set of all those that don’t satisfy the condition has measure zero. We have updated the manuscript to clarify this argument for our readers.

-----------------
Some typos appear in Eq. (24), closing } missing from the denominators of the second and third inequalities. Additionally, it was not clear to me how B_{pi(S)} change to B_{pi(T)}.
-----------------

RESPONSE: Thank you for pointing out this typo in the second and third inequalities of Eq. (24). We have also clarified in our resubmission how it is that B_{pi(S)} can be replaced with B_{pi(T)} (it is because d(U, V) \leq d(U’,V) whenever U \subseteq U’).





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Checking algorithm for Problem 1:
- Learn by any algorithm a dictionary A and codes x_i that solve Problem 1.
- Check that that A and the x_i satisfy the assumptions of Thm. 1:
   - List the support sets of the x_i 
      - Discard those for which there are (k-1){m \choose k} or fewer x_i with that support.
      - Discard those for which the supported x_i are not in GLP. (This should also exclude any supports less than k in size.)
   - From the support sets that remain, list all k-uniform hypergraphs with supports drawn from this set that satisfy the SIP
   - List the k-uniform hypergraphs over which A is injective.
   - Discard those for which there aren’t more than (k-1){m \choose k} x_i in general linear position on each support of the hyper graph.
   - For each of the hyper graphs, determine from A, the x_i 
   - If A satisfies spark condition, check that the x_i in some hypergraph satisfying the SIP have enough samples each, calculate the constants, check that you encode below the constants.
