TO DO:

-add Ganguli paper 
-add "Deep Learning" non-uniqueness statements

-(in response:) add comment that we are aware of only one work treating combinatorial hypergraph designs in the setting of dictionary learning, "An incidence geometry approach to dictionary learning" (arxiv, non-published paper)

-remark: more general statement

-----------------------------

1) Computability of constants.

The main constants in our work -- C_1, C_2 -- are computable, elementarily, in terms of two positive quantities that are natural in our context: (a) the "Lower Bound" L_H(A) of a matrix A given a hypergraph H (the support structure of sparse x's generating the data), which is a measure of how much A has a "spark condition" supported H, and (b) the value in Def. 4 of r(V_1,...,V_l) given subspaces V_i corresponding to spans of A's columns, which bounds the distance between an intersection of such spans and each one individually. The first quantity (a) is, in some form or another, a standard object in the field [chaz: no it's not -- it's new, we made it up.] and may be computed as the smallest singular value of a particular matrix; in particular, for fixed sparsity constraint k on H, the lower bound L_H(A) is efficiently computable (polynomial time).  Because deciding L_H(A) > 0 is already NP-hard [Tillman-Pfetsch-2014], it seems likely that without fixing a sparsity level there is no efficient computation in general for (a).  The calculation of (b) involves determining "canonical angles" between subspaces, which is usually reduced again to a numerical (efficient) singular value decomposition given the fixed subspaces.  However, upon close inspection, C_2 nonetheless requires an exponential number of instances of computing r.  Thus, while our constants are effective (an algorithm exists to compute them), they are not necessarily polynomially-time computable.

2) Reflection on optimality of mathematical results

One natural question that arises is the mathematical "tightness" of result statements for the three types of quantities: (a) required coding error eps(delta_1, delta_2) sufficient to guarantee uniqueness in terms of desired dictionary delta_1 and sparse representation error delta_2, (b) the constants C_1 and C_2 involved in our sufficient linear bound on eps(delta_1, delta_2), and (c) the number of sufficient samples N guaranteeing uniqueness.  With respect to (a), even with fixed dictionary, the recovery of sparse representations up to an error that is linear in the noise is classically optimal (up to constants).  Thus, the fact that we have a sufficient error in coding for uniqueness being linear in terms of both the desired dictionary and sparse vector recovery error is optimal.  Although optimal up to constants, our solution to Problem 1 involves constants C_1 and C_2, but it would be interesting and relevant for practical applications to determine the minimal such constants.  In the case of sparsity k=1, we have given an example showing optimality of constants in this case.  Finally, with respect to (c), we regard this is a fundamental open problem in the field: how many sparsely generated samples determine the linear coding model?  (One reason to be skeptical that a polynomial suffice is again [Tillman-Pfetsch-2014].)  However, there is some good news -- if k is fixed and we constrain sparse codes \hat{x}_i to come from a polynomial number of supports, then a polynomial number of samples N is enough to guarantee uniqueness.

3) We provide an algorithm for deciding whether dictionary A and sparse codes x determine a dataset Y with a unique sparse coding.

4) We reduce the theoretical overhead of algorithmic sparse linear coding by providing clean sufficient conditions under which solving Problem 1 determines model parameters (up to symmetries) -- without regard to the idiosyncrasies of any particular dictionary learning method.

5) Proof differences
[Chaz]

6) New concepts introduced
[Chaz]
