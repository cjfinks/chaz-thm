{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red78\green78\blue78;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c38039\c38039\c38039;\cssrgb\c100000\c100000\c100000;}
\margl1440\margr1440\vieww13940\viewh19480\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
BIBLIOGRAPHY:\
\pard\pardeftab720\qj\partightenfactor0
\cf2 Quantification of artistic style through sparse coding analysis in the drawings of Pieter Bruegel the Elder, PNAS\
Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks\
\pard\pardeftab720\partightenfactor0
\cf2 \
Editor Comments:\
\
I actually don't want a revision at the moment, just a response.\
\
My job as editor is certainly to not get suckered.\
\
I think the claims are undeniably important, but I'm not clear on exactly\
how we get there.\
\
Both of the referees are somewhat skeptical of the extent of the\
contribution being made. They both say you're overselling. Your cover\
letter goes into detail covering contributions, and some of what you say is\
convincing. But parts are simply lawyerly.\
\
Here are some things I'm thinking in the back of my mind.\
\
(a) This is at best about local stability, rather than stability. The size\
of the neighborhood where you're getting stability is presumably almost\
infinitesimal in general. While I agree that the paper is written in a\
quantitative way and has as you say constants which can be quantitative, it\
is I would say after a few readings, more\
seemingly-quantitative-but-actually-purely-qualitative.\
If it could be shown to be surprisingly effective in some specific case, I\
might withdraw from this position.\
\
RESPONSE:\
\
We are somewhat confused by this statement as we understand even the tightest possible noise bound to be necessarily \'91local\'92 and approaching zero as the domain-restricted matrix lower-bound vanishes, e.g. as the number of dictionary elements grows infinite. One need only consider the case k=1, where every dictionary element spans a ray through the origin, and each identifiable datum generated from the model (1) lies within a cylinder of radius \\eta around one (and only one) of these rays. Clearly, for bounded data in a finite dimensional space, this radius is cannot remain finite in the limit of infinitely many distinct rays.\
\
It seems instead your concern is that the bound is in general so small it is \'91practically\'92 infinitesimal. Indeed, our guarantees apply to the deterministic \'93worst-case\'94 scenario where noise is isotropic and \'91what can go wrong, will go wrong\'92. Calculations specific to the data at hand would consider the probability that confounding noise actually occurs and yield more forgiving probabilistic guarantees.\
\
We have simulated the distribution of noise that can be tolerated in the deterministic worst-case for some generic matrices and sparse data supported on our two example hypergraphs from the introduction.  \
\
[TODO calculate constant]\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
(b) Point (a) is important because when you say these results somehow\
validate various neuroscience ideas, I am saying in the back of my mind\
that you're bravely talking yourself on a ledge. Namely, nothing in\
neuroscience can really depend on infinitesimal stability. Trying to make a\
claim like this seems to undermine your credibility with me. It seems the\
referees had the same problem.\
\
RESPONSE:\
\
Our response to point (a) aside, and though we are not inclined to assume that neural circuits are incapable of extreme precision [citations?], we actually share your skepticism here. However, we simply confirm the well-posedness of the problem which specialists in the field have suggested neural circuits may be solving (see e.g. Ganguli paper, Sommer paper, NIPS paper) and feel it is their credibility, if anyone\'92s, that should be questioned and not ours. \
\
If you are nevertheless reluctant to become complicit in propagating this neuroscientific hypothesis, we have no qualms downplaying it in the manuscript. Instead, we could discuss the relevance of our results to recent work applying dictionary learning and related methods to solve inverse problems in other areas of science. For example, TODO (PNAS Bin Yu paper, PNAS Forgery). \
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
(c) It's not clear why exact uniqueness should be so important for\
neuroscience. It seems more likely that mere similarity is what's maybe\
important. In the same sense it's not clear why formal Lipschitz stability\
in the mathematical sense should be so important for neuroscience. It seems\
more important that some sort of qualitative similarity should persist\
under perturbations.\
\
RESPONSE:\
\
Look, man, we didn\'92t invent this stuff; we\'92re just here cleaning it up. Here\'92s how we rationalize the hypothesis:\
\
Suppose some quantities of interest are encoded in the sparse activity of neurons in the sending region, sparse for whatever reason [e.g. machine learning papers on representations/sparsity, note that sparse activity has been observed in brains]. These quantities are to be transmitted to some distant region through as few wires (axons) as possible, e.g. due to space constraints inside the skull.\
\
The hypothesis is that the brain, up-to date on the latest fad, solves this problem by applying a (noisy) \'91random\'92 projection into the lower-dimensional space spanned by these axons. The neurons in the receiving region are then tasked with decoding the quantities of interest from this compressed signal. Suppose, however, that they cannot read out this information directly from the compressed signal. Rather, the neurons in the receiving region must first reproduce the original sparse pattern of activity before they can decode from it the quantities of interest (perhaps this sparse representation has the same advantages in the receiving region as in the sending region). Moreover \'97 and this is central to the hypothesis \'97 they must accomplish this feat without knowledge of the random projection applied by the sender, i.e. via dictionary learning. \
\
In this way, the \'91qualitative similarity\'92 (i.e. between the quantities of interest) to which you refer is contingent on the uniqueness and stability of sparse representations. We have proven that *any* dictionary learning procedure implemented in the receiving region (biologically plausible algorithms are an active area of research [cite]) does \'91well enough\'92 will indeed yield a sparse activity pattern (at least, up to an inherent relabelling ambiguity) that is \'91similar\'92 to the original pattern in the sending region. (The stability criterion need not be Lipschitz continuity, but it just so happens to be; note that Def. 1 imposes no such constraint.)\
\
It is entirely possible, and in our opinion very likely, that the encoding and decoding procedures utilized by actual nervous networks are far more sophisticated. The science is just not there yet. At the very least, we have proven that the only published hypothesis regarding how this could be done is well-founded, mathematically.\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
(d) An editor who doesn't want to get suckered is always looking at papers\
to find a very specific 'gadget' or 'gadgets' that mark the distinction\
between the submitted work and 'obvious', 'trivial' work.\
\
I can't really tell if the hypergraph construct is such a gadget. It's\
really only handled in passing and only two examples are mentioned.\
\
RESPONSE:\
\
In our previous response, we described in detail how the perspective we take on the problem enables the following insights beyond a \'92trivial\'92 (not really!) extension of (HS15) to the noisy case:\
\
1) a subset of dictionary elements is recoverable even if dictionary size is overestimated,\
2) data require only a polynomial number of distinct sparse supports,\
3) the spark condition is not a necessary property of recoverable dictionaries.\
\
All of these generalizations are direct consequences of defining a new matrix lower bound induced by a hypergraph. We therefore believe the term \'91gadget\'92 strongly downplays the insight this construct offers into the problem, similar to how it would downplay the role a gear-shift plays in a car. It is certainly not only treated in passing, as it is baked into every one of our theorems. We have provided only a few obvious hypergraph constructions because the point is that there even *is* a theory of hypergraphs underlying this problem. Our examples briefly demonstrate the gains to be made from this discovery, so as to entice the community into fully exploring its ramifications in practice. \
\
[Cite the hypergraph arxiv paper from Dalhousie? Or will this make us seem less special?]\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
I can't really tell if your cardinality bound on the number of samples\
needed for a stable representation is one such gadget. I am unable on my\
own to conjure up an example where I might have thought an exponential\
number of samples would be required but you show me very explicitly that\
no, a dramatically smaller number is required.\
\
RESPONSE:\
\
If there is any one \'91gadget\'92 to which we may credit our results, it is the pigeon-hole principle. While we have not managed to exorcise exponentiality from the number of required samples in general (which may very well be impossible), we have applied the pigeon-hole principle in a novel way (Lem. 5) to prove that only a polynomial number of sparse supports are necessary in general for stable identification of the generating dictionary. In our view, this lends much more legitimacy to the use of this model in practice, where data in general are unlikely to exhibit the exponentially many possible k-wise combinations of dictionary elements (as all previous results have required).\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
So I'm at a standstill. I would need to be convinced that you have actual\
gadgets that go beyond what I would have come up with and that the\
identifiability problem is dramatically different than what an 'obvious' or\
'easy-to-guess' solution might say, by showing me something very concrete\
that I can understand. The lack of any explicit implementation on a\
computer on a specific example doesn't help.\
\
RESPONSE:\
\
Frankly, we feel at a loss here. It seems to us that assessing a solution as \'91obvious\'92 after-the-fact, when it has already informed one\'92s intuition, is fundamentally unfair. It\'92s always easy to guess, easier to guess wrong, and hardest to prove. Still, we offer a nice surprise for everyone who reads our paper and realizes how absurd it is that they have never come across Def. 1 anywhere in the literature on dictionary learning before.\
\
Problems 1 and 2 have been studied for two decades now, and no one has pointed out the relation between them, nor the existence of the underlying hypergraph structure and its numerous ramifications. Our solution to Prob. 2, that of most interest to practitioners of dictionary learning methods, is the first of its kind in both the noise-free and noisy domains. If these solutions are the \'91easy-to-guess\'92, \'91obvious\'92 solutions, then so be it; we cannot change geometry. Actually, we prefer results for which intuition can play its role in lending credence to the truth. Would you have even believed us if we claimed polynomial sample complexity?\
\
Reviewer #2\
Suitable Quality? Yes\
Sufficient General Interest? No\
Conclusions Justified? Yes\
Clearly Written? No\
Procedures Described? Not Applicable\
Willingness to Re-review? Yes\
\
\
Comments\
In my opinion, while the extension from exact to noisy stability of\
dictionary learning (DL) is significant, the fact that the analysis relies\
on metrics of the data that are not feasible to compute limits its impact\
to the scientific community beyond computer science and applied\
mathematics. While the authors state in their response that their results\
validate the extensive successful use of DL in practice, there seems to be\
little impact to this given that the methodology is already in widespread\
use; instead, practical criteria that allows practitioners to establish\
whether the data model obtained from DL is optimal or not would have very\
high impact. My questions in the review were probing whether any\
contribution of this type was present, and the responses appear to point\
toward a negative answer.\
\
RESPONSE:\
\
What sets our work apart from the vast majority of results in the field is that they do not depend on any kind of assumption about the particular random distribution from which the sparse supports, coefficients, or dictionary entries are drawn. As a result, our paper only directly justifies \'91in principle\'92 the inferences of those who apply DL methods to inverse problems in their research. Our \'91target\'92 practitioner, however, is not the scientist who seeks a black box yielding the probability of their inferences; rather, it is the statistician or computer scientist who builds these black boxes on a case-by-case basis. On the statistical side, we have forged a path for high-impact results that derive from our deterministic guarantees the statistical criteria underlying the inferences in more domain-specific probabilistic models. On the computational side, we halve the work it takes to prove the consistency of any dictionary learning algorithm. We expect our work will make many such assists in the coming seasons, and just as in the game of hockey, we think this deserves as much acknowledgement as the goal.}