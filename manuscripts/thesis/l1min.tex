%% thesis.tex 2014/04/11
%
% Based on sample files of unknown authorship.
%
% The Current Maintainer of this work is Paul Vojta.
%
% To compile this file, run "latex thesis", then "biber thesis"
% (or "bibtex thesis", if the output from latex asks for that instead),
% and then "latex thesis" (without the quotes in each case).

\documentclass{ucbthesis}

% *** CITATION STUFF ***
%\usepackage{biblatex} 
%\bibliography{references} 
% the above doesn't work, using this instead:
\usepackage[backend=bibtex,style=numeric]{biblatex}
\addbibresource{references}

% *** MATH STUFF ***
\usepackage{amsmath, amssymb, amsthm} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{question}{Question}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

% *** ALIGNMENT STUFF***
\usepackage{array}
\usepackage{bm}
\usepackage{rotating} % provides sidewaystable and sidewaysfigure
\usepackage{url}

% Double spacing, if you want it.  Do not use for the final copy.
% \def\dsp{\def\baselinestretch{2.0}\large\normalsize}
% \dsp

% If the Grad. Division insists that the first paragraph of a section
% be indented (like the others), then include this line:
% \usepackage{indentfirst}

\addtolength{\abovecaptionskip}{\baselineskip}

%\hyphenation{mar-gin-al-ia}
%\hyphenation{bra-va-do}

\begin{document}

We can already see this may be possible by examining the case $k=1$. Consider the dataset generated as in \eqref{LinearModel} in the noiseless case $\eta = 0$, i.e.:
\begin{align}\label{LinearModel}
\mathbf{z}_i = \mathbf{A}\mathbf{x}_i,\ \ \  \text{$i=1,\ldots,N$},
\end{align}
with the additional constraint that $\|\mathbf{Ae}_i\|_2 = 1$ for all $i \in [m]$. Consider the following ``convexified" version of 
%Prob.~\ref{OptimizationProblem}:
Prob. 2:

\begin{problem}\label{ConvexifiedOptimizationProblem}
Find a $n \times m$ matrix $\mathbf{B}$ with $\|\mathbf{Be}_i\|_2 = 1$ for all $i \in [m]$ and vectors \mbox{$\mathbf{\overline x}_1, \ldots, \mathbf{\overline x}_N$} solving:
\begin{equation}\label{l1min}
\min \sum_{i = 1}^N \|\mathbf{\overline x}_{i}\|_1 \ \
\text{subject to} \ \ \mathbf{z}_i = \mathbf{B}\mathbf{\overline x}_i, \ \text{for all $i$}.
\end{equation}
\end{problem}

%\begin{conjecture}
%Fix a matrix $\mathbf{A}$ with $\|\mathbf{A}\|_2=1$ and vectors $\mathbf{x}_i$ satisfying also the assumptions of Thm.~\ref{SLCopt}. Every solution to Prob.~\ref{ConvexifiedOptimizationProblem} satisfies BLAH and BLAH.
%\end{conjecture}

\begin{proposition}
Fix $c > 0$. If $\mathbf{x}_i = c\mathbf{e}_i$ for $i = 1, \ldots, m$, then every solution to Prob.~\ref{ConvexifiedOptimizationProblem} satisfies $\mathbf{A} = \mathbf{BP}$ and $\mathbf{x}_i = \mathbf{P}^\top \mathbf{\overline x}_i$ for some $m \times m$ permutation matrix $\mathbf{P}$.
\end{proposition}

\begin{proof}
Fix $i \in [m]$. Writing $\mathbf{\overline x}_i = \sum_{j=1}^m \overline c^{(i)}_j \mathbf{e}_j$, we have:
\begin{align}\label{gtc}
c = \|c\mathbf{Ae}_i\|_2 = \|\mathbf{B \overline x}_i\|_2 = \|\sum_{j=1}^m \overline c^{(i)}_j \mathbf{Be}_j\|_2 \leq \sum_{j=1}^m |\overline c^{(i)}_j| \|\mathbf{Be}_j\|_2 = \|\mathbf{\overline x}_i\|_1
\end{align}
So $\|\mathbf{\overline x}_i\|_1 \geq c$ for all $i \in [m]$. Therefore $\sum_{i=1}^m \|\mathbf{\overline x}_i\|_1 \geq mc$. But since $\mathbf{B} = \mathbf{A}$ and $\mathbf{\overline x}_i = \mathbf{x}_i$ ($i = 1, \ldots, m$) satisfy the constraints of the minimization problem, we must have $\sum_{i=1}^m \|\mathbf{\overline x}_i\|_1 \leq \sum_{i=1}^m \|\mathbf{x}_i\|_1 = mc$ also. Thus $\sum_{i=1}^m \|\mathbf{\overline x}_i\|_1 = mc$. Since again $\|\mathbf{\overline x}_i\|_1 \geq c$ for all $i \in [m]$, we must have $\| \mathbf{\overline x}_i\|_1 = c$ for all $i \in [m]$.

Recalling \eqref{gtc} we therefore have $c = \|\mathbf{B\overline x}_i\|_2 \leq \|\mathbf{\overline x}_i\|_1 = c$, with equality only when $\overline c^{(i)}_j \mathbf{Be}_j$ are colinear. This would be the case either if $\mathbf{\overline x}_i$ is $1$-sparse, in which case we may apply 
%Thm.~\ref{DeterministicUniquenessTheorem}
Thm. 1
to guarantee both dictionary and code recovery, or $\mathbf{B}$ has colinear columns. In the latter case, the same guarantees hold for a suitable submatrix of $\mathbf{B}$ containing one representative column from every colinear set (note that since $\|\mathbf{Be}_j\| = 1$ for all $j \in [m]$, these columns are identical up to a sign).
\end{proof}


%\begin{proof}
%Now, writing $\mathbf{\overline x}_i = \sum_{j=1}^m \overline c_j\mathbf{e}_j$, we have:
%\begin{align}
%\sum_{j=1}^m \overline c_j^2 =  \left( \sum_{j=1}^m  |\overline c_j| \right)^2 = \sum_{j=1}^m \overline c_j^2 + \sum_{j=1}^m |\overline c_j |\sum_{\ell \neq j} | \overline c_\ell |
%\end{align}
%We therefore have $\overline c_j \sum_{\ell \neq j} | \overline c_\ell | = 0$ for all $j \in [m]$. Since $\mathbf{\overline x}_i \neq 0$, we must have $\overline c_j \neq 0$ for at least some $j \in [m]$, in which case $c_\ell = 0$ for all $\ell \neq j$, and we can be sure $\mathbf{\overline x}_i$ is a $1$-sparse vector. Since this applies for all $i \in [m]$, we may appeal to Thm.~\ref{DeterministicUniquenessTheorem} (with $k=1$).
%\end{proof}


\end{document}
