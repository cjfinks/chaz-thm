\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {0.1}{\ignorespaces \textbf {Learning a dictionary from increasingly noisy data}. The (unraveled) basis elements of the $8 \times 8$ discrete cosine transform (DCT) form the 64 columns of the left-most matrix above. Three increasingly imprecise dictionaries (columns reordered to best match original) are recovered by FastICA \cite {hyvarinen2000independent} trained on data generated from $8$-sparse linear combinations of DCT elements corrupted with additive noise (increasing from left to right).}}{8}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {0.2}{\ignorespaces Probability of successful dictionary and code recovery (as per Thm. 1\hbox {}) for a number of samples $N$ given as a fraction of the deterministic sample complexity $N = |\mathcal {H}|[(k-1){m \atopwithdelims ()k} + 1]$ when $\mathcal {H}$ is taken to be the set of $m$ consecutive intervals of length $k$ in a cyclic order on $[m]$ (i.e. $|\mathcal {H}|=m$). Successful recovery is nearly certain far below the deterministic sample complexity.}}{18}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {0.3}{\ignorespaces The constant $C_2(\mathbf {A}, \mathcal {H})$ computed for generic unit-norm matrices $\mathbf {A} \in \mathbb {R}^{n \times m}$ and the hypergraph $\mathcal {H}$ consisting of the rows and columns formed by arranging the elements of $[m]$ into a square grid. The results suggest that the bound is typically concentrated around a reasonable value.}}{19}
