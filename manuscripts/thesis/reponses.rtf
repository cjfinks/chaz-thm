{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica-Oblique;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww33400\viewh19080\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\i\fs24 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1\i0 \cf0 HOW DO WE IMPROVE ON HS15?\
\
\'97\'97\'97\'97\'97\'97\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 REVIEWER: Specifically, the authors could address why going from the noise-free case to the noisy case is not incremental. PNAS specifically targets submissions that go beyond what could be published in a more specialized journal. Could the authors, for example, contrast the proof technique between the noisy and noise-free cases? Or, perhaps, the practical implications?\
\
RESPONSE: With all due respect to the reviewer, we believe the most compelling practical implication of a noisy result over a noiseless result is plainly evident: never has there been an experiment without noise and an unstable model is therefore useless in practice. We assume the reviewer must instead be concerned with what additional practical "take-away" message our manuscript contains.  One significant one is the outline of a procedure that can (sufficiently) affirm if one\'92s proposed solution to Prob. 1 or Prob. 2 is indeed unique. We have updated the manuscript with an explicit statement of this fact, and we outline one such procedure here:\
\
Given a dictionary A and codes x_i that solve Problem 1 (e.g., learned by any algorithm), check that that they satisfy the assumptions of Thm. 1:\
   - List the support sets of the x_i.\
   - Discard those for which there are (k-1)\{m \\choose k\} or fewer x_i with that support.\
   - Discard those for which the supported x_i are not in general linear position. (This should also exclude any supports less than k in size.)\
   - From the support sets that remain, list all SIP-satisfying hypergraphs that are subsets of this set.\
   - Discard those for which L_\{2H\}(A) = 0.\
   - For each of the remaining hypergraphs, determine the constant C_1 from A and the x_i (or for every subset of the x_i of size (k-1)\{m \\choose k\}).\
   - Check that the derived upper bound on \\varepsilon exceeds that of the original problem. If yes, the solution is "unique".\
\
There are other practical (polynomial time) implications of our discovery of sufficient combinatorial designs for support sets of generating codes x_i. Moreover, we have shown that a subset of dictionary elements are recoverable even if the number of dictionary elements in total is unknown; these observations are discussed in more detail below. \
\
The reviewer also raises the concern that, regardless of practical implications, our results may amount to an incremental advance over those of (HS15) by way of similar techniques. We disagree with this assessment on both counts: our main result (Thm. 1) goes far beyond a straightforward extension of that in (HS15) to the noisy case and this required a significant deviation from their approach.\
\
It is understandable that the reviewer may have thought otherwise, considering how the proof of Thm. 1 is presented in the manuscript. As observed by our second reviewer, however, the extension to the noisy case did indeed require a novel combination of several results in the literature. Specifically, the main difficulty was to generalize Lemma 1 to the case where the k-dimensional subspaces spanned by corresponding (through the map \\pi) submatrices of A and B are assumed only to be proximal (small \'91d\'92), and not identical as in (HS15). In contrast to the noiseless case, here it must be explicitly demonstrated that this proximity relation is propagated through the repeated intersections of these submatrix spans all the way down to the spans of dictionary elements themselves. We designed and proved Lemma 3 to address this issue, which draws its bound from the convergence guarantees of an alternating projections algorithm first proposed by von Neumann. This result, combined with a little known fact about the distance \'91d\'92 requiring proof in (M10), constitute the more obscure components of the deduction in Eq. (26). To reiterate, this step is completely trivial in the noiseless case and required no mention for the inductive steps taken in (HS15).  \
\
Our proof of Lemma 1 diverges perhaps even more significantly from the approach taken in (HS15) by way of Lemma 4. Key to our reduction of the sample complexity given in (HS15) by an exponential factor is the introduction of a combinatorial design (the "singleton intersection property") for support sets. Since in this case the map \\pi from supports in the hypergraph to \{[m] \\choose k\} is not surjective, one can not apply the same inductive method as in (HS15), which freely chooses supports in the codomain to intersect at (k-1) nodes and map back to some corresponding (k-1)-sized intersection of supports in the domain. Instead, we demonstrate the surprising fact that by pigeonholing the images of supports in the (SIP-satisfying) hypergraph H, one can still guarantee a bijection between the nodes and therefore the subspaces spanned by individual dictionary elements. We note that it was necessary to forgo inductive methods altogether in order to prove this fact for all hypergraphs satisfying the SIP; otherwise, we would require that the supports in the hypergraph have intersections of size k\'92 for every k\'92 < k (e.g., this is not the case for the small SIP example we give consisting of the rows and columns of nodes arranged in a square grid). Again, to our surprise, it so happens that this new induction-less argument easily generalizes to the case where B has an arbitrary number of columns, in which case we find that a one-to-one correspondence exists between a subset of columns of A and B of a size that has a nice closed-form expression for regular SIP hypergraphs.\
\
To be clear, the new perspective we take on the problem yields the following powerful conclusions beyond those of a straightforward extension of (HS15) to the noisy case:\
\
1) An extension to the case where the number of dictionary elements is unknown: The results of (HS15) only apply to the case where the matrix B has the same number of columns as A. We forgo this assumption and show that B must have at least as many columns as A and contains (up to noise) a subset of the columns of A. The size of this subset depends on a simple relation between the number of columns of B and the regularity of the support-set hypergraph.\
\
2) A significant reduction in sample complexity: To identify the n x m generating dictionary A, (HS15) require that data be sampled from every k-dimensional subspace spanned by the m columns of A (that is, \{m \\choose k\} subspaces in total). We show that the data need only be sampled from m subspaces in total (e.g. those supported by consecutive intervals of length k in some cyclic order on [m]) and in some cases as few as 2\\sqrt\{m\} (e.g. when m = k^2, take the supports to be the rows and columns of [m] arranged in a square grid). Moreover, if the size of the support set of reconstructing codes is known to be polynomial in m and k, then the pigeonholing argument in the proof of Thm. 1 requires only a polynomial number of samples distributed over a polynomial number of supports; thus, N is then polynomial in \\bar m, k. This point was only hinted at in the Discussion section of our original submission, but we have included it in the updated manuscript to make clear the power of our approach over that taken in (HS15). \
\
3) No spark condition requirement for the generating matrix A: One of the mathematically significant contributions made by (HS15) was to forgo the constraint that the recovery matrix B also satisfy the spark condition, in contrast to all previously known uniqueness results which (either explicitly or implicitly) made this assumption.  Our proof is powerful enough to show that, in fact, even the matrix A need not satisfy the spark condition to be identifiable! Rather, it need only be injective on the union of subspaces with supports that form sets in a hypergraph satisfying the singleton intersection property (SIP). (For example, consider the matrix A = [e_1,.., e_5, v] where v = e_1 + e_3 + e_5, and take H to be the set of all consecutive pairs of [m] arranged in cyclic order. Then A satisfies the assumptions of Thm. 1 for dictionary recovery without satisfying the spark condition.) We had omitted this point in our original submission of the manuscript to keep things simple, but we have decided to include this fact in our revision. This also required us to redefine the restricted matrix lower bound L_k to be in terms of a hypergraph H (L_H in the revised manuscript), which is an interesting object for further study in its own right. \
\
We must also reiterate here that our solution to Prob. 2, that of most interest to practitioners of dictionary learning methods, is to our knowledge the first of its kind in both the noise-free and noisy domains. We recognize that this was not clearly communicated in the Discussion section of our submitted manuscript.\
\
Finally, we should mention that our main mathematical results justify the neurally plausible model of bottleneck communication between brain regions, first explained in depth in this NIPS paper (IHS10) and then in a review of sparse linear coding applications to neuroscience (GS12).\
\
-----------------\
\
REVIEWER:The paper could also compare the sample complexity results on N with those given in the noise-free case. For example, Table I in reference (18) gives a comparison of sample complexity requirements for different conditions in the noise-free case. Where would the results of this paper stand in that table?\
\
\
RESPONSE: In brief, since all of the theorems and corollaries in (HS15) are corollaries of our theorems, all sample complexities improve by our work. The main point of difference between our statements about sample complexity and those of (HS15) is the assumed constraint on the underlying support set of sparse codes x_i. Our theory of combinatorial designs (the \'93singleton intersection property\'94) requires only that there be a sufficient number of x_i drawn from each support in a hypergraph satisfying the SIP, whereas the theory in (HS15) requires that a sufficient number of x_i be drawn from every support of size k in [m]. Below, we make an explicit comparison with Table I in (HS15) row by row, assuming for our results that sufficient data have been sampled from all supports in a hypergraph H satisfying the SIP:\
\
ROW I. Our result here is |H|(k-1)\{m \\choose k\} + 1. We improve on the result in (HS15) by an exponential factor since our theory does not require that data be sampled with every possible support of size k in [m]; as noted in our manuscript, for every k < m there exists a hypergraph H with |H| = m that satisfies the SIP. \
\
ROW 2. Our result here is again |H|(k-1)\{m \\choose k\} + 1 with certainty, not with almost certainty (i.e. probability 1) as in (HS15). Here, the authors in (HS15) have applied probabilistic arguments to achieve an almost certain result with a sample complexity on the same order as that for which we have achieved a certain result by means of our theory of combinatorial designs.    \
\
ROW 3. We cannot make a direct comparison here because we have not calculated the probability that a random set of supports satisfy the SIP (see our response to a similar question by the second reviewer). We think this is an interesting problem for the community to solve; regardless, the result of (HS15) here is still implied by our more general theory.\
\
ROW 4. Our result here is |H|(k-1)\{m \\choose k\} + 1 with probability 1. Here is the only case where the sample complexity stated in (HS15) is technically better than ours, but it differs in flavor: their x_i are assumed to be distributed as (k+1) samples per support for every support of size k in [m], whereas ours supposes that (k-1)\{m \\choose k\} codes x_i are distributed over each support in some hypergraph H satisfying the SIP. Regardless, the (technically) better result of (HS15) is still implied by our more general theory in the case \\varepsilon = 0 with the addition of their probabilistic argument.\
\
ROW 5: Again, we cannot make a direct comparison here for the reason stated in ROW 3.\
\
To sum up, while we appreciate this question, we feel that since our results improve in every case except for one noise-free technicality (ROW 4), and since the results of (HS15) are all entailed anyway by our more general theory, an explicit comparison such as that which we provided above would not be the best use of our limited space in the manuscript; although we are willing to add a discussion about this if necessary. Many thanks for your careful review of the mathematics. We have tried and continue to try to be as clear, concise, and correct as we possibly can to elevate this work into the top echelon of applied mathematical theory papers.\
\
\'97\'97\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 REVIEWER: An editor who doesn't want to get suckered is always looking at papers\
to find a very specific 'gadget' or 'gadgets' that mark the distinction\
between the submitted work and 'obvious', 'trivial' work.\
\
I can't really tell if the hypergraph construct is such a gadget. It's\
really only handled in passing and only two examples are mentioned.\
\
-------------------------------------\
\
RESPONSE:\
\
In our previous response, we described in detail how the definition of a new matrix lower bound induced by a hypergraph enables the following advancements in our understanding of Prob. 1 beyond an extension of previous results to the noisy case:\
\
1) a subset of dictionary elements is recoverable even if dictionary size is overestimated,\
2) data require only a polynomial number of distinct sparse supports,\
3) the spark condition is not a necessary property of recoverable dictionaries.\
\
Given these insights, we believe the term "gadget" strongly downplays the power of this construct. Certainly, we don't see it as being only treated in passing, as it is incorporated into every one of our theorems. The examples we provide briefly demonstrate the gains to be made from this discovery, so as to entice the community into fully exploring the ramifications of an underlying theory of hypergraphs in practice. \
\
\'97\'97\'97\'97\'97\'97\
\
REVIEWER: So I'm at a standstill. I would need to be convinced that you have actual\
gadgets that go beyond what I would have come up with and that the\
identifiability problem is dramatically different than what an 'obvious' or\
'easy-to-guess' solution might say, by showing me something very concrete\
that I can understand. The lack of any explicit implementation on a\
computer on a specific example doesn't help.\
\
-------------------------------------\
\
RESPONSE:\
\
It seems to us that assessing a solution as "obvious" after-the-fact -- when it has already informed one's intuition -- is a bit unfair. It is always easy to guess, easier to guess wrong, and hardest to prove. Still, if unintuitive results are what sell these days, we offer a nice surprise for everyone who reads our paper and realizes how strange it is that they have never come across a definition of the problem akin to Def. 1 anywhere in the literature on dictionary learning before.\
\
Problems 1 and 2 have been studied for two decades now, and no one has pointed out the relation between them, nor the existence of the underlying hypergraph structure. Our solution to Prob. 2, that of most interest to practitioners of dictionary learning methods, is the first of its kind in both the noise-free and noisy domains. If these solutions are the "easy-to-guess", "obvious" solutions, then so be it; we cannot change geometry. Actually, we prefer results for which intuition can play its role in lending credence to the truth.\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
PRACTICAL IMPLICATIONS:\
\
REVIEWER: When is it possible to determine (numerically) if enough data has been gathered to obtain an accurate estimate of the underlying sparsity dictionary?\
-----------------\
\
RESPONSE: A statement of this very general nature requires that we calculate the probability that all of the conditions of Thm. 1 are satisfied by random data. We have provided an "almost certain" guarantee of this flavor in Cor 2, wherein the sparse vector supports are constrained to form a hypergraph satisfying the SIP. It would be useful and interesting to calculate furthermore the probability that random supports form a hypergraph satisfying the SIP. We have opted not to include these calculations due in part to space constraints, but also because we believe that what sets our work apart from the vast majority of results in the field is their deterministic nature, e.g. they do not depend on any kind of assumption about the particular random distribution from which the sparse supports, coefficients, or dictionary entries are drawn.\
\
-----------------\
Is there any new intuition to be had regarding the necessary sampling of the union of subspaces from this analysis (with respect to the intuition obtained from existing uniqueness results)?\
-----------------\
\
RESPONSE: Yes, there is indeed. While all existing uniqueness results require a sufficient number of samples be drawn from each of \{m \\choose k\} possible supports, we have shown that it is enough that a sufficient number of samples be drawn from every support in a subset of drastically smaller size, provided this subset forms a regular hypergraph satisfying the SIP. As was pointed out in our manuscript, a support set of size m satisfying these criteria can always be constructed for any k < m (e.g. take the consecutive intervals of length k in some cyclic order on [m]), and in certain cases such a set can be constructed from as few as 2\\sqrt(m) supports (e.g. when m = k^2, take the supports to be the rows and columns of [m] arranged in a square grid).\
\
\'97\'97\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 REVIEWER: \
This is at best about local stability, rather than stability. The size\
of the neighborhood where you're getting stability is presumably almost\
infinitesimal in general. While I agree that the paper is written in a\
quantitative way and has as you say constants which can be quantitative, it\
is I would say after a few readings, more\
seemingly-quantitative-but-actually-purely-qualitative.\
If it could be shown to be surprisingly effective in some specific case, I\
might withdraw from this position.\
\
-------------------------------------\
\
RESPONSE:\
\
[Files included: 2017-06210R-AuthorResponseManuscript.pdf, compute_C2.py, decomp_svd.py, sample_complexity.py, C2_Grid_m64_n32_k8_r2_nt500.pdf, C2_Grid_m256_n128_k16_r2_nt500.pdf, C2_Grid_m1024_n512_k32_r2_nt50.pdf, prob_vs_samples.png, samples_vs_m.png]\
\
We are somewhat confused by this statement as we understand even the tightest possible noise bound to be necessarily "local" and approaching zero as the domain-restricted matrix lower-bound vanishes, e.g. as the number of dictionary elements grows. One need only consider the case k=1, where every dictionary element spans a ray through the origin, and each identifiable datum generated from the model of Eq. 1 lies within a cylinder of radius \\eta around one (and only one) of these rays. Clearly, for bounded data in a finite dimensional space, this radius cannot remain finite in the limit of infinitely many distinct rays.\
\
The issue seems to rather be that the bound (defined using the constant C1, which depends in turn on the constant C2) is, in general, so small that it is "practically infinitesimal". As is the case for any deterministic guarantee, we must entertain the "worst-case" scenario, where noise is isotropic and "what can go wrong, will go wrong\'94. Consequently, some degree of pessimism is forced upon us here, whereas calculations specific to the data at hand would consider the probability distribution of confounding noise and likely yield more forgiving probabilistic bounds. To respond to this valid concern nonetheless, we have investigated our deterministic constants more thoroughly.\
\
The denominator of C1 involves a relatively standard quantity in the field of compressed sensing (the "restricted isometry constant"), and it is known to be reasonable for many random distributions generating original dictionaries A and codes. The constant C2, on the other hand, depends on a much less well studied quantity \\xi computed from the "Friedrichs angle" between certain spans of A's columns. \
\
Upon re-examination of our proof, we have determined that our expression for C2 was egregiously sub-optimal. As it turns out, the minimum in the denominator of C2 need only be computed over subsets of H of size r+1 (where r is the regularity of the hypergraph) -- not over all subsets of H. Moreover, the 2^|H| in the numerator -- which in fact need only have been |H| in our previous submission -- can actually be set to r+1 (e.g. for the SIP hypergraph consisting of consecutive intervals of length k in [m] we have |H| = m, whereas r=k).\
\
To determine the practical utility of this adjusted constant, we have performed computer experiments (Python code "compute_C2.py", "decomp_svd.py" included) calculating it for some generic matrices with example hypergraphs from our paper's introduction (i.e. the rows and columns formed by arranging [m] into a square grid). The results suggest that the bound is actually quite reasonable; certainly, it is not \'93practically infinitesimal" (Figures "C2_Grid_m64_n32_k8_r2_nt500.pdf", "C2_Grid_m256_n128_k16_r2_nt500.pdf", "C2_Grid_m1024_n512_k32_r2_nt50.pdf", included over different latent dimensions m=64,256,1024).\
\
-------------------------------------\
\
REVIEWER: I can't really tell if your cardinality bound on the number of samples\
needed for a stable representation is one such gadget. I am unable on my\
own to conjure up an example where I might have thought an exponential\
number of samples would be required but you show me very explicitly that\
no, a dramatically smaller number is required.\
\
\
RESPONSE:\
\
If there is any one "gadget" to which we may credit our results, it is the pigeonhole principle, which we have applied in a way (see Lem. 4) that demonstrates that only a polynomial number of sparse supports are necessary in general for stable identification of the generating dictionary. In our view, this lends much more legitimacy to the use of the sparse linear coding model in practice, where data in general are unlikely (if ever) to exhibit the exponentially many possible k-wise combinations of dictionary elements, as all previous results have required.\
\
It may very well be impossible to to exorcise exponentiality from the number of required samples in the deterministic or almost-certain case. However, our guarantees can easily be extended to hold with some probability for any number of samples by appealing instead to a probabilistic pigeonholing at the point in our proof of Thm. 1 where the deterministic pigeonhole principle is applied to demonstrate that for every S in H, there exist k vectors x_i supported on S whose corresponding \\bar x_i all share the same support. (A famous example of such an argument is the counter-intuitive "birthday paradox", which demonstrates that the probability of two people having the same birthday in a room of twenty-three is in fact greater than 50%.) This point in the proof then has some probability of success, which must occur for all S in H in order for the proof as a whole to be valid.\
\
In this spirit, we have computed for all sample sizes up to our deterministic sample complexity the probability with which our guarantees still hold when H is one of the two example hypergraphs from our paper's introduction, the set of consecutive intervals of length k in a cyclic order on [m] (Figure "prob_vs_samples.png", Python code "sample_complexity.py"). The probability of our guarantees holding saturates when the number of samples reaches only a fraction of the deterministic sample complexity. \
\
We have also computed the number of samples required for our guarantees to hold with probability 99.9% for fixed k as m increases (Figure "samples_vs_m.png"). The ratio of the number of samples required for this 99.9% guarantee with respect to the number of samples required by the deterministic 100% guarantee of Thm. 1 tends to zero as m increases.\
\
It would be a simple matter to add discussion of these facts to the manuscript, if so desired.\
\
-------------------------------------\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 REVIEWER:\
In my opinion, while the extension from exact to noisy stability of\
dictionary learning (DL) is significant, the fact that the analysis relies\
on metrics of the data that are not feasible to compute limits its impact\
to the scientific community beyond computer science and applied\
mathematics. While the authors state in their response that their results\
validate the extensive successful use of DL in practice, there seems to be\
little impact to this given that the methodology is already in widespread\
use; instead, practical criteria that allows practitioners to establish\
whether the data model obtained from DL is optimal or not would have very\
high impact. My questions in the review were probing whether any\
contribution of this type was present, and the responses appear to point\
toward a negative answer.\
\
-------------------------------------\
\
RESPONSE:\
\
What sets our work apart from the vast majority of results in the field is that they are deterministic, and do not depend on any kind of assumption about the particular random distribution from which the sparse supports, coefficients, or dictionary entries are drawn. Consequently, our paper directly justifies only "in principle" the inferences of those who apply dictionary learning methods to inverse problems in their research. But this is unavoidably the case for NP-hard problems.\
\
In the realm of practicality, we have spelled-out the problem (i.e. estimate C1) for statisticians, who will derive from our deterministic guarantees the statistical criteria for inference in more domain-specific probabilistic models, and we have cut in half the work it takes a computer scientist to prove the consistency of any dictionary learning algorithm (i.e. prove that the algorithm converges to any solution encoding the data to within the epsilon in Eq. 8). Our work is the assist to these many impactful results to come, and (just as in hockey) we feel this deserves as much acknowledgement as any contingent goal.\
\
-------------------------------------\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
THEORETICAL IMPLICATIONS\
\
REVIEWER: \
Point (a) is important because when you say these results somehow\
validate various neuroscience ideas, I am saying in the back of my mind\
that you're bravely talking yourself on a ledge. Namely, nothing in\
neuroscience can really depend on infinitesimal stability. Trying to make a\
claim like this seems to undermine your credibility with me. It seems the\
referees had the same problem.\
\
-------------------------------------\
\
RESPONSE:\
\
Our response to point (a) aside, and though we are not inclined to assume that neural circuits are incapable of extreme precision when necessary, we actually share your skepticism here regarding the modern theory of "sparse coding" (O04) for efficient representation of sensory input in brains (vision: (O96), (H96), (B97), (vH98); audio: (B96), (S06), (C12)), as well as corresponding models of bottleneck communication between neurons that utilize ideas from compressed sensing ((C10), (I10), (G12)). Nonetheless, our paper confirms the well-posedness of the central noisy sensory coding problem that specialists in these fields have suggested neural circuits might be solving, and we have verified (for the first time) that the bottleneck communication model represents a neurally plausible way of faithfully transmitting sparse sensory representations through a noisy bandwidth-limited channel.\
\
If the editors are still reluctant to become complicit in propagating unproven hypotheses of neural computation, we have no qualms downplaying this application in the manuscript. Perhaps instead, we could elaborate more on implications for the repeatability of discoveries in experimental science contingent on machine learning, which includes an explanation for the universality of the above results (e.g. "Gabors") in sensory representation (independent of any particular theory of brain computation). For example, over the years there have been many appeals to dictionary learning with sparseness constraint for uncovering latent structure in data (e.g. forgery detection (H10), (O10); brain recordings (J01), (A14), (L16); gene expression (Wu16)), several of which appear in PNAS. \
\
-------------------------------------\
\
REVIEWER: It's not clear why exact uniqueness should be so important for\
neuroscience. It seems more likely that mere similarity is what's maybe\
important. In the same sense it's not clear why formal Lipschitz stability\
in the mathematical sense should be so important for neuroscience. It seems\
more important that some sort of qualitative similarity should persist\
under perturbations.\
\
RESPONSE:\
\
Unfortunately, how brains work is still largely a mystery. There are, however, some ideas for why certain models are more appealing than others. Here is how we rationalize the application of unsupervised sparse coding to the theory of bottleneck communication:\
\
Suppose some quantities of interest are encoded in the sparse activity of neurons in a sending region (it has been proposed that neural activity in certain brain regions is "sparse", e.g. for energy efficiency). These quantities are to be transmitted to some other region through as few wires (axons) as possible, e.g. due to space constraints inside the skull.\
\
The hypothesis is that the brain, up-to date on the latest fad, solves this problem by applying a (noisy) "random" projection into the lower-dimensional space spanned by these axons. The neurons in the receiving region are then tasked with decoding the quantities of interest from this compressed signal. Suppose, however, that they cannot read out this information directly from the compressed signal. Rather, the neurons in the receiving region must first reproduce to some extent the original sparse pattern of activity before they can decode from it the quantities of interest (perhaps a sparse representation is necessary or advantageous in the receiving region, just as in the sending region). Moreover - and this is central to the hypothesis - they must accomplish this feat without knowledge of the random projection applied by the sender, i.e. via dictionary learning with a sparseness constraint. \
\
In this way, the persistence of a "qualitative similarity", modeled mathematically as the formal similarity between the decodable quantities of interest in the two regions, is contingent on the uniqueness and stability of sparse representations. We have proven that any dictionary learning procedure implemented in the receiving region (biologically plausible algorithms are an active area of research (P15)) that does "well enough" will indeed yield a sparse activity pattern that is similar (at least, up to inherent relabelling ambiguity) to the original pattern in the sending region, as required. \
\
It is entirely possible, and in our opinion very likely, that the encoding and decoding procedures implemented in nervous tissue are far more sophisticated. The science is just not there yet. At the very least, we have demonstrated that the only published hypothesis regarding how this could be done is well-founded, mathematically.\
\
-------------------------------------\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 REVIEWER:\
The perturbation bound is used by the authors to provide some explanation for why, in practice, different methods lead to similar dictionaries. Can that also be used to explain the second (bold) part of their observation that \'93[these waveforms] appear in dictionaries learned by a variety of algorithms trained with respect to different natural image datasets.\'94? Just curious.\
\
RESPONSE:\
To be clear, this (bold) observation is just an observation which motivates the work. Imagining natural image patches to form a population which satisfies (1) with respect to some ground truth dictionary A, we have shown that there exists a sufficient number of samples to uniquely identify it or something close to it via approximate solutions. Given this fundamental stability and uniqueness of the problem solutions, the fact that a variety of methods geared to approximately solve Problems 1 and 2 all seem to capture similar structure is much less surprising. \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
FUTURE DIRECTIONS\
\
NOTE: Structured dictionaries? Just derive the bounds specific to them. Could be efficiently computable in specific cases.\
\
REVIEWER\
\
As l0 norm is intractable, does all the properties still hold for l1 norm minimization? Can we solve it efficiently (polynomial time) to global optimality? Similar issue for other works, such as\
https://arxiv.org/pdf/1807.05595.pdf\
\
RESPONSE:\
We make no claims in this paper about l1-norm minimization, though we are hopeful a clever reader may find some way to build off our results and derive related guarantees for this continuous relaxation of the sparse coding problem.\
\
\
TODO : can we deterministically generate matrices injective on SIP hyper graphs? The (mCk)-hypergraph is one such possibility, so maybe not in general? But for special cases (convolution matrices..?)}