\chapter{Introduction}\label{Intro}

%\begin{itemize}
%\item Background to the topic.
%\item Brief review of current knowledge.
%\item Indicate the gap in knowledge. State the aim of the research and how it fits into the gap.
%\item Can include an outline of what follows.
%\end{itemize}

%Why do neuroscientists care about dictionary learning?

%Sparse representation modeling is an approach to signal processing that seeks to describe signals as linear combinations of a few elementary waveforms selected from a pre-specified ``dictionary". 

It is a longstanding practice in the field of signal processing to describe signals as linear combinations of elementary waveforms from a pre-specified ``dictionary". When this dictionary forms a basis for the signal space, every signal has a unique decomposition into these atomic components. In the simplest case, the basis is orthonormal and the representational coefficient scaling a given elementary waveform is merely the inner product of that waveform with the signal. %When non-orthogonal, the coefficients are computed by taking the inner product with respect to the components of the dictionary inverse.

Until recently, bases have been the default form of signal representation due largely to their simplicity. For many signal analysis tasks, however, there is no one basis expressive enough to reveal clearly all of the relevant features of the signal. For example, a signal can be decomposed into its constituent frequencies via the Fourier transform, a linear change of basis. If our signal can be either a sine wave or a delta function, neither the standard basis nor the Fourier basis can capture one case as effectively as it can the other. 

The need for greater freedom of expression led to the development of redundant signal representations utilizing overcomplete dictionaries containing more atoms than there are dimensions of the signal. In this case, there are infinitely many ways in which a signal may be decomposed into its constituent components, and the intention to seek the most informative such representation as measured by some task-specific cost function. 

A popular approach to the design of overcomplete dictionaries has been to seek one with respect to which every signal in the signal class of interest has a sparse representation; that is, it can be represented, or at least well-approximated, as a combination of only a few dictionary elements from the bunch. Concluding our example, the union of the standard basis with the Fourier basis is an over-complete dictionary with respect to which both sines and delta functions (or any superposition thereof) have the sparsest possible representations as combinations of its elementary waveforms.

Early approaches to sparse coding assumed, as was assumed in our recurring example, a model of the signal class from which a suitable sparsifying dictionary could then be derived. While such dictionaries are typically characterized by an analytic formulation and a fast implicit implementation, these models tend to be over-simplistic when applied to natural phenomena. 

In contrast, an alternative approach to dictionary design is conditioned on the assumption that the structure of complex natural phenomena can be more accurately extracted directly from a training dataset of signals, a process referred to as dictionary learning. In the seminal work \cite{Olshausen96}, the authors trained a dictionary for sparse representation of small image patches collected from a number of images of the natural environment. Remarkably, the relatively simple algorithm they applied produced dictionary elements sharing qualitative similarities with linear filters estimated from response properties of simple-cell neurons in mammalian visual cortex, which until then were more weakly described by the analytic Gabor filters (see also \cite{hurri1996image, bell1997independent, van1998independent}). Curiously, these waveforms (e.g., Gabor'-like wavelets) appear in dictionaries learned by a variety of algorithms trained over different natural image datasets, suggesting that learned features in natural signals may, in some sense, be canonical \cite{donoho2001can}.

% C&P: A key contribution to the area of dictionary learning was provided by Olshausen and Field in 1996 [34]. In their widely celebrated paper, the authors trained a dictionary for sparse representation of small image patches collected from a number of natural images. With relatively simple algorithmic machinery, the authors were able to show a remarkable resultVthe trained atoms they obtained were incredibly similar to the mammalian simple-cell receptive fields, which until then were only weakly explained via Gabor filters. The finding was highly motivating to the sparse representation community, as it demonstrated that the single assumption of sparsity could account for a fundamental biological visual behavior. Also, the results demonstrated the potential in example-based methods to uncover elementary structures in complex signal data.

Sparse coding is a modern signal processing technique that views each of $N$ observed $n$-dimensional signal samples as a (noisy) linear combination of at most $k$ elementary waveforms drawn from a ``dictionary" of size $m \ll N$ (see \cite{Zhang15} for a comprehensive review). 
%\IEEEPARstart{A}{}common modern approach to pattern analysis in signal processing is to view each of $N$ observed $n$-dimensional signal samples as a (noisy) linear combination of at most $k$ elementary waveforms drawn from some unknown ``dictionary" of size $m \ll N$ (see \cite{Zhang15} for a comprehensive review). 
Optimizing dictionaries subject to this and related sparsity constraints has revealed seemingly characteristic sparse structure in several signal classes of current interest (e.g., in vision \cite{wang2015sparse}). 
Of particular note are the seminal works in the field \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent}, which discovered that dictionaries optimized for coding small patches of ``natural" images share qualitative similarities with linear filters estimated from response properties of simple-cell neurons in mammalian visual cortex. Curiously, these waveforms (e.g., ``Gabor'' wavelets) appear in dictionaries learned by a variety of algorithms trained over different natural image datasets, suggesting that learned features in natural signals may, in some sense, be canonical \cite{donoho2001can}.

Motivated by these discoveries and more recent work relating compressed sensing \cite{eldar2012compressed} to a theory of information transmission through random wiring bottlenecks in the brain \cite{Isely10}, this thesis concerns itself with the following question: when is a dictionary indeed identifiable from data? Answers to this question may also have implications in practice wherever an appeal is made to latent sparse structure of data (e.g., forgery detection \cite{hughes2010, olshausen2010applied}; brain recordings \cite{jung2001imaging, agarwal2014spatially, lee2016sparse}; and gene expression \cite{wu2016stability}). 
While several algorithms have recently been proposed to provably recover unique dictionaries under specific conditions (see \cite[Sec.~I-E]{Sun16} for a summary of the state-of-the-art), few theorems can be invoked to justify the consistency of inference under this model of data more broadly. To my knowledge, a universal guarantee of the uniqueness and stability of learned dictionaries and the sparse representations they induce over noisy data has yet to appear in the literature.

In this work, it is proven very generally that uniqueness and stability is a typical property of learned dictionaries. More specifically, it is demonstrated that matrices injective on a sparse domain are identifiable from \mbox{$N = m(k-1){m \choose k} + m$} noisy linear combinations of $k$ of their $m$ columns up to an error that is linear in the noise (Thm.~\ref{DeterministicUniquenessTheorem}). In fact, provided $n \geq \min(2k,m)$, in almost all cases the problem is well-posed, as per Hadamard \cite{Hadamard1902}, given a sufficient amount of data (Thm.~\ref{robustPolythm} and Cor.~\ref{ProbabilisticCor}). 
These guarantees also hold for the related (and perhaps more commonly posed, e.g. \cite{rehnsommer2007}) optimization problem seeking a dictionary minimizing the average number of elementary waveforms required to reconstruct each sample of the dataset (Thm.~\ref{SLCopt}). To practical benefit, the derivations impose no restrictions on learned dictionaries (e.g., that they, too, be injective over some sparse domain) beyond an upper bound on dictionary size, which is necessary in any case to avoid trivial solutions (e.g., allowing $m = N$). %That is, every pair of solutions of comparable size has some number of dictionary elements in common (up to noise), and similarly so for the coefficients of sparse codes they induce.

\section{The dictionary learning problem(s)}

Let $\mathbf{A} \in \mathbb R^{n \times m}$ be a matrix with columns $\mathbf{A}_j$ ($j = 1,\ldots,m$) and let dataset $Z$ consist of measurements:
\begin{align}\label{LinearModel}
\mathbf{z}_i = \mathbf{A}\mathbf{x}_i + \mathbf{n}_i,\ \ \  \text{$i=1,\ldots,N$},
\end{align}
for $k$-\emph{sparse} $\mathbf{x}_i \in \mathbb{R}^m$ having at most $k<m$ nonzero entries and \emph{noise} $\mathbf{n}_i \in \mathbb{R}^n$, with bounded norm $\| \mathbf{n}_i \|_2 \leq  \eta$ representing our worst-case uncertainty in measuring the product $\mathbf{A}\mathbf{x}_i$. Let us first consider the following decidable\footnote{Note that Prob.~\ref{InverseProblem} is decidable for rational inputs $\mathbf{z}_i$ \cite{chrishillar} since the statement that it has a solution can be expressed as a logical sentence in the theory of algebraically closed fields, and this theory has quantifier elimination \cite{basu2006algorithms}.} formulation of the dictionary learning problem.
%The first mathematical problem we consider is the following.

\begin{problem}\label{InverseProblem}
Find a dictionary matrix $\mathbf{B}$ and $k$-sparse codes $\mathbf{\overline x}_1, \ldots, \mathbf{\overline x}_N$ that satisfy $\|\mathbf{z}_i - \mathbf{B}\mathbf{\overline x}_i\|_2 \leq \eta$ for all $i = 1,\ldots,N$.
\end{problem}

Note that every solution to Prob.~\ref{InverseProblem} represents infinitely many equivalent alternatives $\mathbf{BPD}$ and $\mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\overline x}_1, \ldots, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\overline x}_N$ parametrized by a choice of permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$. 
Identifying these ambiguities (labelling and scale) yields a single orbit of solutions represented by any particular set of elementary waveforms (the columns of $\mathbf{B}$) and their associated sparse coefficients (the entries of $\mathbf{\overline x}_i$) that reconstruct each data point $\mathbf{z}_i$. 

Previous theoretical work addressing the noiseless case $\eta =0$ (e.g., \cite{li2004analysis, Georgiev05, Aharon06, Hillar15}) for matrices $\mathbf{B}$ having exactly $m$ columns has shown that a solution to Prob.~\ref{InverseProblem}, when it exists, is unique up to such relabeling and rescaling provided the $\mathbf{x}_i$ are sufficiently diverse and $\mathbf{A}$ satisfies the \textit{spark condition}:
\begin{align}\label{SparkCondition}
\mathbf{A}\mathbf{x}_1 = \mathbf{A}\mathbf{x}_2 \implies \mathbf{x}_1 = \mathbf{x}_2, \ \ \ \text{for all $k$-sparse } \mathbf{x}_1, \mathbf{x}_2,
\end{align}
%
which is necessary to guarantee the uniqueness of arbitrary $k$-sparse $\mathbf{x}_i$. These results can be generalized to the practical setting  $\eta > 0$ by considering the following natural notion of stability with respect to measurement error.


\begin{definition}\label{maindef}
Fix $Y = \{ \mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$. We say $Y$ has a \textbf{$k$-sparse representation in $\mathbb{R}^m$} if there exists a matrix $\mathbf{A}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ such that $\mathbf{y}_i = \mathbf{A}\mathbf{x}_i$ for all $i$. 
This representation is \textbf{stable} if for every $\delta_1, \delta_2 \geq 0$, there exists some $\varepsilon = \varepsilon(\delta_1, \delta_2)$ that is strictly positive for positive $\delta_1$ and $\delta_2$ such that if $\mathbf{B}$ and $k$-sparse $\mathbf{\overline x}_1, \ldots, \mathbf{\overline x}_N \in \mathbb{R}^m$ satisfy:
\begin{align*}
	\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\overline x}_i\|_2 \leq \varepsilon(\delta_1, \delta_2),\ \   \text{for all $i=1,\ldots,N$},
\end{align*}
then there is some permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ such that for all $i, j$:
\begin{align}\label{def1}
\|\mathbf{A}_j - (\mathbf{BPD})_j\|_2 \leq \delta_1 \ \text{and} \ \|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\overline x}_i\|_1 \leq \delta_2.
\end{align}
\end{definition}

To see how Prob. \ref{InverseProblem} motivates Def. \ref{maindef}, suppose that $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ and fix $\delta_1, \delta_2$ to be the desired accuracies of recovery in \eqref{def1}. Consider any dataset $Z$ generated as in \eqref{LinearModel} with $\eta \leq \frac{1}{2} \varepsilon(\delta_1, \delta_2)$. Using the triangle inequality, it follows that any $n \times m$ matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\overline x}_1, \ldots, \mathbf{\overline x}_N$ solving Prob.~\ref{InverseProblem} are necessarily within $\delta_1$ and $\delta_2$ of the original dictionary $\mathbf{A}$ and codes $\mathbf{x}_1, \ldots, \mathbf{x}_N$, respectively.\footnote{We mention that the different norms in \eqref{def1} reflect the distinct meanings typically ascribed to the dictionary and sparse codes in modeling data.}

The main result of this work is a very general uniqueness theorem (Thm.~\ref{DeterministicUniquenessTheorem}) directly 
implying (Cor.~\ref{DeterministicUniquenessCorollary}), which guarantees that sparse representations of a dataset $Z$ are unique up to noise whenever generating dictionaries $\mathbf{A}$ satisfy a spark condition on supports and the original sparse codes $\mathbf{x}_i$ are sufficiently diverse (e.g., Fig.~\ref{noisyrecovery}).  Moreover, an explicit, computable $\varepsilon(\delta_1, \delta_2)$ that is linear in desired accuracy $\delta_1$, and essentially so in $\delta_2$, is defined in (\ref{epsdel}).

The next chapter gives formal statements of these findings. The same guarantees are then extended (Thm.~\ref{SLCopt}) to the following alternate formulation of the dictionary learning problem, which seeks to minimize the total number of nonzero entries in sparse codes.

\begin{problem}\label{OptimizationProblem}
Find matrices $\mathbf{B}$ and vectors \mbox{$\mathbf{\overline x}_1, \ldots, \mathbf{\overline x}_N$} solving:
\begin{align}\label{minsum}
\min \sum_{i = 1}^N \|\mathbf{\overline x}_{i}\|_0 \ \
\text{subject to} \ \ \|\mathbf{z}_i - \mathbf{B}\mathbf{\overline x}_i\|_2 \leq \eta, \ \text{for all $i$}.
\end{align}
\end{problem}

Surprisingly, the development of Thm.~\ref{DeterministicUniquenessTheorem} is general enough to provide some uniqueness and stability even when generating $\mathbf{A}$ do not fully satisfy (\ref{SparkCondition}) and recovery dictionaries $\mathbf{B}$ have more columns than $\mathbf{A}$.  Moreover, the approach incorporates a theory of combinatorial designs for the sparse supports of generating codes $\mathbf{x}_i$ that should be of independent interest. These results are then adapted to apply to dictionaries and codes drawn from arbitrary (continuous) probability distributions (Cor.~\ref{ProbabilisticCor}).

\section{Outline of the thesis}

TODO 
%The technical proofs of Thms.~\ref{DeterministicUniquenessTheorem} and ~\ref{SLCopt} are deferred to Chap.~\ref{DUT}, following some necessary definitions and a fact in combinatorial matrix analysis (Lem.~\ref{MainLemma}; proven in the Appendix). %These results and their applications are discussed in Chap.~\ref{Discussion} and outline open questions and directions for future research in Sec.~\ref{FutureDirections}. 
%The Appendix contains a proof of Lem.~\ref{MainLemma}.