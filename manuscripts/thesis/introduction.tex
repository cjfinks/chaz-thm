\chapter{Introduction}\label{Intro}

%\begin{itemize}
%\item Background to the topic.
%\item Brief review of current knowledge.
%\item Indicate the gap in knowledge. State the aim of the research and how it fits into the gap.
%\item Can include an outline of what follows.
%\end{itemize}

%Why do neuroscientists care about dictionary learning?

%Sparse representation modeling is an approach to signal processing that seeks to describe signals as linear combinations of a few elementary waveforms selected from a pre-specified ``dictionary". 

It is a longstanding practice in the field of signal processing to describe signals as linear combinations of elementary waveforms selected from a pre-specified ``dictionary". When this dictionary forms a basis for the signal space, every signal has a unique decomposition into these atomic components. In the most basic such case, the basis is orthonormal and the representational coefficient scaling a given elementary waveform is simply the inner product of that waveform with the signal. %When non-orthogonal, the coefficients are computed by taking the inner product with respect to the components of the dictionary inverse.

Until recently, bases have been the default form of signal representation due largely to their simplicity. For many signal analysis tasks, however, there is no one basis expressive enough to reveal clearly all of the relevant features of the signal. For example, a signal can be decomposed into its constituent frequencies via the Fourier transform, a linear change of basis. If our signal can be either a sine wave or a delta function, neither the standard basis nor the Fourier basis can capture one case as effectively as it can the other. 

The need for such freedom of expression eventually led to the development of redundant signal representations utilizing overcomplete dictionaries, which contain more atoms than there are dimensions of the signal. In such cases, there are infinitely many ways in which a signal may be decomposed into a superposition of the waveforms constituting the dictionary, and the intention is rather to seek the most informative such representation as measured by some task-specific cost function. 

A popular approach to the design of overcomplete dictionaries has been to seek one with respect to which every signal in the signal class of interest admits a sparse representation; that is, it can be represented, or at least well-approximated, as a combination of only a few dictionary elements from the bunch. Finishing with our example, the union of the standard basis with the Fourier basis is an over-complete dictionary with respect to which both sines and delta functions (or any finite superpositions thereof) achieve the sparsest possible representation.

Early approaches to sparse representation modeling assumed a model of the signal class from which a suitable sparsifying dictionary could be derived, as we have in our recurring example. While such dictionaries are typically characterized by an analytic formulation and a fast implicit implementation, they unfortunately tend to be over-simplistic models when applied to natural phenomena. 

An alternative modern approach to dictionary design is conditioned on the assumption that the sparse structure of signals conveying information about complex natural phenomena can be more accurately extracted directly from a training dataset, a process referred to as dictionary learning  (see \cite{Zhang15} for a comprehensive review). In the seminal work \cite{Olshausen96}, a dictionary trained over a collection of small patches extracted from images of the natural environment was shown to share qualitative similarities with linear filters estimated from the response properties of simple-cell neurons in mammalian visual cortex, which until then had been more weakly described analytically as Gabor filters (see also \cite{hurri1996image, bell1997independent, van1998independent}). This remarkable discovery showed that the assumption of sparsity alone could potentially account for a fundamental property of the visual system, and demonstrated the potential of the machine learning approach to dictionary design. Even more curiously, these waveforms (e.g., Gabor-like wavelets) tend to appear in dictionaries learned by a variety of machine learning algorithms trained over different natural image datasets, suggesting that the optimal dictionaries for these signals may, in some sense, be canonical \cite{donoho2001can}.

%Sparse coding is a modern signal processing technique that views each of $N$ observed $n$-dimensional signal samples as a (noisy) linear combination of at most $k$ elementary waveforms drawn from a ``dictionary" of size $m \ll N$ (see \cite{Zhang15} for a comprehensive review). 

%and more recent work relating compressed sensing \cite{eldar2012compressed} to a theory of information transmission through random wiring bottlenecks in the brain \cite{Isely10}, 
In light of these discoveries, it is natural to wonder when a dictionary admitting optimally sparse representations of a signal class is indeed identifiable from a representative sample. Answers to this question have implications in practice whenever an appeal is made to latent sparse structure of data (e.g., forgery detection \cite{hughes2010, olshausen2010applied}; brain recordings \cite{jung2001imaging, agarwal2014spatially, lee2016sparse}; and gene expression \cite{wu2016stability}). 
Even though several algorithms have recently been proposed to provably recover a unique dictionary under specific conditions (see \cite[Sec.~I-E]{Sun16} for a summary of the state of the art), few theorems can be invoked to justify the consistency of inference under this model of data more broadly. Surprisingly, despite the now ubiquitous application of dictionary learning methods in practice, a universal guarantee of the uniqueness and stability of learned dictionaries and the sparse representations they induce over noisy data (noise being inevitable in practice) has yet, to the best of my knowledge, to appear in the literature.

%injective on a sparse domain
In this work, it is proven very generally that uniqueness and stability is a typical property of learned dictionaries. Specifically, if each of $N$ observed $n$-dimensional signal samples is viewed as a (noisy) linear combination of at most $k$ elementary waveforms drawn from a suitable dictionary of size $m \ll N$, then the dictionary is identifiable from \mbox{$N = m(k-1){m \choose k} + m$} noisy linear combinations of $k$ of its columns up to an error that is linear in the noise (Thm.~\ref{DeterministicUniquenessTheorem}). In fact, provided $n \geq \min(2k,m)$, in almost all cases the problem is well-posed, as per Hadamard \cite{Hadamard1902}, given a sufficient amount of data (Thm.~\ref{robustPolythm} and Cor.~\ref{ProbabilisticCor}). 
Similar guarantees also hold for the related (and perhaps more commonly posed, e.g. \cite{rehnsommer2007}) optimization problem seeking a dictionary minimizing the average number of elementary waveforms required to reconstruct each sample of the dataset (Thm.~\ref{SLCopt}). To great practical benefit (and technical pain!), these guarantees apply without imposing any assumptions at all on learned dictionaries beyond an upper bound on their size, which is necessary in any case to avoid trivial solutions (e.g., allowing $m = N$). %That is, every pair of solutions of comparable size has some number of dictionary elements in common (up to noise), and similarly so for the coefficients of sparse codes they induce.

\section{The dictionary learning problem(s)}

Let us proceed to rigorously define the two formulations of the dictionary learning problem with which this work concerns itself. Fix a matrix $\mathbf{A} \in \mathbb R^{n \times m}$ with the elementary waveforms of the dictionary as its columns  $\mathbf{A}_j$ ($j = 1,\ldots,m$) and let dataset $Z$ consist of measurements:
\begin{align}\label{LinearModel}
\mathbf{z}_i = \mathbf{A}\mathbf{x}_i + \mathbf{n}_i,\ \ \  \text{$i=1,\ldots,N$},
\end{align}
for $k$-\emph{sparse} $\mathbf{x}_i \in \mathbb{R}^m$ having at most $k<m$ nonzero entries and \emph{noise} $\mathbf{n}_i \in \mathbb{R}^n$, with bounded norm $\| \mathbf{n}_i \|_2 \leq  \eta$ representing our worst-case uncertainty in measuring the product $\mathbf{A}\mathbf{x}_i$. We shall first consider the following decidable\footnote{Note that Prob.~\ref{InverseProblem} is decidable for rational inputs $\mathbf{z}_i$ \cite{chrishillar} since the statement that it has a solution can be expressed as a logical sentence in the theory of algebraically closed fields, and this theory has quantifier elimination \cite{basu2006algorithms}.} formulation of the dictionary learning problem.
%The first mathematical problem we consider is the following.

\begin{problem}\label{InverseProblem}
Find a matrix $\mathbf{B}$ and $k$-sparse codes $\mathbf{\overline x}_1, \ldots, \mathbf{\overline x}_N$ that satisfy $\|\mathbf{z}_i - \mathbf{B}\mathbf{\overline x}_i\|_2 \leq \eta$ for all $i = 1,\ldots,N$.
\end{problem}

Note that every solution to Prob.~\ref{InverseProblem} represents infinitely many equivalent alternatives $\mathbf{BPD}$ and $\mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\overline x}_1, \ldots, \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\overline x}_N$ parametrized by a choice of permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$. 
Identifying these ambiguities (labelling and scale) yields a single orbit of solutions represented by any particular set of elementary waveforms (the columns of $\mathbf{B}$) and their associated sparse coefficients (the entries of $\mathbf{\overline x}_i$) that reconstruct each data point $\mathbf{z}_i$. 

Previous theoretical work addressing the noiseless case $\eta =0$ (e.g., \cite{li2004analysis, Georgiev05, Aharon06, Hillar15}) for matrices $\mathbf{B}$ having exactly $m$ columns has shown that a solution to Prob.~\ref{InverseProblem}, when it exists, is unique up to such relabeling and rescaling provided the $\mathbf{x}_i$ are sufficiently diverse and $\mathbf{A}$ satisfies the \textit{spark condition}:
\begin{align}\label{SparkCondition}
\mathbf{A}\mathbf{x}_1 = \mathbf{A}\mathbf{x}_2 \implies \mathbf{x}_1 = \mathbf{x}_2, \ \ \ \text{for all $k$-sparse } \mathbf{x}_1, \mathbf{x}_2,
\end{align}
%
which is necessary to guarantee the uniqueness of arbitrary $k$-sparse $\mathbf{x}_i$. We shall generalize these results to the practical setting  $\eta > 0$ by considering the following natural notion of stability with respect to measurement error.


\begin{definition}\label{maindef}
Fix $Y = \{ \mathbf{y}_1, \ldots, \mathbf{y}_N\} \subset \mathbb{R}^n$. We say $Y$ has a \textbf{$k$-sparse representation in $\mathbb{R}^m$} if there exists a matrix $\mathbf{A}$ and $k$-sparse $\mathbf{x}_1, \ldots, \mathbf{x}_N \in \mathbb{R}^m$ such that $\mathbf{y}_i = \mathbf{A}\mathbf{x}_i$ for all $i$. 
This representation is \textbf{stable} if for every $\delta_1, \delta_2 \geq 0$, there exists some $\varepsilon = \varepsilon(\delta_1, \delta_2)$ that is strictly positive for positive $\delta_1$ and $\delta_2$ such that if $\mathbf{B}$ and $k$-sparse $\mathbf{\overline x}_1, \ldots, \mathbf{\overline x}_N \in \mathbb{R}^m$ satisfy:
\begin{align*}
	\|\mathbf{A}\mathbf{x}_i - \mathbf{B}\mathbf{\overline x}_i\|_2 \leq \varepsilon(\delta_1, \delta_2),\ \   \text{for all $i=1,\ldots,N$},
\end{align*}
then there is some permutation matrix $\mathbf{P}$ and invertible diagonal matrix $\mathbf{D}$ such that for all $i, j$:
\begin{align}\label{def1}
\|\mathbf{A}_j - (\mathbf{BPD})_j\|_2 \leq \delta_1 \ \text{and} \ \|\mathbf{x}_i - \mathbf{D}^{-1}\mathbf{P}^{\top}\mathbf{\overline x}_i\|_1 \leq \delta_2.
\end{align}
\end{definition}

To see how Prob. \ref{InverseProblem} motivates Def. \ref{maindef}, suppose that $Y$ has a stable $k$-sparse representation in $\mathbb{R}^m$ and fix $\delta_1, \delta_2$ to be the desired accuracies of recovery in \eqref{def1}. Consider any dataset $Z$ generated as in \eqref{LinearModel} with $\eta \leq \frac{1}{2} \varepsilon(\delta_1, \delta_2)$. Using the triangle inequality, it follows that any $n \times m$ matrix $\mathbf{B}$ and $k$-sparse $\mathbf{\overline x}_1, \ldots, \mathbf{\overline x}_N$ solving Prob.~\ref{InverseProblem} are necessarily within $\delta_1$ and $\delta_2$ of the original dictionary $\mathbf{A}$ and codes $\mathbf{x}_1, \ldots, \mathbf{x}_N$, respectively.\footnote{We mention that the different norms in \eqref{def1} reflect the distinct meanings typically ascribed to the dictionary and sparse codes in modeling data.}

The main result of this work is a very general uniqueness theorem (Thm.~\ref{DeterministicUniquenessTheorem}) directly 
implying (Cor.~\ref{DeterministicUniquenessCorollary}), which guarantees that sparse representations of a dataset $Z$ are unique up to noise whenever generating dictionaries $\mathbf{A}$ satisfy a spark condition on supports and the original sparse codes $\mathbf{x}_i$ are sufficiently diverse (e.g., Fig.~\ref{noisyrecovery}).  Moreover, an explicit, computable $\varepsilon(\delta_1, \delta_2)$ is given in (\ref{epsdel}) that is linear in desired accuracy $\delta_1$, and essentially so in $\delta_2$.

The same guarantees can be extended (Thm.~\ref{SLCopt}) to the following alternate formulation of the dictionary learning problem, which seeks to minimize the total number of nonzero entries in sparse codes.

\begin{problem}\label{OptimizationProblem}
Find matrices $\mathbf{B}$ and vectors \mbox{$\mathbf{\overline x}_1, \ldots, \mathbf{\overline x}_N$} solving:
\begin{align}\label{minsum}
\min \sum_{i = 1}^N \|\mathbf{\overline x}_{i}\|_0 \ \
\text{subject to} \ \ \|\mathbf{z}_i - \mathbf{B}\mathbf{\overline x}_i\|_2 \leq \eta, \ \text{for all $i$}.
\end{align}
\end{problem}

Surprisingly, the development of Thm.~\ref{DeterministicUniquenessTheorem} is general enough to provide some uniqueness and stability even when generating $\mathbf{A}$ do not fully satisfy (\ref{SparkCondition}) and recovery dictionaries $\mathbf{B}$ have more columns than $\mathbf{A}$.  Moreover, the approach incorporates a theory of combinatorial designs for the sparse supports of generating codes $\mathbf{x}_i$ that should be of independent interest. 

\section{Outline of the thesis}

Formal statements of the main findings described above are given in Chap.~\ref{Results}, along with their adaptation to dictionaries and codes drawn from arbitrary (continuous) probability distributions (Cor.~\ref{ProbabilisticCor}). All results assume real matrices and vectors. For clarity of exposition, the technical proofs of Thms.~\ref{DeterministicUniquenessTheorem} and ~\ref{SLCopt} are deferred to Chap.~\ref{DUT}, following some necessary definitions and the statement of a key lemma in combinatorial matrix analysis (Lem.~\ref{MainLemma}, the proof of which is relegated to the chapter's Appendix). These results and their applications are discussed in Chap.~\ref{Discussion}, which concludes with some open questions and directions for future research seeded in part by the results of more practically-minded simulations. 
%The Appendix contains a proof of Lem.~\ref{MainLemma}.