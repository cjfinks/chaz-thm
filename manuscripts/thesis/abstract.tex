\begin{abstract}
Learning optimal dictionaries for sparse representation modeling has led to the discovery of characteristic sparse features in several classes of natural signals. 
However, universal guarantees of the uniqueness and stability of such features in the presence of noise are lacking. 
This work presents very general conditions guaranteeing when dictionaries yielding the sparsest encodings of a dataset are unique and stable with respect to measurement or modeling error. 
The stability constants are explicit and computable; as such, there is an effective procedure sufficient to affirm if a proposed solution to the dictionary learning problem is unique within bounds commensurate with the noise. 

Two formulations of the dictionary learning problem are considered. The first seeks a dictionary which admits a sparse representation of bounded support size for each point in a dataset. In this case, beyond the original extension of existing guarantees to the noisy regime, a theory of combinatorial designs for sparse supports is introduced to demonstrate that the dictionary and corresponding sparse codes are almost always identifiable up to scaling/labeling ambiguities and an error commensurate with the noise, given sufficient data. This is the case even if the dictionary fails to satisfy the spark condition, its size is overestimated, or the data is distributed over only a polynomial number of subspaces spanned by the dictionary. 

The second formulation of the problem seeks a dictionary which minimizes the average support size of the induced sparse representations of the data. The guarantees in this case are the first of their kind in both in the noiseless and noisy regimes, and are derived by demonstrating that in fact, given sufficient data, this second problem reduces to an instance of the first. Importantly, in both cases the guarantees apply without imposing any assumptions at all on learned dictionaries beyond a natural upper bound on their size. 

This work serves to justify, in principle, dictionary learning in general as means of discovering latent sparse structure in real data. Though much work remains to be done deriving criteria for use in practice, it is my hope that the theoretical tools developed herein will be of use to this end.
%The work closes with some open questions and directions for future research, seeded in part by the results of more practically-minded simulations. 
\end{abstract}