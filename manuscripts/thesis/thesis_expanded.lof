\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {0.1}{\ignorespaces \textbf {Learning a dictionary from increasingly noisy data}. The (unraveled) basis elements of the $8 \times 8$ discrete cosine transform (DCT) form the 64 columns of the left-most matrix above. Three increasingly imprecise dictionaries (columns reordered to best match original) are recovered by FastICA \cite {hyvarinen2000independent} trained on data generated from $8$-sparse linear combinations of DCT elements corrupted with additive noise (increasing from left to right).}}{7}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {0.2}{\ignorespaces Probability of successful dictionary and code recovery (as per Thm. 1\hbox {}) for a number of samples $N$ given as a fraction of the deterministic sample complexity $N = |\mathcal {H}|[(k-1){m \atopwithdelims ()k} + 1]$ when the support set hypergraph $\mathcal {H}$ is the set of $m$ consecutive intervals of length $k$ in a cyclic order on $[m]$. Each plot has $k$ ranging from $2$ to $m-1$ (the case $k=1$ requires $N=m$), with lighter grey lines corresponding to larger $k$. Successful recovery is nearly certain with far fewer samples than the deterministic sample complexity. }}{20}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {0.3}{\ignorespaces Distribution of $C_2(\mathbf {A}, \mathcal {H})$ computed for 1.33x overcomplete generic unit-norm dictionaries $\mathbf {A} \in \mathbb {R}^{n \times m}$ (i.e. with $n = 3m/4$) when the support set hypergraph $\mathcal {H}$ consists of the rows and columns formed by arranging the elements of $[m]$ into a square grid (i.e. $m = k^2$). The distribution becomes more concentrated as $m$ grows.}}{21}
