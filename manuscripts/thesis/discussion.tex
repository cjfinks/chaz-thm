\section{Discussion}\label{Discussion}

A main motivation for this work is the emergence of seemingly unique representations from sparse coding models trained on natural data, despite the varied assumptions underlying the many algorithms in current use. Our results constitute an important step toward explaining these phenomena as well as unifying many publications on the topic by deriving general deterministic conditions under which identification of parameters in this model is not only possible but also robust to uncertainty in measurement and model choice.

We have shown that, given sufficient data, the problem of determining a dictionary and sparse codes with minimal support size (Prob.~\ref{OptimizationProblem}) reduces to an instance of Prob.~\ref{InverseProblem}, to which our main result (Thm.~\ref{DeterministicUniquenessTheorem}) applies: every dictionary and sequence of sparse codes consistent with the data are equivalent up to inherent relabeling/scaling ambiguities and a discrepancy (error) that scales linearly with the measurement noise or modeling inaccuracy. The constants we provide are explicit and computable; as such, there is an effective procedure that sufficiently affirms if a proposed solution to these problems is indeed unique up to noise and inherent ambiguities, although it is not efficient in general.

%A compelling feature of this model is its simple instantiation of the principle of Occam's razor: a natural video, for instance, is modeled as a linear combination of a small number of spatiotemporal building blocks, each representing an archetypical feature latent in the data. 

%For theoretical neuroscience in particular, dictionary learning and related methods have recovered characteristic components of natural images \cite{Olshausen96, hurri1996image, bell1997independent, van1998independent} and sounds \cite{bellsejnowski1996, smithlewicki2006} that reproduce response properties of certain cortical neurons.  

%to the recovery of ``mouse neuronal activity representing location on a track \cite{agarwal2014spatially}

%Moreover, we show that even if the meta-parameter for the number of dictionary elements is overestimated, a subset of parameters may still be identifiable up to noise. 

Beyond an extension of existing noiseless guarantees \cite{Hillar15} to the noisy regime and their novel application to Prob.~\ref{OptimizationProblem}, our work contains a theory of combinatorial designs for support sets key to identification of dictionaries. We incorporate this idea into a fundamental lemma in matrix theory (Lem.~\ref{MainLemma}) that draws upon the definition of a matrix lower bound (\ref{Ldef}) induced by a hypergraph. The new insight offered by this combinatorial approach allows for guaranteed recovery of some or all dictionary elements even if: 1) dictionary size is overestimated, 2) data cover only a polynomial number of distinct sparse supports, and 3) dictionaries do not satisfy the spark condition. 

The absence of any assumptions about dictionaries solving Prob.~\ref{InverseProblem} was a major technical obstruction in proving Thm.~\ref{DeterministicUniquenessTheorem}. We sought such a general guarantee because of the practical difficulty in ensuring that an algorithm maintain a dictionary satisfying the spark condition \eqref{SparkCondition} at each iteration, an implicit requirement of all previous works except \cite{Hillar15}; indeed, even certifying a dictionary has this property is NP-hard \cite{tillmann2014computational}.

One direct application of this work is to theoretical neuroscience, wherein our theorems justify the mathematical soundness of one of the few hypothesized theories of bottleneck communication in the brain \cite{Isely10}: that sparse neural population activity is recoverable from its noisy linear compression through a randomly constructed (but unknown) wiring bottleneck by any biologically plausible unsupervised sparse coding method that solves Prob.~\ref{DeterministicUniquenessTheorem} or \ref{SLCopt} (e.g., \cite{rehnsommer2007, rozell2007neurally, pehlevan2015normative}).\footnote{We refer the reader to \cite{ganguli2012compressed} for more on interactions between dictionary learning and neuroscience.}

In fact, uniqueness guarantees with minimal assumptions apply to all areas of data science and engineering that utilize learned sparse structure. For example, several groups have applied compressed sensing to signal processing tasks; for instance, in MRI analysis \cite{lustig2008compressed}, image compression \cite{Duarte08}, and even the design of an ultrafast camera \cite{Gao14}. It is only a matter of time before these systems incorporate dictionary learning to encode and decode signals (e.g., in a device that learns structure from motion \cite{kong2016prior}), just as scientists have used sparse coding to make sense of their data \cite{jung2001imaging, agarwal2014spatially, lee2016sparse, wu2016stability}. 

Assurances offered by our theorems certify that different devices and algorithms learn equivalent representations given enough data from statistically identical systems.\footnote{To contrast with the current hot topic of ``Deep Learning'', there are few such uniqueness guarantees for these models of data; moreover, even small noise can dramatically alter their output \cite{goodfellow2014explaining}.} 
Indeed, a main reason for the sustained interest in dictionary learning as an unsupervised method for data analysis seems to be the assumed well-posedness of parameter identification in the model, confirmation of which forms the core of our findings.

\section{Future Directions}

There are many challenges left open by this work. All conditions stated here guaranteeing the uniqueness and stability of sparse representations have only been shown sufficient; it remains open, therefore, to extend them to necessary conditions, be they on the required number of samples, the structure of the support set hypergraph, or tolerable signal-to-noise ratio for a given recovery error. On this last note, the reader should be aware that the deterministic conditions derived here accomodate always the ``worst-case" noise, whereas the ``effective" noise sampled from a concentrated distribution might be significantly reduced, especially for high-dimensional data. It would be of great practical benefit to see how drastically all conditions can be relaxed by requiring less-than-certain guarantees in this way and others, such as by probabilistic pigeonholing for reducing sample complexity as discussed following the proof of Thm.~\ref{DeterministicUniquenessTheorem}.

Another interesting question raised by this work is for which special cases is it efficient to check that a solution to Prob.~\ref{InverseProblem} or \ref{OptimizationProblem} is unique up to noise and inherent ambiguities. Considering that the sufficient conditions we have described for checking this in general are NP-hard to compute, are the necessary conditions hard? Are Probs.~\ref{InverseProblem} and \ref{OptimizationProblem} then also hard (e.g., see \cite{Tillmann15})? Finally, since Prob.~\ref{SLCopt} is intractable in general, but efficiently solvable by $\ell_1$-norm minimization when the matrix is known (and has a large enough lower bound over sparse domains \cite{eldar2012compressed}), is there a version of Thm.~\ref{SLCopt} certifying when Prob.~\ref{OptimizationProblem} can be solved efficiently in full by similar means?  % [*** Incorporate this open problem with more words ***]

It is my hope that these remaining challenges pique the interest of the community in applying the theoretical tools showcased here to derive practical guidelines for researchers to bolster their analyses. I briefly outline some of these directions below. 

% THIS SECTION ADDRESSES REVIEWER COMPLAINTS:

\subsection{Tolerable Error Bounds}

A concern raised in peer review of this work was the size of the constant $C_1$, which sets the tolerable signal-to-noise ratio for dictionary and code recovery up to an acceptable error. Referring to the definition of $C_1$ in \eqref{Cdef1}, the reader should note that $L_k$ is a standard quantity in the field of compressed sensing (the ``restricted isometry constant", see footnote \ref{ripfootnote}) and it is known to be reasonable for many random distributions generating dictionaries $\mathbf{A}$ and sparse codes $\mathbf{x}_i$ [CITATION]. The constant $C_2$, on the other hand, incorporates the more obscure quantity $\xi$ defined in \eqref{FriedrichsDefinition}, which is computed from the ``Friedrichs angle" between certain spans of subsets of the columns of $\mathbf{A}$. Simulations for small (pseudo-)randomly generated dictionaries $\mathbf{A}$ suggest nonetheless that the constant $C_2$ is likely reasonable in general as well, at least for the case where $m=k^2$ and $\mathcal{H}$ is taken to be the set of rows and columns formed by arranging the elements of $[m]$ in a square grid (see Fig. \ref{reasonableC2}).

Finally, the error bounds can be improved by tightening the constant $C_1$, e.g. via an improvement in Lemma ?.

\subsection{Sample Complexity and Support Set Hypergraphs}

Deterministic sample complexity can be reduced by tightening the pigeonholing argument in the proof of Thm.~\ref{DeterministicUniquenessTheorem}. The argument iterates over supports $S \in \mathcal{H}$, in each case determining a corresponding support $\overline S \in {\bar m \choose k}$ without consideration of previously matched support pairs, whereas the assumption $L_\mathcal{H}(\mathbf{A}) > 0$ implies that no two supports in $\mathcal{H}$ can correspond to the same $\overline S$. The number of bins to pigeonhole into therefore decreases by one at each iteration. 

There is also the number of sparse supports to consider for fixed $m$ and $k$, i.e. the size of the SIP-satisfying, regular, $k$-uniform hypergraph $\mathcal{H}$ over $[m]$. These requirements can be loosened, too; in particular, though imposing regularity allows for better clarity of exposition, a close examination of Lemma (?) reveals that non-regular hypergraphs suffice so long as they satisfy a constraint on the sequence of node degrees to maintain compatibility with the iterative argument. It is natural to ask, then, what are the necessary constraints on the support set hypergraph, and what is the smallest hypergraph that satisfies these constraints for given $m$ and $k$? 

As discussed following the proof of Thm.~\ref{DeterministicUniquenessTheorem}, probabilistic guarantees can be .... 

The problem reduces to counting integer solutions to the problem $\sum_i n_i = N$ subject to $n_i < k$ for all $i$. It appears there is no closed formula for this problem, but the number of solutions can be computed in a number of operations independent of $N$. The number is the coefficient of $X_N$ in the polynomial $(1 + X + \ldots + X^k)^n$. Written as a rational function of $X$, 

%see:
%https://math.stackexchange.com/questions/553960/extended-stars-and-bars-problemwhere-the-upper-limit-of-the-variable-is-bounded

\subsection{Guarantees for Convex Optimization}

An additional consequence of Thm.~\ref{DeterministicUniquenessTheorem} of potential practical relevance to is the implied lower bound $L_{2k}(\mathbf{B}) \geq \left(L_{2k}(\mathbf{A}) - C_1\varepsilon \right) / \|\mathbf{D}\|_1$ in the case where $\mathbf{A}$ satisfies \eqref{SparkCondition}. Since solving Prob.~\ref{SLCopt} directly requires a combinatorial search over sparse supports for the $\mathbf{\overline x}_i$ for each proposed dictionary $\mathbf{B}$, rendering it intractable in general, a common strategy is to replace the $\ell_0$-norm in \eqref{minsum} with the $\ell_1$-norm, thereby transforming the inference of sparse $\mathbf{\overline x}_i$ for a fixed dictionary $\mathbf{B}$ into a convex optimization solvable by a linear program. A major advance in compressive sensing was the discovery that, for fixed $\mathbf{B}$, solutions to \eqref{minsum} using either norm yield the same sparse vectors $\mathbf{\overline x}_i$ (up to noise) provided $L_{2k}(\mathbf{B})$ is large enough \cite{?}. What the current work provides are conditions on the generating dictionary $\mathbf{A}$ and $k$-sparse codes $\mathbf{x}_i$ under which \emph{all} matrices $\mathbf{B}$ (of bounded column-norm) that solve Prob.~\ref{OptimizationProblem} also share this property by virtue of uniqueness and stability; that is, conditions which guarantee that the sparse codes $\mathbf{\overline x}_i$ solving Prob.~\ref{OptimizationProblem} also solve the convexified problem when the corresponding dictionary $\mathbf{B}$ is held fixed. It remains to determine practical conditions that reject matrices not solving Prob.~\ref{OptimizationProblem} from nonetheless solving the convexified problem.
%

Regardless, an implication of the above observation is that the following algorithm solves Prob.~\ref{OptimizationProblem} without appeal to combinatorial search through supports:
\begin{enumerate}
\item List all matrices $\mathbf{B}$.
\item For each matrix $\mathbf{B}$, solve $\mathbf{\overline x}_i = \arg \min \|\mathbf{v}\|_1$ s.t. $\|\mathbf{Bv} - \mathbf{z}_i\| \leq \eta$ for $i = 1, \ldots, N$.
\item Quantify the proposed solution of the previous step as $\rho(\mathbf{B}, \mathbf{\overline x}_1, \ldots, \mathbf{\overline x}_N) = \sum_{i=1}^{N} \|\mathbf{\overline x}_i\|_0$.
\item Output $\arg \min_{\mathbf{B}, \mathbf{\overline x}_1, \ldots, \mathbf{\overline x}_N} \left( \rho \right)$.
 \end{enumerate}
%


%Since its inception some twenty years ago, sparse coding has become a standard tool in signal analysis, yielding myriad insights into the structure of natural signals across a variety of scientific domains. In this work, 

%We make a final remark about the tightness of our results and the computability of our derived constants.
%We remark that our constants have been derived for deterministic ``worst-case'' noise, whereas the ``effective'' noise might be smaller when sampled from a distribution.

%Our results suggest that this correspondence could be due to the ``universality'' of sparse representations in natural data, an early idea in neural theory \cite{pitts1947}. 

% \vspace{-.2 cm}

%\showacknow % Display the acknowledgments section




\begin{figure}\label{probpigeon}
\begin{center}
\includegraphics[width=1 \linewidth]{figures/prob_vs_samples.png}
\caption{Probability of successful dictionary and code recovery (as per Thm. \ref{DeterministicUniquenessTheorem}) for a number of samples $N$ given as a fraction of the deterministic sample complexity $N = |\mathcal{H}|[(k-1){m \choose k} + 1]$ when the support set hypergraph $\mathcal{H}$ is the set of $m$ consecutive intervals of length $k$ in a cyclic order on $[m]$. Each plot has $k$ ranging from $2$ to $m-1$ (the case $k=1$ requires $N=m$), with lighter grey lines corresponding to larger $k$. Successful recovery is nearly certain with far fewer samples than the deterministic sample complexity. }
\vspace{-.6 cm}
\label{probvsamples}
\end{center}
\end{figure}

\begin{figure}\label{reasonableC2}
\begin{center}
\includegraphics[width=1 \linewidth]{figures/C2_m36_nt1000.pdf}
\caption{Constant $C_2(\mathbf{A}, \mathcal{H})$ computed for generic unit-norm overcomplete dictionaries $\mathbf{A} \in \mathbb{R}^{n \times m}$ (with $m = 2n$) when the support set hypergraph $\mathcal{H}$ consists of the rows and columns formed by arranging  the elements of $[m]$ into a square grid (i.e. $m = k^2$. The form of the distribution suggests a reasonable value should generally be expected.}
\vspace{-.6 cm}
\label{samples_vs_m}
\end{center}
\end{figure}


