\section{Discussion}\label{Discussion}

The main motivation for this work was the observation that characteristic sparse representations emerge from sparse coding models trained over a variety of natural scene datasets by a variety of learning algorithms. The theorems proven here provide some insight into this phenomenon by establishing very general conditions under which identification of the model parameters is not only possible but also robust to measurement and modeling error.

The guarantees concerning the identification of a dictionary and corresponding sparse codes of minimal average support size (Thm.~\ref{SLCopt}), which is the optimization problem of most interest to practitioners (Prob.~\ref{OptimizationProblem}), are to my knowledge the first of their kind in both the noise-free and noisy domains. It has been shown here that, given sufficient data, this problem reduces to an instance of Prob.~\ref{InverseProblem} to which the main result (Thm.~\ref{DeterministicUniquenessTheorem}) then applies: every dictionary and corresponding set of sparse codes consistent with the data are equivalent up to inherent relabeling/scaling ambiguities and a discrepancy (error) that scales linearly with the noise. 
In fact, %provided $n \geq \min(2k,m)$, 
in almost all cases these problems are well-posed given a sufficient amount of data (Thm.~\ref{robustPolythm} and Cor.~\ref{ProbabilisticCor}). 
Furthermore, the derived scaling constants are explicit and computable; as such, there is an effective procedure that suffices to affirm if a proposed solution to these problems is indeed unique up to noise and inherent ambiguities, although it is not efficient in general. %Specifically, given a dictionary and sparse codes purported to solve to Prob.~\ref{InverseProblem}, one checks that that they satisfy the corresponding constraints on $\mathbf{A}$ and the $\mathbf{x_i}$ given in the statement of Thm.~\ref{DeterministicUniquenessTheorem}. 



%to the recovery of ``mouse neuronal activity representing location on a track \cite{agarwal2014spatially}

The absence of any assumptions at all about dictionaries that solve Prob.~\ref{InverseProblem} was a major technical hurdle in proving Thm.~\ref{DeterministicUniquenessTheorem}. 
This very general guarantee was sought because of the practical difficulty of ensuring that an algorithm maintain a dictionary satisfying the spark condition \eqref{SparkCondition} at each iteration, which (to my knowledge) has been an explicit or implicit assumption of all previous works except \cite{Hillar15}; indeed, even certifying a dictionary has this property is NP-hard \cite{tillmann2014computational}.

Several results in the literature had to be combined to extend the guarantees derived in \cite{Hillar15} into the noisy regime. The main challenge was to generalize Lem.~\ref{MainLemma} to the case where the $k$-dimensional subspaces spanned by corresponding submatrices $\mathbf{A}_S$ and $\mathbf{B}_{\pi(S)}$ are assumed to be``close" but not identical. Unlike in \cite{Hillar15} where an inductive argument could be applied to the noiseless case, here it had to be explicitly demonstrated that this proximity relation is propagated through iterated intersections right down to the spans of the dictionary elements themselves. Lem.~\ref{DistanceToIntersectionLemma} was designed to encapsulate this fact, proven by appeal to a convergence guarantee for an alternating projections algorithm first proposed by von Neumann. This result, combined with a little known fact \eqref{eqdim} about the distance metric between subspaces, make up the more obscure components of the deduction.

The proof of Lem.~\ref{MainLemma} diverges most significantly from the approach taken in \cite{Hillar15} by way of Lem.~\ref{NonEmptyLemma}, which utilizes a combinatorial design for support sets (the ``singleton intersection property") to reduce the deterministic sample complexity by an exponential factor. This constitutes a significant advance toward legitimizing dictionary learning in practice, since data must otherwise exhibit the exponentially many possible $k$-wise combinations of dictionary elements required by (to my knowledge) all previously published results; although an exponential number of samples per support is still required (unless $k$ is held fixed). The issue is that the map $\pi:\mathcal{H} \to {[m] \choose k}$ is surjective only when $\mathcal{H}$ is taken to be ${[m] \choose k}$, in which case one may proceed by induction as in \cite{Hillar15}, freely choosing supports in the codomain of $\pi$ to intersect over $(k-1)$ indices to then map back to some corresponding set of $(k-1)$ indices at the intersection of supports in the domain. Here, for $\mathcal{H} \subset {[m] \choose k}$, a bijection between indices had to instead be established by pigeonholing the image of $\pi$ under constraints imposed by the SIP, which was formulated specifically for this purpose. It just so happened that this more general argument for a non-surjective $\pi$ constrained by the SIP applied just as well to the situation where the number of dictionary elements $m$ is over-estimated (i.e. $\overline m > m$), in which case a one-to-one correspondence can be guaranteed between a subset of columns of $\mathbf{A}$ and $\mathbf{B}$ of a size simply expressed in terms of the width of each matrix and the regularity of $\mathcal{H}$. 

One of the mathematically significant achievements in \cite{Hillar15} was to break free from the constraint that the recovery matrix $\mathbf{B}$ satisfy the spark condition in addition to the generating dictionary $\mathbf{A}$. %, in contrast to all previously known uniqueness results which (to my knowledge) have all either explicitly or implicitly made this assumption. 
Here, it has been demonstrated that, in fact, even $\mathbf{A}$ need not satisfy the spark condition! Rather, $\mathbf{A}$ need only be injective on the union of subspaces with supports forming a regular $k$-uniform hypergraph satisfying the SIP (a distinguishing example is given in Sec.~\ref{Results}). This relaxation of constraints inspired the definition of the restricted matrix lower bound $L_{\mathcal{H}}$ in \eqref{Ldef}, which generalizes the well-known (see footnote \ref{ripfootnote}) restricted matrix lower bound $L_k$ to be in terms of a hypergraph $\mathcal{H}$, and an interesting object for further study in its own right. %(For instance, how hard is it to check that $L_\mathcal{H}(\mathbf{A}) > 0$?) 
%The absence of any restrictions on dictionaries solving Prob.~\ref{InverseProblem} is a major technical obstruction, but highly desirable because of the practical difficulty in ensuring that an algorithm maintain a dictionary satisfying the spark condition \eqref{SparkCondition} at each iteration; indeed, even certifying a dictionary has this property is NP-hard \cite{tillmann2014computational}. 

To reiterate, the methods applied here to prove Thm.~\ref{DeterministicUniquenessTheorem} yield the following results beyond a straightforward extension of those in \cite{Hillar15} to the noisy case:

\begin{enumerate}
\item A reduction in deterministic sample complexity: To identify the $n \times m$ generating dictionary $\mathbf{A}$, it is required in \cite{Hillar15} that $k{m \choose k}$ data points be sampled from each of the ${m \choose k}$ subspaces spanned by subsets of $k$ columns of $\mathbf{A}$. It is shown here that in fact it suffices to sample from at most $m$ such subspaces (see Cor.~\ref{DeterministicUniquenessCorollary}). %; thus, for fixed $k$, the deterministic sample complexity is polynomial in $m$.  %HS15 was already poly for fixed k
\item An extension of guarantees to the case where the number of dictionary elements is unknown: The results of \cite{Hillar15} only apply to the case where the matrix $\mathbf{B}$ has the same number of columns as $\mathbf{A}$. It is shown here that if $\mathbf{B}$ has at least as many columns as $\mathbf{A}$ then it contains (up to noise) a subset of the columns of $\mathbf{A}$.
\item Relaxed requirements (no spark condition) on the generating matrix $\mathbf{A}$: Rather, $\mathbf{A}$ need only be injective on the union of subspaces with supports that form a regular uniform hypergraph satisfying the SIP. 
\end{enumerate}

As it seemed to benefit a reviewer of this work, a brief discussion on how the deterministic sample complexity $N = |\mathcal{H}\left[|(k-1){m \choose k} + 1\right]$ derived here compares to those listed in the the top two rows listed in Table I of \cite{Hillar15} may be in order. To be clear, the theory developed here is strictly more general, since $\mathcal{H}$ can always be taken to be ${[m] \choose k}$. The point of difference in this comparison is the assumed set of supports for sparse codes, which is always ${[m] \choose k}$ in \cite{Hillar15}, whereas here it can be assumed to be any regular $k$-uniform hypergraph that satisfies the SIP. By row, %, assuming for our results that sufficient data have been sampled from all supports in a hypergraph H satisfying the SIP:
\begin{enumerate}
\item[I.] The result here improves upon the listed $k{m \choose k}^2$ by an exponential factor, since for every $k < m$ there exists a regular $k$-uniform hypergraph $\mathcal{H}$ with $|\mathcal{H}| = m$ satisfying the SIP. 
\item[II.] The authors in \cite{Hillar15} have applied measure-theoretic arguments to achieve $(k+1){m \choose k}$ with almost-certainty (i.e. with probability one), a factor of $m$ reduction over that for which certainty can alternatively be guaranteed here.    
%\item[III.] A direct comparison cannot be made because it is an open question as to the probability that a random set of supports satisfies the SIP. This would be an interesting problem for the community to solve; regardless, the result of \cite{Hillar15} is still implied by the more general theory given here.
%\item[IV.] The result here is $|\mathcal{H}\left[|(k-1){m \choose k} + 1\right]$ with probability one compared to $(k+1){m \choose k}$ with probability one. In this case the sample complexity stated in \cite{Hillar15} is marginally better, but it relies on a noise-free technicality unique to their assumption $\mathcal{H} = {m \choose k}$. %differs in flavor: their $\mathbf{x_i}$ are assumed to be distributed as $(k+1)$ samples per support for every support of size $k \in [m]$, whereas here it is assumed that $(k-1){m \choose k}$ codes $\mathbf{x_i}$ are distributed over each support in some hypergraph $\mathcal{H}$ satisfying the SIP. 
%\item[V.] Again, a direct comparison cannot be made for the reason stated for row III.
\end{enumerate}

%Beyond an extension of existing noiseless guarantees \cite{Hillar15} to the noisy regime and their novel application to Prob.~\ref{OptimizationProblem}, this work contains a theory of combinatorial designs for support sets key to identification of dictionaries. This idea is incorporated into a fundamental lemma in matrix theory (Lem.~\ref{MainLemma}) that draws upon the definition of a matrix lower bound (\ref{Ldef}) induced by a hypergraph. As a consequence of this combinatorial approach, recovery of some or all dictionary elements can be guaranteed even if: i) dictionary size is overestimated, ii) data cover only a polynomial number of distinct sparse supports, and iii) dictionaries do not satisfy the spark condition. 

Another reviewer of this work lamented that while the extension from exact recovery to the noisy stability of dictionary learning is significant, the fact that the analysis relies on metrics of the data that are not feasible to compute limits its impact to the scientific community beyond computer science and applied mathematics. What sets the main results of this work apart from the vast majority of results in the field is that they are deterministic, and do not depend on any kind of assumption about the particular random distribution from which the sparse supports, coefficients, or dictionary entries are drawn (though Cor.~\ref{ProbabilisticCor} makes a sweeping statement applicable to all continuous distributions). Consequently, the inferences of those applying dictionary learning methods to inverse problems in their research are justified only in principle; but this is unavoidably the case for NP-hard problems. 

Indeed, theoretical validation makes little practical difference if the methodology is already in widespread use, while practical criteria establishing whether the data models obtained by practitioners are optimal or not would have very high impact. To this end, the work has been laid out for those wanting to derive statistical criteria for inference with respect to more domain-specific structured dictionaries and codes (i.e. estimate $C_1$), and reduced by half for those hoping to prove the consistency of any dictionary learning algorithm (i.e. prove convergence to within $\varepsilon(\delta_1,\delta_2)$ given in \eqref{epsdel}). 
%who will derive from our deterministic guarantees the statistical criteria for inference in more domain-specific probabilistic models

Nonetheless, a main reason for the sustained interest in dictionary learning as an unsupervised method for data analysis seems to be the assumed well-posedness of parameter identification in the model, confirmation of which forms the core of these findings. Several groups have applied compressed sensing to signal processing tasks; for instance, in MRI analysis \cite{lustig2008compressed}, image compression \cite{Duarte08}, and even the design of an ultrafast camera \cite{Gao14}. It is only a matter of time before these systems incorporate dictionary learning to encode and decode signals (e.g., in a device that learns structure from motion \cite{kong2016prior}), just as scientists have used sparse coding to %make sense of their data \cite{jung2001imaging, agarwal2014spatially, lee2016sparse, wu2016stability}. 
uncover latent structure in data (e.g., forgery detection \cite{hughes2010, olshausen2010applied}; brain recordings \cite{jung2001imaging, agarwal2014spatially, lee2016sparse}; and gene expression \cite{wu2016stability}). As uniqueness guarantees with minimal assumptions apply to all areas of data science and engineering that utilize learned sparse structure, assurances offered by these theorems give hope that different devices and algorithms may learn equivalent representations given enough data from statistically identical systems.\footnote{To contrast with the current hot topic of ``Deep Learning'', there are few such uniqueness guarantees for these models of data; moreover, even small noise can dramatically alter their output \cite{goodfellow2014explaining}.} 


Within the field of theoretical neuroscience in particular, dictionary learning for sparse coding and related methods have recovered characteristic components of natural images \cite{Olshausen96, hyvarinen1999fast, bell1997independent, van1998independent} and sounds \cite{bellsejnowski1996, smithlewicki2006, Carlson12} that reproduce response properties of cortical neurons. The results of this work suggest that this correspondence could be due to the ``universality" of sparse representations in natural data, an early mathematical idea in neural theory \cite{pitts1947}. Furthermore, they justify the soundness of one of the few hypothesized theories of bottleneck communication in the brain \cite{Isely10}: that sparse neural population activity is recoverable from its noisy linear compression through a randomly constructed (but unknown) wiring bottleneck by any biologically plausible unsupervised sparse coding method that solves Prob.~\ref{DeterministicUniquenessTheorem} or \ref{SLCopt} (e.g., \cite{rehnsommer2007, rozell2007neurally, pehlevan2015normative}).\footnote{We refer the reader to \cite{ganguli2012compressed} for more on interactions between dictionary learning and neuroscience.}

%The solution to Prob.~\ref{OptimizationProblem}, that of most interest to practitioners of dictionary learning methods, is (to my knowledge) the first of its kind in both the noise-free and noisy domains. 

\section{Future Directions}\label{FutureDirections}

There are many challenges left open by this work. First and foremost, it should be stressed that all conditions stated here which guarantee the uniqueness and stability of sparse representations have only been shown sufficient; it remains open to work out a set of necessary conditions on all fronts, be it on the number of required samples per support, the structure of support set hypergraphs, or the tolerable signal-to-noise ratio for a bounded recovery error. It is also worth stressing that the deterministic conditions derived here must accommodate always the worst possible cases. It would be of great practical benefit to see how drastically all conditions can be relaxed by requiring less-than-certain guarantees, as (for instance) exhibited in the discussion on probabilistic pigeonholing following the proof of Thm.~\ref{DeterministicUniquenessTheorem}. In a similar vein, the tolerable signal-to-noise ratio can be reduced by considering the probability that noise sampled from a concentrated isotropic distribution will point in a harmful direction, which may be especially low in high-dimensional spaces or for certain support set hypergraphs.

Another interesting remaining challenge is to work out for which special cases it is efficient to check that a solution to Prob.~\ref{InverseProblem} or \ref{OptimizationProblem} is unique up to noise and inherent ambiguities. Considering that the sufficient conditions detailed here are in general NP-hard to compute, are the necessary conditions also hard to compute? Are Probs.~\ref{InverseProblem} and \ref{OptimizationProblem} then also hard (e.g., see \cite{tillmann2015computational})? Since Prob.~\ref{SLCopt} is intractable in general (i.e. including the noiseless case), but efficiently solvable by convex relaxation when the matrix $\mathbf{A}$ is known and has a large enough lower bound over sparse domains \cite{eldar2012compressed}, is there a version of Thm.~\ref{SLCopt} that lays down general conditions under which Prob.~\ref{OptimizationProblem} can be solved efficiently in full by similar means?  % [*** Incorporate this open problem with more words ***]

%Finally, it was noted that the combinatorial approach applied here allows for guaranteed recovery of some or all dictionary elements even if the dictionary is overestimated (i.e. $\overline m > m$). What if it is underestimated?  How then would the recovered dictionary elements relate to the original columns of $\mathbf{A}$?

%Our results suggest that this correspondence could be due to the ``universality'' of sparse representations in natural data, an early idea in neural theory \cite{pitts1947}. 

I briefly expand on some of these directions below. It is my hope that these remaining challenges pique the interest of the community, and that practical guidelines can be established using the theoretical tools showcased here to support researchers applying sparse coding techniques. 

% THIS SECTION ADDRESSES REVIEWER COMPLAINTS:

\subsection{Signal-to-Noise Ratio}

A concern raised in peer review of this work was the typical size of the constant $C_1$, which sets the tolerable signal-to-noise ratio for dictionary and code recovery up to an acceptable error. Referring to the definition of this constant in \eqref{Cdef1}, the reader should note that the denominator involves $L_k$, a standard quantity in the field of compressed sensing (the ``restricted isometry constant", see footnote \ref{ripfootnote}), which is known to be reasonable for many random distributions generating dictionaries $\mathbf{A}$ and sparse codes $\mathbf{x}_i$ \cite{baraniuk2008simple}. The numerator $C_2$, on the other hand, incorporates the more obscure quantity $\xi$ defined in \eqref{FriedrichsDefinition}, which is computed from the ``Friedrichs angle" between certain spans of subsets of the columns of $\mathbf{A}$. Simulations for small (pseudo-)randomly generated dictionaries $\mathbf{A}$ suggest nonetheless that the constant $C_2$ is likely reasonable in general as well (at least, for the case where $m=k^2$ and $\mathcal{H}$ is taken to be the set of rows and columns formed by arranging the elements of $[m]$ into a square grid; see Fig. \ref{reasonableC2}). These observations motivate the following conjecture:

\begin{conjecture}
For all $t > 0$,
\begin{equation*}
Pr[ \left| C_2- \mathbb{E}[C_2 ] \right| > t] \to 0 \ \ \text{as $k \to \infty$ and $k/m \to 0$} 
\end{equation*}
provided the assumptions of Thm. \ref{DeterministicUniquenessTheorem} are satisfied.
\end{conjecture}
%Finally, the error bounds can be improved by tightening the constant $C_1$, e.g. via an improvement in Lemma ?.
%$C_1 = C_1(\mathbf{A}, \mathcal{H}, \{ \mathbf{x_i} \}_{i=0}^N)$

\subsection{Sample Complexity}

It is possible to tighten the pigeonholing argument in the proof of Thm.~\ref{DeterministicUniquenessTheorem} and thereby reduce the deterministic sample complexity without recourse to uncertainty (as previously suggested). The argument as presented iterates over supports $S \in \mathcal{H}$, in each case determining a corresponding support $\overline S \in {[\overline m] \choose k}$ without consideration of previously matched support pairs; and yet the assumption $L_{2\mathcal{H}}(\mathbf{A}) > 0$ implies that no two supports in $\mathcal{H}$ can map to the same $\overline S$. The number of bins to pigeonhole into thus decreases every iteration, though this is a drop in a bucket of exponential size. It would be interesting to see how much the deterministic sample complexity can be reduced by imposing these constraints holistically, given the specific structure of the hypergraph $\mathcal{H}$.

Incidentally, there is also room to breathe in the restrictions on $\mathcal{H}$. Already, the results of this work motivate the following question, which is only one among many combinatorial problems brought to mind by the SIP (Def.~\ref{sip}):
\begin{question}
Fix integers $m$ and $k < m$. What is the smallest regular hypergraph $\mathcal{H} \subseteq {[m] \choose k}$ satisfying the SIP?
\end{question}

A close examination of the proof of Lemma \ref{NonEmptyLemma} (see the Appendix) reveals, however, that $\mathcal{H}$ need not be regular so long as it satisfies a constraint on the sequence of node degrees compatible with the iterative argument. It is then natural to wonder: what are the necessary constraints on $\mathcal{H}$, and what is the smallest hypergraph satisfying these constraints for given $m$ and $k$?

Opening ourselves up to uncertainty, we can furthermore ask:
\begin{question}\label{probofsip}
Fix $k < m$. What is the probability that a random subset of ${[m] \choose k}$ is regular and satisfies the SIP?
\end{question}

Within the realm of uncertain guarantees, we can also elaborate on the probabilistic pigeonholing strategy outlined in the discussion following the proof of Thm.~\ref{DeterministicUniquenessTheorem}. The problem is to count the number of ways in which vectors supported in $S \in \mathcal{H}$ can be partitioned among supports in ${\overline m \choose k}$ without allocating $k$ or more to any individual one (in which case the logic of the proof fails to imply the result; we are interested in the probability that it doesn't). These are integer solutions to the problem $\sum_i n_i = N$ subject to $n_i < k$ for all $i$, where $i = 1, \ldots, {\overline m \choose k}$. Following closely the exposition in \cite{stackexchangeanswer}, it appears there is no closed formula for this problem, but the number of solutions can be computed in a number of operations independent of $N$. Writing $p = {\overline m \choose k}$, the number is the coefficient of $X^N$ in the polynomial $(1 + X + \ldots + X^{k-1})^p$. Written as a rational function of $X$, 

\begin{equation*}
(1 + X + \ldots + X^{k-1})^p = \left(\frac{1 - X^k}{1 - X} \right)^p = \frac{\left(1 - X^k \right)^p}{\left(1 - X\right)^p}
\end{equation*}
the coeffiecient of $X^i$ in the numerator is zero unless $i$ is a multiple $qk$ of $k$, in which case it is $(-1)^q{p \choose q}$, and the coefficient of $X^j$ in the inverse of the denominator is $(-1)^j {-p \choose j} = {j + p-1 \choose j}$, which is zero unless $j \geq 0$ and otherwise equal to ${j + p-1 \choose p-1}$. It remains to sum over all $i + j = N$, which gives:

\begin{equation*}
n_\text{fails} = \sum_{q=0}^{\text{min}(p, N/k)} (-1)^q {p \choose q} {N - qk + p - 1 \choose p - 1}
\end{equation*}
where the summation is truncated to ensure that $N - qk \geq 0$ (the condition $j \geq 0$) and has at most $p+1 = {\overline m \choose k} + 1$ terms.

The total number of ways to pigeonhole is $n_\text{total} = {N +p- 1 \choose p - 1}$, and so the probability of full recovery is $\left(1 - n_\text{fails} / n_\text{total} \right)^{|\mathcal{H}|}$. 
The curves computed in this way in Fig. \ref{probpigeon} suggest that, while it may very well be impossible to exorcise exponentiality from the number of required samples in the deterministic or almost-certain case, perhaps it is possible with high-probability by one way or another. Informally,

\begin{question}
While fixing $k$ yields polynomial deterministic sample complexity in $m$ (see Cor.~\ref{DeterministicUniquenessCorollary}), is there some more general probablistic sense (perhaps for some restricted class of hypergraphs) by which sample complexity is polynomial in both $m$ and $k$?
\end{question}
%Referring to Fig. \ref{probpigeon}, which computes these probabilities for some small hypergraphs, one is led to wonder if it may be possible to exorcise exponentiality from the sample complexity asymptotically in this way or another. 


%\begin{question}
%Is there a class of hypergraphs $\mathcal{H} = \mathcal{H}(m,k)$ and a polynomial $p(m,k)$ such that $N \geq p(m,k)$ implies successful dictionary and code recovery with a probability approaching one for generic $\mathbf{A}$ and $\mathbf{x}_i$ supported in $\mathcal{H}$ as $m \to \infty$?
%Is there a class of hypergraphs $\mathcal{H}$ for which a number of samples $N$ polynomial in both $m$ and $k$ suffices to guarantee stable recovery of $\mathbf{A}$ and sparse codes $\mathbf{x}_i$ as $k/m \to 0$?
%\end{question}

\subsection{Convex Optimization}

A commonly applied workaround to the intractability of Prob.~\ref{SLCopt} (see \cite{tillmann2015computational}) is to swap out the norm $\|\cdot\|_0$ in \eqref{minsum} for $\|\cdot\|_1$, thereby transforming the inference of sparse $\mathbf{\overline x}_i$ for a given $\mathbf{B}$ into a convex optimization solvable by a linear program. A major advance in compressive sensing was the discovery that \eqref{minsum} can in fact be solved in this way for fixed $\mathbf{B}$ provided $L_{2k}(\mathbf{B})$ is large enough \cite{eldar2012compressed}. The current work provides conditions on the generating dictionary $\mathbf{A}$ and $k$-sparse codes $\mathbf{x}_i$ under which \emph{all} matrices $\mathbf{B}$ (of bounded column-norm) that solve Prob.~\ref{OptimizationProblem} have $L_{2k}(\mathbf{B})$ bounded from below; specifically, $L_{2k}(\mathbf{B}) \geq \left(L_{2k}(\mathbf{A}) - C_1\varepsilon \right) / \|\mathbf{D}\|_1$ in the case where $\mathbf{A}$ satisfies \eqref{SparkCondition}. Thus, there is some noise bound inside of which all solutions to Prob.~\ref{OptimizationProblem} are solutions to the convexified problem as well. It remains to determine practical constraints that would exclude alternative dictionaries which may solve the convefixied problem without solving Prob.~\ref{OptimizationProblem}. 

% SOLUTION CHECKING ALGORITHM:
%\begin{enumerate}
 %  \item For each $\mathbf{x_i}$, determine its support $S$ and increment a counter corresponding to that support.
  % \item For each support $S$ with count greater than $(k-1){m \choose k}$, check the $\mathbf{x_i}$ supported there are in general linear position (i.e. the determinant of the matrix having $\mathbf{x}_i$ as its columns is nonzero) and discard $S$ if not.
 %  \item Check the remaining set of supports satisfies the SIP. 
  % \item If so, list all of its regular $k$-uniform sub-hypergraphs and discard those for which $L_{2\mathcal{H}}(\mathbf{A}) = 0$. 
  % \item For each of the remaining hypergraphs, compute the constant $C_1$ from $\mathbf{A}$ and the $\mathbf{x_i}$.% (or for every subset of the  $\mathbf{x_i}$ of size $(k-1){m \choose k}$).
 %  \item Check that $\varepsilon < L_2(\mathbf{A})$. If yes, the solution is "unique".
%\end{enumerate}


\begin{figure}\label{probpigeon}
\begin{center}
\includegraphics[width=1 \linewidth]{figures/prob_vs_samples.png}
\caption{Probability of successful dictionary and code recovery (as per Thm. \ref{DeterministicUniquenessTheorem}) for a number of samples $N$ given as a fraction of the deterministic sample complexity $N = |\mathcal{H}|[(k-1){m \choose k} + 1]$ when the support set hypergraph $\mathcal{H}$ is the set of $m$ consecutive intervals of length $k$ in a cyclic order on $[m]$. Each plot has $k$ ranging from $2$ to $m-1$ (the case $k=1$ requires $N=m$), with lighter grey lines corresponding to larger $k$. Successful recovery is nearly certain with far fewer samples than the deterministic sample complexity. }
\vspace{-.6 cm}
\label{probvsamples}
\end{center}
\end{figure}

\begin{figure}\label{reasonableC2}
\begin{center}
\includegraphics[width=1 \linewidth]{figures/C2.pdf}
\caption{Distribution of $C_2(\mathbf{A}, \mathcal{H})$ computed for 1.33x overcomplete generic unit-norm dictionaries $\mathbf{A} \in \mathbb{R}^{n \times m}$ (i.e. with $n = 3m/4$) when the support set hypergraph $\mathcal{H}$ consists of the rows and columns formed by arranging  the elements of $[m]$ into a square grid (i.e. $m = k^2$). The distribution becomes more concentrated as $m$ grows.}
\vspace{-.6 cm}
\label{samples_vs_m}
\end{center}
\end{figure}


