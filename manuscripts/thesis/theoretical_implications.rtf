{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww12600\viewh6780\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 THEORETICAL IMPLICATIONS\
\
REVIEWER: \
Point (a) is important because when you say these results somehow\
validate various neuroscience ideas, I am saying in the back of my mind\
that you're bravely talking yourself on a ledge. Namely, nothing in\
neuroscience can really depend on infinitesimal stability. Trying to make a\
claim like this seems to undermine your credibility with me. It seems the\
referees had the same problem.\
\
-------------------------------------\
\
RESPONSE:\
\
Our response to point (a) aside, and though we are not inclined to assume that neural circuits are incapable of extreme precision when necessary, we actually share your skepticism here regarding the modern theory of "sparse coding" (O04) for efficient representation of sensory input in brains (vision: (O96), (H96), (B97), (vH98); audio: (B96), (S06), (C12)), as well as corresponding models of bottleneck communication between neurons that utilize ideas from compressed sensing ((C10), (I10), (G12)). Nonetheless, our paper confirms the well-posedness of the central noisy sensory coding problem that specialists in these fields have suggested neural circuits might be solving, and we have verified (for the first time) that the bottleneck communication model represents a neurally plausible way of faithfully transmitting sparse sensory representations through a noisy bandwidth-limited channel.\
\
If the editors are still reluctant to become complicit in propagating unproven hypotheses of neural computation, we have no qualms downplaying this application in the manuscript. Perhaps instead, we could elaborate more on implications for the repeatability of discoveries in experimental science contingent on machine learning, which includes an explanation for the universality of the above results (e.g. "Gabors") in sensory representation (independent of any particular theory of brain computation). For example, over the years there have been many appeals to dictionary learning with sparseness constraint for uncovering latent structure in data (e.g. forgery detection (H10), (O10); brain recordings (J01), (A14), (L16); gene expression (Wu16)), several of which appear in PNAS. \
\
-------------------------------------\
\
(c) It's not clear why exact uniqueness should be so important for\
neuroscience. It seems more likely that mere similarity is what's maybe\
important. In the same sense it's not clear why formal Lipschitz stability\
in the mathematical sense should be so important for neuroscience. It seems\
more important that some sort of qualitative similarity should persist\
under perturbations.\
\
-------------------------------------\
\
RESPONSE:\
\
Unfortunately, how brains work is still largely a mystery. There are, however, some ideas for why certain models are more appealing than others. Here is how we rationalize the application of unsupervised sparse coding to the theory of bottleneck communication:\
\
Suppose some quantities of interest are encoded in the sparse activity of neurons in a sending region (it has been proposed that neural activity in certain brain regions is "sparse", e.g. for energy efficiency). These quantities are to be transmitted to some other region through as few wires (axons) as possible, e.g. due to space constraints inside the skull.\
\
The hypothesis is that the brain, up-to date on the latest fad, solves this problem by applying a (noisy) "random" projection into the lower-dimensional space spanned by these axons. The neurons in the receiving region are then tasked with decoding the quantities of interest from this compressed signal. Suppose, however, that they cannot read out this information directly from the compressed signal. Rather, the neurons in the receiving region must first reproduce to some extent the original sparse pattern of activity before they can decode from it the quantities of interest (perhaps a sparse representation is necessary or advantageous in the receiving region, just as in the sending region). Moreover - and this is central to the hypothesis - they must accomplish this feat without knowledge of the random projection applied by the sender, i.e. via dictionary learning with a sparseness constraint. \
\
In this way, the persistence of a "qualitative similarity", modeled mathematically as the formal similarity between the decodable quantities of interest in the two regions, is contingent on the uniqueness and stability of sparse representations. We have proven that any dictionary learning procedure implemented in the receiving region (biologically plausible algorithms are an active area of research (P15)) that does "well enough" will indeed yield a sparse activity pattern that is similar (at least, up to inherent relabelling ambiguity) to the original pattern in the sending region, as required. \
\
It is entirely possible, and in our opinion very likely, that the encoding and decoding procedures implemented in nervous tissue are far more sophisticated. The science is just not there yet. At the very least, we have demonstrated that the only published hypothesis regarding how this could be done is well-founded, mathematically.\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97}